{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice.\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File harry-potter.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Get PDF document\n",
    "pdf_path = \"harry-potter.pdf\"\n",
    "\n",
    "print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF acquired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:12<00:00, 17.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': 1,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': 2,\n",
       "  'page_char_count': 143,\n",
       "  'page_word_count': 33,\n",
       "  'page_sentence_count_raw': 2,\n",
       "  'page_token_count': 35.75,\n",
       "  'text': \"HP 1 - Harry Potter and the Sorcerer's Stone Harry Potter and the Sorcerer's Stone     Harry Potter & The Sorcerer’s Stone     by  J.K. Rowling\"},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 44,\n",
       "  'page_word_count': 9,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 11.0,\n",
       "  'text': \"HP 1 - Harry Potter and the Sorcerer's Stone\"},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 2390,\n",
       "  'page_word_count': 471,\n",
       "  'page_sentence_count_raw': 33,\n",
       "  'page_token_count': 597.5,\n",
       "  'text': 'CHAPTER ONE   THE BOY WHO LIVED          M  r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.       Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.       The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.       When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.       None of them noticed a large, tawny owl flutter past the window.       At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. “Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.        It was on the corner of the street that he noticed the first sign of'}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:  # Open in binary mode\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages_and_texts = []\n",
    "        for page_number in tqdm(range(len(pdf_reader.pages))):  # Iterate through pages\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  # No adjustment needed here\n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars\n",
    "                                    \"text\": text})\n",
    "        return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path)\n",
    "pages_and_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a random sample of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 54,\n",
       "  'page_char_count': 2236,\n",
       "  'page_word_count': 444,\n",
       "  'page_sentence_count_raw': 21,\n",
       "  'page_token_count': 559.0,\n",
       "  'text': 'fastest ever —” There were shops selling robes, shops selling telescopes and strange silver instruments Harry had never seen before, windows stacked with barrels of bat spleens and eels’ eyes, tottering piles of spell books, quills, and rolls of parchment, potion bottles, globes of the moon.…       “Gringotts,” said Hagrid.       They had reached a snowy white building that towered over the other little shops. Standing beside its burnished bronze doors, wearing a uniform of scarlet and gold, was —       “Yeah, that’s a goblin,” said Hagrid quietly as they walked up the white stone steps toward him. The goblin was about a head shorter than Harry. He had a swarthy, clever face, a pointed beard and, Harry noticed, very long fingers and feet. He bowed as they walked inside. Now they were facing a second pair of doors, silver this time, with words engraved upon them: Enter, stranger, but take heed Of what awaits the sin of greed, For those who take, but do not earn, Must pay most dearly in their turn. So if you seek beneath our floors A treasure that was never yours, Thief, you have been warned, beware Of finding more than treasure there.   “Like I said, Yeh’d be mad ter try an’ rob it,” said Hagrid.       A pair of goblins bowed them through the silver doors and they were in a vast marble hall. About a hundred more goblins were sitting on high stools behind a long counter, scribbling in large ledgers, weighing coins in brass scales, examining precious stones through eyeglasses. There were too many doors to count leading off the hall, and yet more goblins were showing people in and out of these. Hagrid and Harry made for the counter.       “Morning,” said Hagrid to a free goblin. “We’ve come ter take some money outta Mr. Harry Potter’s safe.”       “You have his key, sir?”       “Got it here somewhere,” said Hagrid, and he started emptying his pockets onto the counter, scattering a handful of moldy dog biscuits over the goblin’s book of numbers. The goblin wrinkled his nose. Harry watched the goblin on their right weighing a pile of rubies as big as glowing coals.       “Got it,” said Hagrid at last, holding up a tiny golden key.       The goblin looked at it closely.       “That seems to be in order.”'},\n",
       " {'page_number': 40,\n",
       "  'page_char_count': 2171,\n",
       "  'page_word_count': 480,\n",
       "  'page_sentence_count_raw': 20,\n",
       "  'page_token_count': 542.75,\n",
       "  'text': 'pocket inside his overcoat he pulled an owl — a real, live, rather ruffled-looking owl — a long quill, and a roll of parchment. With his tongue between his teeth he scribbled a note that Harry could read upside down:   Dear Professor Dumbledore, Given Harry his letter. Taking him to buy his things tomorrow. Weather’s horrible. Hope you’re well. Hagrid   Hagrid rolled up the note, gave it to the owl, which clamped it in its beak, went to the door, and threw the owl out into the storm. Then he came back and sat down as though this was as normal as talking on the telephone.       Harry realized his mouth was open and closed it quickly.       “Where was I?” said Hagrid, but at that moment, Uncle Vernon, still ashen-faced but looking very angry, moved into the firelight.       “He’s not going,” he said.       Hagrid grunted.       “I’d like ter see a great Muggle like you stop him,” he said.       “A what?” said Harry, interested.       “A Muggle,” said Hagrid, “it’s what we call nonmagic folk like them. An’ it’s your bad luck you grew up in a family o’ the biggest Muggles I ever laid eyes on.”       “We swore when we took him in we’d put a stop to that rubbish,” said Uncle Vernon, “swore we’d stamp it out of him! Wizard indeed!”       “You knew?” said Harry. “You knew I’m a — a wizard?”       “Knew!” shrieked Aunt Petunia suddenly. “Knew! Of course we knew! How could you not be, my dratted sister being what she was? Oh, she got a letter just like that and disappeared off to that — that school — and came home every vacation with her pockets full of frog spawn, turning teacups into rats. I was the only one who saw her for what she was — a freak! But for my mother and father, oh no, it was Lily this and Lily that, they were proud of having a witch in the family!”       She stopped to draw a deep breath and then went ranting on. It seemed she had been wanting to say all this for years.       “Then she met that Potter at school and they left and got married and had you, and of course I knew you’d be just the same, just as strange, just as — as — abnormal — and then, if you please, she went and got herself blown up and we got landed with you!”'},\n",
       " {'page_number': 209,\n",
       "  'page_char_count': 2191,\n",
       "  'page_word_count': 497,\n",
       "  'page_sentence_count_raw': 28,\n",
       "  'page_token_count': 547.75,\n",
       "  'text': 'me.…”       Quirrell’s voice trailed away. Harry was remembering his trip to Diagon Alley — how could he have been so stupid? He’d seen Quirrell there that very day, shaken hands with him in the Leaky Cauldron.       Quirrell cursed under his breath.       “I don’t understand…is the Stone inside the mirror? Should I break it?”       Harry’s mind was racing.       What I want more than anything else in the world at the moment, he thought, is to find the Stone before Quirrell does. So if I look in the mirror, I should see myself finding it — which means I’ll see where it’s hidden! But how can I look without Quirrell realizing what I’m up to?       He tried to edge to the left, to get in front of the glass without Quirrell noticing, but the ropes around his ankles were too tight: he tripped and fell over. Quirrell ignored him. He was still talking to himself.       “What does this mirror do? How does it work? Help me, Master!”       And to Harry’s horror, a voice answered, and the voice seemed to come from Quirrell himself.       “Use the boy…Use the boy.…”       Quirrell rounded on Harry.       “Yes — Potter — come here.”       He clapped his hands once, and the ropes binding Harry fell off. Harry got slowly to his feet.       “Come here,” Quirrell repeated. “Look in the mirror and tell me what you see.”       Harry walked toward him.        I must lie, he thought desperately. I must look and lie about what I see, that’s all.       Quirrell moved close behind him. Harry breathed in the funny smell that seemed to come from Quirrell’s turban. He closed his eyes, stepped in front of the mirror, and opened them again.       He saw his reflection, pale and scared-looking at first. But a moment later, the reflection smiled at him. It put its hand into its pocket and pulled out a blood-red stone. It winked and put the Stone back in its pocket — and as it did so, Harry felt something heavy drop into his real pocket. Somehow — incredibly — he’d gotten the Stone.       “Well?” said Quirrell impatiently. “What do you see?”       Harry screwed up his courage.       “I see myself shaking hands with Dumbledore,” he invented. “I — I’ve won the house cup for Gryffindor.”'}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some stats on the text\n",
    "\n",
    "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n",
    "\n",
    "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-distilroberta-v1`](    https://huggingface.co/sentence-transformers/all-distilroberta-v1) has an input size of 384 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>35.75</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>11.00</td>\n",
       "      <td>HP 1 - Harry Potter and the Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2390</td>\n",
       "      <td>471</td>\n",
       "      <td>33</td>\n",
       "      <td>597.50</td>\n",
       "      <td>CHAPTER ONE   THE BOY WHO LIVED          M  r....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0                0                1                        1   \n",
       "1            1                0                1                        1   \n",
       "2            2              143               33                        2   \n",
       "3            3               44                9                        1   \n",
       "4            4             2390              471                       33   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              0.00                                                     \n",
       "1              0.00                                                     \n",
       "2             35.75  HP 1 - Harry Potter and the Sorcerer's Stone H...  \n",
       "3             11.00       HP 1 - Harry Potter and the Sorcerer's Stone  \n",
       "4            597.50  CHAPTER ONE   THE BOY WHO LIVED          M  r....  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  \n",
       "count            221.00  \n",
       "mean             512.25  \n",
       "std              190.40  \n",
       "min                0.00  \n",
       "25%              525.25  \n",
       "50%              578.75  \n",
       "75%              615.50  \n",
       "max              725.25  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks like our average token count per page is 287.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384).\n",
    "\n",
    "Okay, looks like our average token count per page is 512.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-distilroberta-v1` model (this model has an input capacity of 512)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further text processing (splitting pages into sentences)\n",
    "\n",
    "The ideal way of processing text before embedding it is still an active area of research.\n",
    "\n",
    "A simple method I've found helpful is to break the text into chunks of sentences.\n",
    "\n",
    "As in, chunk a page of text into groups of 5, 7, 10 or more sentences (these values are not set in stone and can be explored).\n",
    "\n",
    "But we want to follow the workflow of:\n",
    "\n",
    "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
    "\n",
    "Some options for splitting text into sentences:\n",
    "\n",
    "1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n",
    "2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n",
    "\n",
    "Why split into sentences?\n",
    "\n",
    "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
    "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
    "\n",
    "> **Resource:** See [spaCy install instructions](https://spacy.io/usage). \n",
    "\n",
    "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence.]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't necessarily need to use spaCy, however, it's an open-source library designed to do NLP tasks like this at scale.\n",
    "\n",
    "So let's run our small sentencizing pipeline on our pages of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:01<00:00, 141.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 200,\n",
       "  'page_char_count': 2549,\n",
       "  'page_word_count': 541,\n",
       "  'page_sentence_count_raw': 26,\n",
       "  'page_token_count': 637.25,\n",
       "  'text': 'side of the chamber was a heavy wooden door.       “Do you think they’ll attack us if we cross the room?” said Ron.       “Probably,” said Harry. “They don’t look very vicious, but I suppose if they all swooped down at once…well, there’s no other choice…I’ll run.”       He took a deep breath, covered his face with his arms, and sprinted across the room. He expected to feel sharp beaks and claws tearing at him any second, but nothing happened. He reached the door untouched. He pulled the handle, but it was locked.       The other two followed him. They tugged and heaved at the door, but it wouldn’t budge, not even when Hermione tried her Alohomora charm.       “Now what?” said Ron.       “These birds…they can’t be here just for decoration,” said Hermione.       They watched the birds soaring overhead, glittering — glittering?       “They’re not birds!” Harry said suddenly. “They’re keys! Winged keys — look carefully. So that must mean…” he looked around the chamber while the other two squinted up at the flock of keys. “…yes — look! Broomsticks! We’ve got to catch the key to the door!”       “But there are hundreds of them!”       Ron examined the lock on the door.       “We’re looking for a big, old-fashioned one — probably silver, like the handle.”       They each seized a broomstick and kicked off into the air, soaring into the midst of the cloud of keys. They grabbed and snatched, but the bewitched keys darted and dived so quickly it was almost impossible to catch one.       Not for nothing, though, was Harry the youngest Seeker in a century. He had a knack for spotting things other people didn’t. After a minute’s weaving about through the whirl of rainbow feathers, he noticed a large silver key that had a bent wing, as if it had already been caught and stuffed roughly into the keyhole.       “That one!” he called to the others. “That big one — there — no, there — with bright blue wings — the feathers are all crumpled on one side.”       Ron went speeding in the direction that Harry was pointing, crashed into the ceiling, and nearly fell off his broom.       “We’ve got to close in on it!” Harry called, not taking his eyes off the key with the damaged wing. “Ron, you come at it from above — Hermione, stay below and stop it from going down and I’ll try and catch it. Right, NOW!”       Ron dived, Hermione rocketed upward, the key dodged them both, and Harry streaked after it; it sped toward the wall, Harry leaned forward and with a nasty, crunching noise, pinned it against the stone with one hand. Ron and',\n",
       "  'sentences': ['side of the chamber was a heavy wooden door.',\n",
       "   '      “Do you think they’ll attack us if we cross the room?”',\n",
       "   'said Ron.',\n",
       "   '      “Probably,” said Harry. “',\n",
       "   'They don’t look very vicious, but I suppose if they all swooped down at once…well, there’s no other choice…I’ll run.”',\n",
       "   '      He took a deep breath, covered his face with his arms, and sprinted across the room.',\n",
       "   'He expected to feel sharp beaks and claws tearing at him any second, but nothing happened.',\n",
       "   'He reached the door untouched.',\n",
       "   'He pulled the handle, but it was locked.',\n",
       "   '      The other two followed him.',\n",
       "   'They tugged and heaved at the door, but it wouldn’t budge, not even when Hermione tried her Alohomora charm.',\n",
       "   '      “Now what?”',\n",
       "   'said Ron.',\n",
       "   '      “These birds…they can’t be here just for decoration,” said Hermione.',\n",
       "   '      They watched the birds soaring overhead, glittering — glittering?',\n",
       "   '      “They’re not birds!”',\n",
       "   'Harry said suddenly. “',\n",
       "   'They’re keys!',\n",
       "   'Winged keys — look carefully.',\n",
       "   'So that must mean…” he looked around the chamber while the other two squinted up at the flock of keys. “…',\n",
       "   'yes — look!',\n",
       "   'Broomsticks!',\n",
       "   'We’ve got to catch the key to the door!”',\n",
       "   '      “But there are hundreds of them!”',\n",
       "   '      Ron examined the lock on the door.',\n",
       "   '      “We’re looking for a big, old-fashioned one — probably silver, like the handle.”',\n",
       "   '      They each seized a broomstick and kicked off into the air, soaring into the midst of the cloud of keys.',\n",
       "   'They grabbed and snatched, but the bewitched keys darted and dived so quickly it was almost impossible to catch one.',\n",
       "   '      Not for nothing, though, was Harry the youngest Seeker in a century.',\n",
       "   'He had a knack for spotting things other people didn’t.',\n",
       "   'After a minute’s weaving about through the whirl of rainbow feathers, he noticed a large silver key that had a bent wing, as if it had already been caught and stuffed roughly into the keyhole.',\n",
       "   '      “That one!”',\n",
       "   'he called to the others. “',\n",
       "   'That big one — there — no, there — with bright blue wings — the feathers are all crumpled on one side.”',\n",
       "   '      Ron went speeding in the direction that Harry was pointing, crashed into the ceiling, and nearly fell off his broom.',\n",
       "   '      “We’ve got to close in on it!”',\n",
       "   'Harry called, not taking his eyes off the key with the damaged wing. “',\n",
       "   'Ron, you come at it from above — Hermione, stay below and stop it from going down and I’ll try and catch it.',\n",
       "   'Right, NOW!”',\n",
       "   '      Ron dived, Hermione rocketed upward, the key dodged them both, and Harry streaked after it; it sped toward the wall, Harry leaned forward and with a nasty, crunching noise, pinned it against the stone with one hand.',\n",
       "   'Ron and'],\n",
       "  'page_sentence_count_spacy': 41}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful!\n",
    "\n",
    "Now let's turn out list of dictionaries into a DataFrame and get some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "      <td>29.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "      <td>12.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "      <td>51.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            221.00                     221.00  \n",
       "mean             512.25                      29.63  \n",
       "std              190.40                      12.15  \n",
       "min                0.00                       0.00  \n",
       "25%              525.25                      25.00  \n",
       "50%              578.75                      33.00  \n",
       "75%              615.50                      37.00  \n",
       "max              725.25                      51.00  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our set of text, it looks like our raw sentence count (e.g. splitting on `\". \"`) is quite close to what spaCy came up with.\n",
    "\n",
    "Now we've got our text split into sentences, how about we gorup those sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking our sentences together\n",
    "\n",
    "Let's take a step to break down our list of sentences/text into smaller chunks.\n",
    "\n",
    "As you might've guessed, this process is referred to as **chunking**.\n",
    "\n",
    "Why do we do this?\n",
    "\n",
    "1. Easier to manage similar sized chunks of text.\n",
    "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
    "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n",
    "\n",
    "Something to note is that there are many different ways emerging for creating chunks of information/text.\n",
    "\n",
    "For now, we're going to keep it simple and break our pages of sentences into groups of 10 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 384).\n",
    "\n",
    "On average each of our pages has 10 sentences.\n",
    "\n",
    "And an average total of 287 tokens per page.\n",
    "\n",
    "So our groups of 10 sentences will also be ~287 tokens long.\n",
    "\n",
    "This gives us plenty of room for the text to embedded by our `all-mpnet-base-v2` model (it has a capacity of 384 tokens).\n",
    "\n",
    "To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 20\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 95,\n",
       "  'page_char_count': 2152,\n",
       "  'page_word_count': 435,\n",
       "  'page_sentence_count_raw': 14,\n",
       "  'page_token_count': 538.0,\n",
       "  'text': 'CHAPTER EIGHT   THE POTIONS MASTER   T  here, look.”       “Where?”       “Next to the tall kid with the red hair.”       “Wearing the glasses?”       “Did you see his face?”       “Did you see his scar?”       Whispers followed Harry from the moment he left his dormitory the next day. People lining up outside classrooms stood on tiptoe to get a look at him, or doubled back to pass him in the corridors again, staring. Harry wished they wouldn’t, because he was trying to concentrate on finding his way to classes.       There were a hundred and forty-two staircases at Hogwarts: wide, sweeping ones; narrow, rickety ones; some that led somewhere different on a Friday; some with a vanishing step halfway up that you had to remember to jump. Then there were doors that wouldn’t open unless you asked politely, or tickled them in exactly the right place, and doors that weren’t really doors at all, but solid walls just pretending. It was also very hard to remember where anything was, because it all seemed to move around a lot. The people in the portraits kept going to visit each other, and Harry was sure the coats of armor could walk.       The ghosts didn’t help, either. It was always a nasty shock when one of them glided suddenly through a door you were trying to open. Nearly Headless Nick was always happy to point new Gryffindors in the right direction, but Peeves the Poltergeist was worth two locked doors and a trick staircase if you met him when you were late for class. He would drop wastepaper baskets on your head, pull rugs from under your feet, pelt you with bits of chalk, or sneak up behind you, invisible, grab your nose, and screech, “GOT YOUR CONK!”       Even worse than Peeves, if that was possible, was the caretaker, Argus Filch. Harry and Ron managed to get on the wrong side of him on their very first morning. Filch found them trying to force their way through a door that unluckily turned out to be the entrance to the out-of-bounds corridor on the third floor. He wouldn’t believe they were lost, was sure they were trying to break into it on purpose, and was threatening to lock them in the dungeons when they were',\n",
       "  'sentences': ['CHAPTER EIGHT   THE POTIONS MASTER   T  here, look.”',\n",
       "   '      “Where?”',\n",
       "   '      “Next to the tall kid with the red hair.”',\n",
       "   '      “Wearing the glasses?”',\n",
       "   '      “Did you see his face?”',\n",
       "   '      “Did you see his scar?”',\n",
       "   '      Whispers followed Harry from the moment he left his dormitory the next day.',\n",
       "   'People lining up outside classrooms stood on tiptoe to get a look at him, or doubled back to pass him in the corridors again, staring.',\n",
       "   'Harry wished they wouldn’t, because he was trying to concentrate on finding his way to classes.',\n",
       "   '      There were a hundred and forty-two staircases at Hogwarts: wide, sweeping ones; narrow, rickety ones; some that led somewhere different on a Friday; some with a vanishing step halfway up that you had to remember to jump.',\n",
       "   'Then there were doors that wouldn’t open unless you asked politely, or tickled them in exactly the right place, and doors that weren’t really doors at all, but solid walls just pretending.',\n",
       "   'It was also very hard to remember where anything was, because it all seemed to move around a lot.',\n",
       "   'The people in the portraits kept going to visit each other, and Harry was sure the coats of armor could walk.',\n",
       "   '      The ghosts didn’t help, either.',\n",
       "   'It was always a nasty shock when one of them glided suddenly through a door you were trying to open.',\n",
       "   'Nearly Headless Nick was always happy to point new Gryffindors in the right direction, but Peeves the Poltergeist was worth two locked doors and a trick staircase if you met him when you were late for class.',\n",
       "   'He would drop wastepaper baskets on your head, pull rugs from under your feet, pelt you with bits of chalk, or sneak up behind you, invisible, grab your nose, and screech, “GOT YOUR CONK!”',\n",
       "   '      Even worse than Peeves, if that was possible, was the caretaker, Argus Filch.',\n",
       "   'Harry and Ron managed to get on the wrong side of him on their very first morning.',\n",
       "   'Filch found them trying to force their way through a door that unluckily turned out to be the entrance to the out-of-bounds corridor on the third floor.',\n",
       "   'He wouldn’t believe they were lost, was sure they were trying to break into it on purpose, and was threatening to lock them in the dungeons when they were'],\n",
       "  'page_sentence_count_spacy': 21,\n",
       "  'sentence_chunks': [['CHAPTER EIGHT   THE POTIONS MASTER   T  here, look.”',\n",
       "    '      “Where?”',\n",
       "    '      “Next to the tall kid with the red hair.”',\n",
       "    '      “Wearing the glasses?”',\n",
       "    '      “Did you see his face?”',\n",
       "    '      “Did you see his scar?”',\n",
       "    '      Whispers followed Harry from the moment he left his dormitory the next day.',\n",
       "    'People lining up outside classrooms stood on tiptoe to get a look at him, or doubled back to pass him in the corridors again, staring.',\n",
       "    'Harry wished they wouldn’t, because he was trying to concentrate on finding his way to classes.',\n",
       "    '      There were a hundred and forty-two staircases at Hogwarts: wide, sweeping ones; narrow, rickety ones; some that led somewhere different on a Friday; some with a vanishing step halfway up that you had to remember to jump.',\n",
       "    'Then there were doors that wouldn’t open unless you asked politely, or tickled them in exactly the right place, and doors that weren’t really doors at all, but solid walls just pretending.',\n",
       "    'It was also very hard to remember where anything was, because it all seemed to move around a lot.',\n",
       "    'The people in the portraits kept going to visit each other, and Harry was sure the coats of armor could walk.',\n",
       "    '      The ghosts didn’t help, either.',\n",
       "    'It was always a nasty shock when one of them glided suddenly through a door you were trying to open.',\n",
       "    'Nearly Headless Nick was always happy to point new Gryffindors in the right direction, but Peeves the Poltergeist was worth two locked doors and a trick staircase if you met him when you were late for class.',\n",
       "    'He would drop wastepaper baskets on your head, pull rugs from under your feet, pelt you with bits of chalk, or sneak up behind you, invisible, grab your nose, and screech, “GOT YOUR CONK!”',\n",
       "    '      Even worse than Peeves, if that was possible, was the caretaker, Argus Filch.',\n",
       "    'Harry and Ron managed to get on the wrong side of him on their very first morning.',\n",
       "    'Filch found them trying to force their way through a door that unluckily turned out to be the entrance to the out-of-bounds corridor on the third floor.'],\n",
       "   ['He wouldn’t believe they were lost, was sure they were trying to break into it on purpose, and was threatening to lock them in the dungeons when they were']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>221.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2049.02</td>\n",
       "      <td>431.28</td>\n",
       "      <td>21.6</td>\n",
       "      <td>512.25</td>\n",
       "      <td>29.63</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.94</td>\n",
       "      <td>761.61</td>\n",
       "      <td>158.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>190.40</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.00</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>443.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>525.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.00</td>\n",
       "      <td>2315.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>578.75</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>165.00</td>\n",
       "      <td>2462.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>615.50</td>\n",
       "      <td>37.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2901.00</td>\n",
       "      <td>564.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>725.25</td>\n",
       "      <td>51.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       221.00           221.00           221.00                    221.0   \n",
       "mean        110.00          2049.02           431.28                     21.6   \n",
       "std          63.94           761.61           158.89                      9.0   \n",
       "min           0.00             0.00             1.00                      1.0   \n",
       "25%          55.00          2101.00           443.00                     19.0   \n",
       "50%         110.00          2315.00           493.00                     23.0   \n",
       "75%         165.00          2462.00           518.00                     26.0   \n",
       "max         220.00          2901.00           564.00                     45.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count            221.00                     221.00      221.00  \n",
       "mean             512.25                      29.63        1.98  \n",
       "std              190.40                      12.15        0.58  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              525.25                      25.00        2.00  \n",
       "50%              578.75                      33.00        2.00  \n",
       "75%              615.50                      37.00        2.00  \n",
       "max              725.25                      51.00        3.00  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to get stats\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the average number of chunks is around 1.5, this is expected since many of our pages only contain an average of 10 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item\n",
    "\n",
    "We'd like to embed each chunk of sentences into its own numerical representation.\n",
    "\n",
    "So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<00:00, 14058.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 134,\n",
       "  'sentence_chunk': 'A murmur ran through the crowd as Adrian Pucey dropped the Quaffle,',\n",
       "  'chunk_char_count': 67,\n",
       "  'chunk_word_count': 12,\n",
       "  'chunk_token_count': 16.75}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a random sample\n",
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!\n",
    "\n",
    "Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n",
    "\n",
    "This means we could reference a chunk of text and know its source.\n",
    "\n",
    "Let's get some stats about our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>437.00</td>\n",
       "      <td>437.00</td>\n",
       "      <td>437.00</td>\n",
       "      <td>437.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.24</td>\n",
       "      <td>1008.22</td>\n",
       "      <td>190.59</td>\n",
       "      <td>252.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63.58</td>\n",
       "      <td>525.99</td>\n",
       "      <td>98.11</td>\n",
       "      <td>131.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.00</td>\n",
       "      <td>761.00</td>\n",
       "      <td>146.00</td>\n",
       "      <td>190.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>113.00</td>\n",
       "      <td>1031.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>257.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>170.00</td>\n",
       "      <td>1334.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>333.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.00</td>\n",
       "      <td>2623.00</td>\n",
       "      <td>471.00</td>\n",
       "      <td>655.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count       437.00            437.00            437.00             437.00\n",
       "mean        113.24           1008.22            190.59             252.06\n",
       "std          63.58            525.99             98.11             131.50\n",
       "min           2.00              7.00              2.00               1.75\n",
       "25%          59.00            761.00            146.00             190.25\n",
       "50%         113.00           1031.00            194.00             257.75\n",
       "75%         170.00           1334.00            250.00             333.50\n",
       "max         220.00           2623.00            471.00             655.75"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats about our chunks\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm looks like some of our chunks have quite a low token count.\n",
    "\n",
    "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 184.0 | Text: out of sight.   “Go on then, try and hit me!”said Neville, raising his fists. “I’m ready!”   Harry turned to Hermione.   “Do something,” he said desperately.   Hermione stepped forward.   “Neville,” she said, “I’m really, really sorry about this.”   She raised her wand.   “Petrificus Totalus!”she cried, pointing it at Neville.   Neville’s arms snapped to his sides. His legs sprang together. His whole body rigid, he swayed where he stood and then fell flat on his face, stiff as a board.   Hermione ran to turn him over. Neville’s jaws were jammed together so he couldn’t speak. Only his eyes were moving, looking at them in horror.   “What’ve you done to him?”Harry whispered.   “It’s the full Body-Bind,” said Hermione miserably. “\n",
      "Chunk token count: 7.5 | Text: A magic beyond all we do here!\n",
      "Chunk token count: 191.75 | Text: “Nothing,” said Harry.   Madam Pince the librarian brandished a feather duster at him.   “You’d better get out, then. Go on — out!”   Wishing he’d been a bit quicker at thinking up some story, Harry left the library. He, Ron, and Hermione had already agreed they’d better not ask Madam Pince where they could find Flamel. They were sure she’d be able to tell them, but they couldn’t risk Snape hearing what they were up to.   Harry waited outside in the corridor to see if the other two had found anything, but he wasn’t very hopeful. They had been looking for two weeks, after A, but as they only had odd moments between lessons it wasn’t surprising they’d found nothing. What they really needed was a nice long search without Madam Pince breathing down their necks.\n",
      "Chunk token count: 247.0 | Text: “And I want to see all your family, all the Weasleys, you’ll be able to show me your other brothers and everyone.”   “You can see them any old time,” said Ron. “Just come round my house this summer. Anyway, maybe it only shows dead people. Shame about not finding Flamel, though. Have some bacon or something, why aren’t you eating anything?”   Harry couldn’t eat. He had seen his parents and would be seeing them again tonight. He had almost forgotten about Flamel. It didn’t seem very important anymore. Who cared what the three headed dog was guarding?What did it matter if Snape stole it, really?   “Are you all right?”said Ron. “You look odd.” What Harry feared most was that he might not be able to find the mirror room again. With Ron covered in the cloak, too, they had to walk much more slowly the next night. They tried retracing Harry’s route from the library, wandering around the dark passageways for nearly an hour.   “I’m freezing,” said Ron. “Let’s forget it and go back.”\n",
      "Chunk token count: 151.25 | Text: The castle felt more like home than Privet Drive ever had. His lessons, too, were becoming more and more interesting now that they had mastered the basics.   On Halloween morning they woke to the delicious smell of baking pumpkin wafting through the corridors. Even better, Professor Flitwick announced in Charms that he thought they were ready to start making objects fly, something they had all been dying to try since they’d seen him make Neville’s toad zoom around the classroom. Professor Flitwick put the class into pairs to practice. Harry’s partner was Seamus Finnigan (which was a relief, because\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 250\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many of these are headers and footers of different pages.\n",
    "\n",
    "They don't seem to offer too much information.\n",
    "\n",
    "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 4,\n",
       "  'sentence_chunk': 'CHAPTER ONE  THE BOY WHO LIVED     M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.   Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.   The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.   When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.   None of them noticed a large, tawny owl flutter past the window.   At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. “Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.    It was on the corner of the street that he noticed the first sign of',\n",
       "  'chunk_char_count': 2359,\n",
       "  'chunk_word_count': 440,\n",
       "  'chunk_token_count': 589.75},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'something peculiar — a cat reading a map. For a second, Mr. Dursley didn’t realize what he had seen — then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn’t a map in sight. What could he have been thinking of?It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive — no, looking at the sign; cats couldn’t read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he thought of nothing except a large order of drills he was hoping to get that day.   But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn’t help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr. Dursley was enraged to see that a couple of them weren’t young at all; why, that man had to be older than he was, and wearing an emerald-green cloak!The nerve of him!',\n",
       "  'chunk_char_count': 1468,\n",
       "  'chunk_word_count': 282,\n",
       "  'chunk_token_count': 367.0}]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller chunks filtered!\n",
    "\n",
    "Time to embed our chunks of text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding our text chunks\n",
    "\n",
    "While humans understand text, machines understand numbers best.\n",
    "\n",
    "An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n",
    "\n",
    "But one of my favourite and simple definitions is \"a useful numerical representation\".\n",
    "\n",
    "The most powerful thing about modern embeddings is that they are *learned* representations.\n",
    "\n",
    "Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n",
    "\n",
    "Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
    "\n",
    "> **Note:** Most modern NLP models deal with \"tokens\" which can be considered as multiple different sizes and combinations of words and characters rather than always whole words or single characters. For example, the string `\"hello world!\"` gets mapped to the token values `{15339: b'hello', 1917: b' world', 0: b'!'}` using [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (or BPE via OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library). Google has a tokenization library called [SentencePiece](https://github.com/google/sentencepiece).\n",
    "\n",
    "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
    "\n",
    "Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n",
    "\n",
    "However, we don't need to.\n",
    "\n",
    "The embedding vectors are for our computers to understand.\n",
    "\n",
    "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
    "\n",
    "Enough talking, how about we import a text embedding model and see what an embedding looks like.\n",
    "\n",
    "To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
    "\n",
    "Specifically, we'll get the `all-distilroberta-v1` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding: [-1.65902898e-02 -2.12335847e-02 -2.28640642e-02  8.87595234e-04\n",
      " -4.47793826e-02  3.57989669e-02 -4.02949713e-02  1.52442595e-02\n",
      " -3.82167995e-02 -3.07057966e-02 -2.67945845e-02  5.92993433e-03\n",
      " -6.57519512e-03 -3.78465466e-02 -1.22566912e-02  2.68598758e-02\n",
      "  6.79623261e-02 -9.76923853e-03 -2.20672823e-02 -3.65231708e-02\n",
      "  2.31705885e-03 -2.60507222e-02 -3.11338659e-02 -2.90707983e-02\n",
      " -2.35388279e-02  1.36790741e-02  8.21494237e-02  1.42724961e-02\n",
      "  4.72347206e-03  3.86894941e-02 -1.83376763e-02  2.75211912e-02\n",
      " -1.56715233e-02  2.34227683e-02  2.47984864e-02 -2.05263253e-02\n",
      "  4.73984703e-02  3.46599426e-03 -2.22720597e-02  1.92791745e-02\n",
      "  1.06708752e-02  8.44341069e-02 -6.89409822e-02  3.47610302e-02\n",
      "  2.49605570e-02 -1.25561710e-02 -7.43995607e-02 -6.67000487e-02\n",
      " -1.10511221e-02  9.09242500e-03  2.56566703e-02 -1.84278209e-02\n",
      " -3.60638537e-02  3.19261395e-04 -6.98118936e-03  3.67710032e-02\n",
      "  3.05326772e-03 -1.25956151e-03  5.74665591e-02 -9.76270717e-03\n",
      "  2.74340920e-02  2.40300782e-03 -4.35188189e-02 -5.59056029e-02\n",
      "  8.79028812e-02 -3.05436496e-02 -3.26718614e-02  8.08005687e-03\n",
      "  7.63156451e-03 -6.32095709e-02 -4.65786923e-03 -8.90410878e-03\n",
      " -2.12666560e-02 -4.87976596e-02 -1.55120911e-02 -7.12744668e-02\n",
      " -2.80335099e-02 -2.48307623e-02 -1.16104735e-02  4.48399521e-02\n",
      "  1.36126531e-02  5.69958575e-02 -2.14343723e-02 -3.29638831e-02\n",
      "  6.66818023e-02  8.31928030e-02  1.69849396e-02 -3.49969193e-02\n",
      " -1.38192857e-02  3.63680683e-02 -2.12522466e-02 -5.10459999e-03\n",
      "  4.50926684e-02 -3.16723548e-02 -1.04140742e-02  1.89510081e-02\n",
      "  7.92730786e-03 -4.54072170e-02 -2.86069755e-02 -4.46699932e-02\n",
      "  6.34449944e-02  3.82512808e-02  6.46742508e-02 -7.21961074e-03\n",
      "  1.99634358e-02  3.33112292e-02  1.48599863e-03 -6.66681305e-02\n",
      "  4.15424816e-02 -1.26806246e-02 -1.95443397e-03 -8.74419510e-02\n",
      "  1.05827656e-02 -5.36209680e-02  1.88885815e-02 -5.34995981e-02\n",
      "  1.42311193e-02 -1.61449090e-02  2.55851131e-02 -3.71361002e-02\n",
      "  1.07407896e-02  2.34211190e-03  1.96106620e-02 -2.10927520e-02\n",
      " -3.41919325e-02  1.31618651e-02  2.49171983e-02  5.30135110e-02\n",
      " -5.97527251e-02  2.99047045e-02  2.44085398e-02 -6.57337606e-02\n",
      " -1.94595810e-02  3.97100719e-03 -1.84610952e-02  1.02137364e-02\n",
      "  4.92299534e-02  4.01803628e-02 -5.81720807e-02  3.22821215e-02\n",
      "  1.59251094e-02  2.20937319e-02 -8.88023898e-03 -1.46671068e-02\n",
      "  4.40747738e-02  5.57756200e-02 -8.14599730e-03  2.24899333e-02\n",
      " -7.44708404e-02  5.01949638e-02  3.20252590e-02  2.92552412e-02\n",
      "  6.82563037e-02 -1.83754116e-02 -3.89370248e-02  4.28053811e-02\n",
      "  1.01679573e-02 -2.14874069e-03 -1.71415992e-02 -9.61070298e-04\n",
      "  3.34356688e-02  4.78158332e-02  4.14352641e-02  1.04095004e-02\n",
      "  7.15077817e-02 -4.44886088e-03  1.66255962e-02 -2.61753425e-03\n",
      "  8.54841061e-03 -1.32064754e-02  3.33485752e-02 -7.22421217e-04\n",
      "  2.09614541e-03  3.48579735e-02 -3.70360613e-02 -4.12736572e-02\n",
      "  3.27214226e-02  6.76215887e-02 -4.97362800e-02  6.30188314e-03\n",
      " -2.09218897e-02 -3.59745580e-03 -9.01658610e-02  2.69962866e-02\n",
      "  3.38005804e-04 -7.78834615e-03  1.09754666e-03 -6.24398962e-02\n",
      "  2.73952670e-02 -6.71192184e-02  8.62958878e-02 -5.30462665e-03\n",
      " -3.17746960e-02  6.07266836e-02 -7.24434108e-02 -5.10782525e-02\n",
      "  5.36884926e-02  1.88325346e-02  1.91424992e-02 -4.43821698e-02\n",
      " -1.83151606e-02  2.37659682e-02 -6.73269061e-03  2.14282726e-03\n",
      " -5.05957846e-03  6.13241270e-03  5.55954203e-02 -6.92871958e-02\n",
      " -3.60294878e-02  6.03556354e-03 -1.86803602e-02 -3.24501395e-02\n",
      " -5.35324402e-02 -3.99938598e-02 -1.26819555e-02  7.24313036e-03\n",
      "  6.19147085e-02 -1.52548216e-02  7.39991441e-02  1.84077099e-02\n",
      " -1.25280991e-02  4.05580644e-03 -8.45704600e-03 -6.73966706e-02\n",
      "  4.90405858e-02  2.11720169e-02  2.52998201e-03 -3.43066012e-03\n",
      " -3.46821290e-03 -5.57979122e-02  1.85510740e-02 -5.39022759e-02\n",
      "  6.16086088e-02 -2.34245928e-03 -5.61404321e-03  7.70045072e-02\n",
      " -2.39377022e-02 -2.00519003e-02  9.78184417e-02 -1.18508048e-01\n",
      "  6.23907112e-02 -6.06725104e-02  2.18434706e-02 -7.05090445e-03\n",
      "  1.36367287e-02 -3.34774889e-02  6.01171404e-02 -3.39568444e-02\n",
      "  3.20835263e-02 -9.17496905e-03 -4.67099575e-03 -4.53296211e-03\n",
      " -3.79474424e-02 -1.57829653e-02  2.81725619e-02 -1.82709191e-02\n",
      "  2.80573145e-02  2.95153894e-02  5.55474870e-02  2.11571455e-02\n",
      " -1.19119808e-02 -3.56343086e-03 -3.54808085e-02  6.30842671e-02\n",
      " -4.71609533e-02 -1.03341257e-02  3.15119289e-02  4.66167964e-02\n",
      "  3.41427885e-02  2.22997437e-03  1.26885925e-03  5.63309453e-02\n",
      " -1.61590800e-02  7.18228742e-02  1.95161924e-02  3.27513665e-02\n",
      "  4.21712957e-02 -2.32012160e-02 -3.53635824e-03  1.07753733e-02\n",
      "  6.08001426e-02 -5.02168536e-02  8.53808224e-02  3.85925993e-02\n",
      "  5.68417599e-03  2.44627837e-02 -9.70991049e-03  2.12806594e-02\n",
      "  5.40992133e-02 -1.69204921e-02 -4.20807526e-02 -1.75992940e-02\n",
      "  2.94377320e-02  1.93737354e-02 -2.28676163e-02 -1.52969854e-02\n",
      "  3.17724858e-04  5.02485037e-03  4.05740887e-02 -6.02938756e-02\n",
      " -8.26549158e-02  2.44237185e-02  2.36510970e-02 -9.75865591e-03\n",
      " -1.65280793e-02 -4.16171923e-02 -7.41903717e-03 -2.72681508e-02\n",
      " -8.24333206e-02  7.49535672e-03  2.03636512e-02 -5.19627519e-03\n",
      "  1.38725387e-02  4.44791988e-02  1.89377517e-02 -2.15027984e-02\n",
      " -3.10878437e-02  2.75182873e-02 -4.05452810e-02 -5.04118949e-02\n",
      " -1.81034636e-02 -4.64768894e-02  1.62545834e-02 -1.78411584e-02\n",
      "  5.11651533e-03  2.98931729e-03  4.14630165e-03  1.04815587e-02\n",
      "  1.26511790e-02 -1.96799040e-02 -2.68161558e-02 -1.67585630e-02\n",
      " -1.09539554e-01  1.21664256e-02 -1.90443825e-02 -1.40720308e-02\n",
      "  3.24433409e-02 -1.47619948e-03 -4.18384038e-02 -3.52827050e-02\n",
      " -2.12218110e-02 -5.00304885e-02  2.54620332e-02 -3.43445526e-03\n",
      " -4.94664209e-03  2.31812596e-02 -4.29280987e-03  2.52510477e-02\n",
      "  3.91770117e-02  1.16416654e-02 -5.66626787e-02  6.76572099e-02\n",
      " -4.02272344e-02  6.68368209e-03  1.29379993e-02 -2.08721776e-02\n",
      "  1.54581666e-02  2.06862222e-02  2.32384373e-02  1.05626639e-02\n",
      " -2.93277204e-03  4.49131988e-02  1.13074239e-02 -3.68735343e-02\n",
      " -5.77734970e-03  2.05111261e-02 -5.43949455e-02  1.55874910e-02\n",
      "  2.15910914e-04 -6.03020266e-02  7.32739735e-03  9.27544087e-02\n",
      "  1.90764572e-02 -1.08936571e-01 -3.39534953e-02  3.49137522e-02\n",
      " -6.15843534e-02  2.59175356e-02 -2.81201415e-02  1.08013768e-02\n",
      " -1.73878558e-02  2.67288871e-02 -2.34947484e-02  2.96678767e-03\n",
      " -2.82620490e-02 -2.48976741e-02  1.65644735e-02  2.14388575e-02\n",
      "  1.52718474e-03  3.17372265e-03 -4.27877996e-03  4.28946726e-02\n",
      " -1.17889727e-02 -3.11002117e-02 -6.02140045e-03  2.06835312e-03\n",
      "  4.90759313e-02 -8.38066684e-04  6.47955984e-02 -4.34059277e-02\n",
      " -2.65744496e-02  1.88361034e-02  3.14421132e-02  5.92746660e-02\n",
      "  8.93925428e-02 -2.23949477e-02  2.78147552e-02 -3.42483930e-02\n",
      "  9.37148836e-03 -1.42542943e-02  5.03243469e-02  3.32843661e-02\n",
      " -5.95628843e-03 -1.46273589e-02  8.59863535e-02 -2.77939234e-02\n",
      " -4.89724055e-03 -4.69218418e-02  4.77389917e-02  1.38374651e-02\n",
      " -3.19210142e-02  1.46269593e-02 -3.52768274e-03  1.70005020e-03\n",
      " -3.44218239e-02 -1.23978054e-04  7.25264428e-03 -3.64128835e-02\n",
      " -9.05740634e-03 -5.36335558e-02  3.21268104e-02 -1.24498922e-02\n",
      "  4.39323224e-02  1.21256420e-02 -1.52653810e-02 -4.27581817e-02\n",
      " -3.30331437e-02  5.76680638e-02  4.81416620e-02  4.29579727e-02\n",
      " -3.85161564e-02  5.49577512e-02  6.81387633e-03 -2.85678376e-02\n",
      " -5.08067608e-02 -2.58135777e-02  1.19096352e-04 -1.08942380e-02\n",
      " -3.02506201e-02  1.37060165e-01  2.53276769e-02  1.67166274e-02\n",
      " -2.46103927e-02 -3.35790478e-02  7.36371875e-02  5.35343308e-03\n",
      " -2.56847497e-02 -1.15197282e-02 -4.46400084e-02  1.97605044e-02\n",
      "  4.96246926e-02  8.75270925e-03  2.29254533e-02 -8.97951797e-03\n",
      "  6.73791906e-03 -6.23706281e-02  6.64516985e-02 -1.26701510e-02\n",
      " -7.39416778e-02 -3.11292317e-02 -1.94747001e-02 -3.53464857e-02\n",
      " -3.44317406e-02  5.70715219e-02 -2.81001311e-02  6.04563095e-02\n",
      "  9.95491818e-03  2.53261141e-02  1.70766655e-02 -2.90926080e-02\n",
      "  3.71675082e-02 -2.30671670e-02  1.65396165e-02 -3.47185438e-03\n",
      " -3.83209880e-03 -6.45110989e-03 -1.76797174e-02  2.24383604e-02\n",
      " -6.25337660e-02 -5.08883744e-02  3.26640904e-03 -3.73724550e-02\n",
      " -2.22620973e-03  7.02750161e-02  1.80380582e-33 -2.46157944e-02\n",
      "  2.96633616e-02 -4.90065813e-02 -4.63024825e-02  4.32886332e-02\n",
      "  1.21397628e-02 -3.04098353e-02 -7.60980370e-03  1.67999845e-02\n",
      " -1.88249145e-02 -7.94803724e-03  2.51319446e-02 -9.98358149e-03\n",
      "  9.61799696e-02  1.44412834e-02 -2.71646045e-02 -3.74628417e-02\n",
      "  3.80055122e-02 -1.70058440e-02 -2.28625014e-02 -2.87974961e-02\n",
      " -6.43526092e-02  6.29079491e-02  2.65992433e-03  1.00239394e-02\n",
      " -2.29380764e-02 -3.43021452e-02  1.21368347e-02 -1.36033976e-02\n",
      " -1.53423706e-03  1.49984946e-02 -4.03802376e-03  1.77040547e-02\n",
      " -5.72189838e-02 -2.61936355e-02 -6.48146495e-04 -2.34863739e-02\n",
      " -1.80978626e-02  2.18351427e-02  8.13952535e-02 -1.33310948e-02\n",
      " -2.74529960e-03  1.75661165e-02 -6.60762517e-03 -3.72255780e-02\n",
      "  6.96603348e-03  4.25388217e-02  9.12699383e-03  1.50642749e-02\n",
      " -6.64711744e-02  6.38256669e-02  3.25540826e-02  2.43313499e-02\n",
      " -2.89967265e-02 -2.45356448e-02  2.29389928e-02  1.88524816e-02\n",
      "  1.81427933e-02  5.52712530e-02  1.16127934e-02  3.68593112e-02\n",
      "  2.26230919e-02  2.11016554e-02 -1.75653715e-02  2.14300770e-02\n",
      "  8.82195905e-02  5.56881949e-02 -3.62775405e-04  4.46835011e-02\n",
      " -1.31794978e-02 -2.02224473e-03 -2.73702275e-02  1.81948058e-02\n",
      "  6.57817945e-02  1.21558933e-02  5.26763611e-02 -1.19964108e-02\n",
      "  4.54741083e-02  9.18522030e-02 -4.66426089e-02  1.95570011e-02\n",
      " -4.27304115e-03 -6.05209991e-02 -2.49374676e-02 -9.72746778e-03\n",
      " -3.58191989e-02  3.47406045e-02  3.17637175e-02  1.78962555e-02\n",
      "  3.19614597e-02  7.86358565e-02  7.38144368e-02  5.03782295e-02\n",
      " -2.34239129e-03 -1.17960470e-02 -2.12881397e-02 -4.97400239e-02\n",
      "  1.95843726e-02  4.52700211e-03 -9.74445045e-03  2.73531377e-02\n",
      " -3.60663747e-03 -1.71784926e-02  1.15288014e-03  3.64402905e-02\n",
      "  2.51818523e-02  4.12311032e-03 -9.47685540e-03  1.89576223e-02\n",
      " -3.56488302e-02  1.35556245e-02  1.76661760e-02 -2.77104191e-02\n",
      " -1.56310983e-02 -1.32765388e-02 -1.64837278e-02 -9.77750309e-03\n",
      "  4.09395806e-02  2.80736294e-02 -1.92174583e-03  3.32036614e-02\n",
      "  7.17339478e-03  3.09875738e-02  2.45732535e-02 -1.40837263e-02\n",
      "  4.60710563e-02 -7.71707594e-02  2.81387911e-04  1.53056346e-02\n",
      " -1.17439069e-02 -3.60022448e-02 -7.44421268e-03  2.01699464e-03\n",
      " -1.05232028e-02 -1.30388951e-02 -9.43030044e-03  9.14998911e-03\n",
      " -4.91609685e-02 -1.31351622e-02 -2.54385136e-02  3.78920995e-02\n",
      "  5.27274758e-02  5.30224927e-02 -2.73693725e-02 -5.83846346e-02\n",
      " -8.35748448e-04 -4.08380367e-02 -2.20838301e-02 -2.47002067e-03\n",
      " -2.95620896e-02 -3.98498215e-02 -2.95594335e-02 -2.97221858e-02\n",
      "  2.82623898e-03  3.26529518e-02 -1.57589675e-03 -2.52484027e-02\n",
      "  6.99253827e-02 -9.71484650e-03  7.38361198e-03  8.39954056e-03\n",
      "  2.65689529e-02 -1.41871525e-02  3.29822302e-02  2.78689489e-02\n",
      "  2.02164915e-03  4.30212868e-03  2.08594860e-03 -3.01189418e-03\n",
      "  3.47046852e-02 -5.15272878e-02  6.09128457e-03  5.30976504e-02\n",
      "  1.06535982e-02  4.68575545e-02 -5.24881706e-02  9.48055461e-03\n",
      "  2.48008762e-02  6.88202586e-03 -4.60824966e-02 -3.68830049e-03\n",
      " -1.65157523e-02 -5.96723706e-02 -4.90686707e-02  7.61382794e-03\n",
      " -2.66868658e-02  3.87521349e-02  1.21596539e-02 -4.41061296e-02\n",
      "  4.19057496e-02  2.33071800e-02  2.53892355e-02  1.82197113e-02\n",
      " -3.38201523e-02  5.17597310e-02 -4.46128361e-02  4.04000990e-02\n",
      "  3.54135633e-02 -5.19629382e-02 -1.64290220e-02 -6.14326121e-03\n",
      " -2.68836506e-02 -1.68111396e-03 -3.88330407e-02 -3.60925607e-02\n",
      " -1.16256624e-02 -1.67781289e-03  4.99083437e-02  8.89629312e-03\n",
      "  2.00272016e-02  4.78492398e-03  5.97883500e-02 -2.00939961e-02\n",
      "  9.85680986e-03 -3.74296792e-02 -2.83320192e-02  4.62172814e-02\n",
      "  9.54717398e-03  1.05645845e-03  4.71724644e-02 -3.03796176e-02\n",
      " -4.12398055e-02  2.59161387e-02 -5.53934947e-02 -1.23973573e-02\n",
      " -2.62891334e-02 -2.46955510e-02  5.47237322e-03 -2.29579695e-02\n",
      " -8.24911986e-03  3.26612405e-02 -1.32661639e-03 -3.25948559e-02\n",
      " -6.83793938e-03  9.00836010e-03  3.61583903e-02 -4.52284003e-03\n",
      "  5.26114069e-02  2.09962483e-02  5.12761548e-02  3.68080325e-02\n",
      "  5.75975794e-03  1.03558628e-02 -1.79288462e-02 -4.38263938e-02\n",
      " -1.49073275e-02 -5.21189198e-02  8.09496716e-02 -5.53296022e-02\n",
      " -1.97728593e-02 -2.89120767e-02 -2.01741401e-02 -4.53249700e-02\n",
      "  4.56787348e-02 -8.52142554e-03  3.39370854e-02  6.19854182e-02\n",
      " -1.49066225e-02 -5.07898219e-02  1.49201248e-02 -1.74803957e-02\n",
      "  5.09076081e-02  1.22595401e-02 -2.61788499e-02 -1.04630617e-02\n",
      " -3.30829173e-02  8.94264784e-04  3.43963541e-02 -1.12481331e-02\n",
      "  9.66506898e-02  3.80663946e-02 -5.51547809e-03 -6.65431900e-04]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding: [-2.15228982e-02 -7.79802073e-03  9.25099012e-04 -8.50584731e-03\n",
      "  5.22884307e-03  3.85090448e-02 -4.49492745e-02  6.06165901e-02\n",
      " -2.77314056e-02 -1.29296072e-02 -1.68407417e-03 -6.83187228e-03\n",
      " -7.65867950e-03 -6.64654188e-03 -2.00638175e-02  3.87779400e-02\n",
      "  2.86662076e-02 -5.51781990e-03 -1.22674415e-02 -8.34266655e-03\n",
      " -6.48086937e-03 -2.40233131e-02  2.29458362e-02 -2.44883820e-02\n",
      " -2.05650106e-02  1.66674443e-02  7.21327737e-02  4.76790033e-02\n",
      " -2.81534735e-02  2.77372357e-02 -2.43528765e-02  4.89520505e-02\n",
      " -7.03037996e-03  1.64505206e-02  3.87898535e-02  2.32622176e-02\n",
      "  4.35343385e-02  1.20665366e-02 -1.42591121e-02 -1.50841838e-02\n",
      " -8.29565302e-02  9.12268832e-02 -1.90074556e-02  3.79963522e-03\n",
      "  1.63399484e-02  3.65550742e-02 -1.45577854e-02 -5.18743470e-02\n",
      " -3.24401297e-02  1.81982592e-02  7.85926636e-03  2.16432326e-02\n",
      " -4.97880168e-02 -3.61221679e-03 -8.56438559e-03 -1.03401374e-02\n",
      "  1.56821441e-02 -2.75810268e-02 -7.16333743e-03 -2.63017379e-02\n",
      "  2.79277060e-02 -2.71650348e-02 -5.32178767e-02  6.29210903e-04\n",
      "  1.08092651e-01  1.39153395e-02 -1.70661211e-02  4.38998900e-02\n",
      " -1.65359285e-02 -6.67736679e-02  5.87043539e-03 -4.08510603e-02\n",
      " -2.11900324e-02 -7.42749274e-02  3.41704674e-02 -7.69784749e-02\n",
      " -2.67933272e-02 -5.77680431e-02 -3.03256698e-02  1.41533343e-02\n",
      "  3.05690169e-02  4.23202477e-02 -5.77048361e-02  1.82859898e-02\n",
      "  7.29770139e-02 -7.90171791e-03  2.70543173e-02 -5.23664430e-02\n",
      " -2.88359970e-02  1.62492786e-02 -2.87909713e-02  1.40715353e-02\n",
      "  3.25558819e-02 -5.90184778e-02 -7.95009825e-03  2.42011733e-02\n",
      "  1.19027030e-02  2.36515626e-02 -1.82428081e-02 -1.71578974e-02\n",
      "  3.23837325e-02  2.70667449e-02  3.80730093e-03  2.97397492e-03\n",
      " -1.01050166e-02  6.14312254e-02 -2.95424648e-02 -2.06969455e-02\n",
      "  3.72565538e-02  6.88353367e-03  2.45685671e-02 -5.97646534e-02\n",
      "  2.93044839e-02 -5.21380603e-02  2.97159515e-02 -9.31652412e-02\n",
      "  3.93321216e-02 -4.78455052e-03  2.51197312e-02  5.04884636e-03\n",
      " -3.60987969e-02 -4.44360375e-02 -3.72305997e-02  1.74289364e-02\n",
      " -5.91451265e-02  1.69028938e-02  5.47571480e-02 -3.16726565e-02\n",
      " -5.32997064e-02  2.34307311e-02  4.89776209e-02 -2.35146731e-02\n",
      " -1.91505440e-02 -8.74457508e-02 -3.53405699e-02 -4.94928146e-03\n",
      " -2.81799864e-03  3.28886695e-03 -3.78609076e-02  2.49722507e-02\n",
      "  3.02903913e-02  1.35219013e-02 -1.50381345e-02 -3.52248885e-02\n",
      "  1.71383731e-02  5.30944653e-02 -1.75941829e-02  2.06144098e-02\n",
      "  1.39621301e-02  7.23014921e-02  4.50918404e-03  3.69534120e-02\n",
      " -3.25177722e-02 -1.52178062e-02  6.32713642e-03  4.00047265e-02\n",
      " -2.71663107e-02  1.08832838e-02  2.44987234e-02  8.65017809e-03\n",
      "  1.64620727e-02  4.74339398e-03  3.67232785e-02  4.96446677e-02\n",
      "  3.55654173e-02 -9.24846251e-03  4.04141508e-02  8.93776026e-03\n",
      " -6.94125192e-04 -8.61860055e-04  4.00994904e-02 -2.24048309e-02\n",
      " -5.76792359e-02 -1.21093169e-02 -5.08388579e-02 -3.50042023e-02\n",
      "  1.67205110e-02  1.16062863e-02 -9.73118376e-03  1.70614403e-02\n",
      "  1.10313306e-02  2.59382240e-02 -7.54773095e-02 -4.70512398e-02\n",
      "  4.50869370e-03 -3.29303071e-02  4.64596637e-02 -9.42940451e-03\n",
      " -2.56709214e-02 -9.90570709e-02  6.60109296e-02  4.08833362e-02\n",
      " -4.95852865e-02  1.68002266e-02 -8.46680328e-02 -7.29619116e-02\n",
      "  3.35790142e-02  4.76308214e-03  3.75376344e-02 -5.57547957e-02\n",
      "  1.70999914e-02 -1.16730547e-02  3.41019481e-02  1.68322865e-02\n",
      " -5.26350981e-04  1.89747866e-02 -4.86020334e-02 -3.50383632e-02\n",
      " -4.14872244e-02  1.45470852e-03  1.26351127e-02 -2.65011545e-02\n",
      " -3.41715962e-02  6.93242392e-03 -2.07681842e-02  4.01476100e-02\n",
      "  3.45804431e-02  1.08575337e-02  3.42816412e-02  4.23322655e-02\n",
      " -8.10069311e-03  2.96809785e-02 -9.39209014e-03 -6.63605658e-03\n",
      "  6.60046861e-02  4.29035397e-03 -2.06552688e-02  1.12097748e-02\n",
      " -5.04649021e-02 -4.92263958e-02  1.62704587e-02 -3.22788917e-02\n",
      "  2.61608884e-02  1.95173454e-02  1.29342675e-02  4.85146679e-02\n",
      " -5.42903021e-02  1.44990450e-02  9.99005139e-02 -6.95130676e-02\n",
      " -4.95595112e-03 -1.77037884e-02 -1.37647614e-02 -2.83440258e-02\n",
      " -2.88115293e-02 -5.70283532e-02  5.89221567e-02 -3.86230908e-02\n",
      "  3.35461758e-02 -7.74013996e-03  1.41369393e-02 -1.10670691e-02\n",
      " -4.21974994e-03  4.60349722e-03  5.66767491e-02 -4.88095842e-02\n",
      "  3.58184502e-02 -1.80671234e-02  1.01320811e-01  6.42055795e-02\n",
      "  6.08769478e-03 -1.58803444e-02 -1.42040448e-02  4.06201296e-02\n",
      " -4.84540723e-02 -4.94356491e-02  8.11407994e-03  3.31632607e-02\n",
      "  1.73655315e-03 -1.01410048e-02  2.96979174e-02  4.34657522e-02\n",
      " -1.34893470e-02  4.39849645e-02  2.26651523e-02  1.25344545e-02\n",
      "  1.22435912e-02 -2.05916297e-02 -1.40960496e-02 -2.75447965e-02\n",
      "  5.78651056e-02 -6.68787211e-02  4.98068668e-02  4.29513752e-02\n",
      "  9.85620916e-03  2.97745280e-02 -5.86009696e-02  6.54283464e-02\n",
      "  4.88063209e-02 -2.56016124e-02 -4.42865193e-02 -1.46858180e-02\n",
      "  8.60323384e-03  4.18894775e-02 -1.30106891e-02  2.27113180e-02\n",
      "  3.22510600e-02 -1.31252399e-02  2.10826583e-02 -7.98784196e-02\n",
      " -7.84291252e-02  4.83091325e-02 -1.15547050e-02 -2.18870174e-02\n",
      "  2.72850692e-02  3.70882861e-02  9.74850729e-03 -1.89277641e-02\n",
      "  2.95159277e-02  6.10503852e-02  3.81889269e-02  2.20022239e-02\n",
      " -8.77353922e-03  5.28390557e-02 -9.32123512e-03  2.20449161e-04\n",
      " -2.94645075e-02 -6.19591735e-02  9.71089583e-03 -5.11895828e-02\n",
      " -1.22394795e-02 -6.48608208e-02 -2.21565142e-02 -1.10507766e-02\n",
      " -1.30585488e-02  1.08915372e-02 -3.07445563e-02 -1.19031845e-02\n",
      "  2.09992938e-03 -7.57960603e-04  2.71390267e-02  5.65768443e-02\n",
      " -7.43712708e-02  3.97551991e-02 -6.20402675e-03 -5.52947372e-02\n",
      "  7.59455934e-03  2.46278029e-02 -4.62248847e-02 -2.81296242e-02\n",
      " -3.40412967e-02 -4.32912521e-02 -9.58360732e-03  5.90345170e-03\n",
      "  4.27316837e-02 -3.93412635e-02  2.56451778e-04 -7.79603934e-03\n",
      "  3.33294496e-02  2.86483485e-02 -2.92312484e-02  5.56678288e-02\n",
      "  1.58567633e-02  2.72279996e-02  2.86977664e-02 -3.81967165e-02\n",
      " -1.52876917e-02 -5.32363215e-03  4.07030806e-03 -1.52098213e-03\n",
      " -6.07552156e-02  1.57411005e-02  4.41372730e-02 -3.29356752e-02\n",
      "  4.18707263e-03  6.10776525e-03 -3.54036987e-02  5.82160577e-02\n",
      " -2.42292061e-02 -5.67969419e-02  2.46705562e-02  7.64016509e-02\n",
      " -1.70787666e-02 -8.25189874e-02 -2.99086627e-02  1.98063683e-02\n",
      " -2.88503412e-02  2.27361545e-02  2.93965526e-02  5.02186716e-02\n",
      " -3.07468381e-02 -3.90718430e-02 -2.60850564e-02  4.89783124e-04\n",
      "  2.36902665e-02 -2.07286999e-02  1.46895964e-02 -1.44212767e-02\n",
      "  1.02716144e-02 -2.69169658e-02  4.26815227e-02  2.74504516e-02\n",
      " -2.03525480e-02 -4.55363188e-03 -2.19547399e-03 -2.48928089e-02\n",
      "  4.17505316e-02 -2.15010755e-02  2.02234201e-02 -2.96150465e-02\n",
      "  1.02198878e-02  1.54044926e-02  4.15158309e-02 -1.72114815e-03\n",
      "  6.10230230e-02  2.95865946e-02 -2.41737626e-02 -2.43112054e-02\n",
      "  4.27502170e-02  3.17859873e-02  6.84548989e-02  5.86677678e-02\n",
      " -6.83040079e-03 -5.75580969e-02  7.16388300e-02 -1.79629009e-02\n",
      "  1.90173760e-02  2.67682667e-03  1.31722745e-02  1.91450566e-02\n",
      "  3.33502553e-02  9.32533201e-03 -1.70159899e-02 -4.18030098e-02\n",
      " -1.19661298e-02  4.38202731e-02  4.03314196e-02 -4.10567857e-02\n",
      " -4.77207527e-02 -3.52674648e-02  3.39080091e-03  2.58128438e-02\n",
      "  2.50301827e-02 -5.70061337e-03 -2.94603631e-02 -1.09285815e-02\n",
      " -2.56372266e-03  1.83977969e-02  8.23411420e-02  3.12244985e-02\n",
      " -4.09296006e-02  5.42510450e-02  4.01297994e-02  5.29822428e-03\n",
      " -2.42915358e-02 -3.08636446e-02 -7.65368575e-03 -4.57850937e-03\n",
      " -2.82634180e-02  9.64395031e-02  2.89543420e-02  1.92972310e-02\n",
      " -3.32763381e-02 -2.67283153e-03  7.48563558e-02  6.23942800e-02\n",
      "  5.96133480e-03 -4.64799143e-02 -3.09171025e-02 -2.77158455e-03\n",
      "  1.05406772e-02  1.54179642e-02 -1.44856535e-02  1.63917132e-02\n",
      "  1.67722348e-02 -7.82346502e-02  3.41065205e-03 -3.20805684e-02\n",
      " -4.43537608e-02 -6.45602215e-03 -7.08878338e-02 -5.70817851e-02\n",
      " -2.12429557e-02  2.34312285e-02  2.21958244e-03 -2.11620657e-03\n",
      " -3.05215619e-03  2.36532744e-02  2.54918635e-02  6.90297829e-03\n",
      "  1.75465718e-02  1.25617385e-02  1.19329235e-02 -1.95599999e-02\n",
      " -1.76153313e-02 -6.68683369e-03  8.70821159e-03  1.20763632e-03\n",
      " -3.56985144e-02 -7.65360519e-02  2.22751056e-03 -4.47666161e-02\n",
      " -5.80691639e-03  2.80035883e-02  2.06701167e-33 -3.86263952e-02\n",
      "  1.03980722e-02 -2.25318372e-02 -1.09205907e-02 -6.09315652e-03\n",
      "  2.17080303e-02 -2.52090879e-02  5.23333214e-02  1.69765600e-03\n",
      "  3.49956960e-03 -5.00398502e-03  2.47254390e-02 -8.19996931e-03\n",
      "  6.22178800e-02  3.39485183e-02 -1.66806369e-03 -3.13671529e-02\n",
      "  6.07062615e-02  7.01395283e-03 -1.58467628e-02 -1.31561207e-02\n",
      " -4.78530899e-02  6.53859274e-03 -2.00169142e-02  8.27857032e-02\n",
      " -4.54734042e-02 -7.13099763e-02  2.08531283e-02 -3.04002427e-02\n",
      "  2.69476809e-02 -6.07176078e-03 -3.58041422e-03 -7.44849117e-03\n",
      " -5.26576452e-02 -1.82651244e-02 -4.38513681e-02 -2.11511124e-02\n",
      " -5.12038171e-02  6.68710619e-02  2.58756466e-02 -4.72214445e-03\n",
      " -1.44625120e-02 -6.90015871e-03 -2.49098837e-02 -6.42965436e-02\n",
      " -9.10301413e-03  4.03396450e-02  3.03872898e-02  7.33511662e-03\n",
      "  1.08352127e-02  4.98617031e-02  5.19862734e-02  3.53986621e-02\n",
      " -1.49297277e-02 -2.20196657e-02  5.64400442e-02  4.46461588e-02\n",
      " -3.84628363e-02  7.66947195e-02  1.97097138e-02 -1.89003535e-02\n",
      "  9.06499550e-02  2.10484546e-02 -4.63209487e-03 -3.42289917e-02\n",
      "  9.64953974e-02  6.11828528e-02 -1.90013610e-02  5.07390406e-03\n",
      "  4.95525338e-02  2.97802947e-02 -8.91914591e-03  2.37727761e-02\n",
      "  1.40797468e-02 -1.92913052e-03  1.17767483e-01 -1.32040679e-02\n",
      "  9.90699977e-03  9.45470929e-02 -3.69254872e-02  3.55997533e-02\n",
      "  6.31540641e-03 -7.64536252e-03 -4.15868014e-02 -3.99596430e-03\n",
      "  1.33439520e-04  4.04367521e-02  2.39032861e-02  1.48134008e-02\n",
      "  3.84915024e-02  7.03489035e-02  7.89425001e-02  3.00962757e-02\n",
      " -1.90497682e-04  5.89191094e-02  3.11055351e-02 -5.67665473e-02\n",
      "  3.38916178e-03  1.35418074e-02 -2.90492475e-02  1.93993021e-02\n",
      "  7.96410348e-03  1.83577146e-02  4.42266697e-03 -1.61006546e-03\n",
      " -3.82846855e-02 -4.30544429e-02 -3.59540945e-03  1.16069615e-02\n",
      " -1.89123210e-02 -2.79755741e-02  5.61765544e-02 -3.34076509e-02\n",
      " -5.08807227e-02 -3.25226746e-02  7.83521775e-03 -1.91734023e-02\n",
      "  5.94736636e-02 -2.65357587e-02  4.53172810e-02 -1.80402212e-02\n",
      " -1.07418206e-02  2.63088699e-02 -1.75722428e-02 -9.31730785e-04\n",
      " -4.44799894e-03 -6.34835064e-02  3.49458004e-03  4.54389155e-02\n",
      " -4.12257314e-02 -4.84148152e-02 -1.71956196e-02 -1.80804431e-02\n",
      " -2.63073575e-03 -3.02607846e-02 -3.50830033e-02  4.20266530e-03\n",
      " -3.56156081e-02 -2.81791594e-02 -5.54149318e-03  2.38171499e-02\n",
      "  5.63822910e-02  7.33753890e-02 -2.24820655e-02  4.52477951e-03\n",
      " -3.31595056e-02 -1.97024774e-02 -3.40226144e-02 -3.33411805e-02\n",
      " -3.63468304e-02 -5.13177738e-03 -2.46759839e-02  2.00916063e-02\n",
      "  5.57936449e-03 -3.64194028e-02  1.40958382e-02 -4.64291833e-02\n",
      "  4.92615849e-02 -3.70536856e-02  3.66368244e-04 -2.25139912e-02\n",
      "  2.08797976e-02 -5.81128001e-02  1.50869116e-02 -5.48285805e-03\n",
      "  5.42319205e-04  3.68645117e-02 -2.39754431e-02 -4.31912392e-02\n",
      "  6.71679601e-02 -9.23241749e-02  1.88795831e-02  2.27905717e-02\n",
      "  3.32120247e-02  2.31583063e-02 -1.30487587e-02  1.34720271e-02\n",
      "  8.20852220e-02  1.06502511e-02 -2.23771539e-02  1.47919394e-02\n",
      "  6.35388587e-03 -3.28639783e-02 -1.71262550e-03  5.28082475e-02\n",
      " -8.25087279e-02 -2.32286565e-02  4.23677638e-02 -5.45634292e-02\n",
      "  3.36574391e-02  4.35426645e-02  1.71139184e-02  4.92438599e-02\n",
      " -1.37344263e-02  4.96943519e-02 -5.75561970e-02 -5.40280342e-03\n",
      " -1.26684434e-03 -1.43954083e-02 -2.18227953e-02 -9.99155361e-03\n",
      " -2.90870033e-02 -2.59137340e-02  1.26986895e-02 -3.19436309e-03\n",
      " -1.35016628e-02 -9.75760818e-03  3.63758542e-02 -4.36577760e-03\n",
      " -1.87006854e-02  5.22086769e-02  7.21984580e-02 -2.26308387e-02\n",
      "  2.07442529e-02 -1.39183095e-02 -1.34625938e-03  4.87105846e-02\n",
      "  1.36510516e-03  2.04825699e-02  3.57983895e-02 -2.05948818e-02\n",
      " -1.77338254e-02  4.42667902e-02 -1.00496702e-01 -6.87099900e-03\n",
      "  4.14285576e-03 -2.97053549e-02 -2.44097002e-02 -3.12399417e-02\n",
      "  6.18085731e-03 -9.84526426e-03 -2.07634475e-02 -1.75898504e-02\n",
      "  1.34774763e-02  2.12179348e-02  1.77283946e-03 -2.62890402e-02\n",
      "  4.51455675e-02  3.59200425e-02  4.77867238e-02  2.34301984e-02\n",
      "  3.03890537e-02  1.66282058e-02 -3.77523974e-02 -4.28887196e-02\n",
      "  2.04049051e-02 -4.32218332e-03  1.35475174e-02 -8.86799395e-02\n",
      " -7.46056950e-03 -2.47586723e-02  1.38286119e-02 -5.51470593e-02\n",
      "  3.51191387e-02 -4.21440415e-02  1.75776966e-02  1.46228485e-02\n",
      " -2.37191059e-02 -5.22530526e-02 -5.03631420e-02 -6.08381256e-02\n",
      "  7.92744290e-03  5.90820471e-03 -3.46167013e-02  1.71444453e-02\n",
      " -1.17022386e-02 -2.89281625e-02  1.58888511e-02  2.15968437e-04\n",
      "  9.39113945e-02  8.54471800e-05 -6.95311278e-02  2.00913139e-02]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding: [-1.22164143e-02 -2.13698354e-02 -3.36170569e-03 -3.22685316e-02\n",
      " -2.41235606e-02  3.00755203e-02 -2.04258468e-02  5.20664379e-02\n",
      " -9.10869893e-03 -5.31128310e-02 -3.06588896e-02  3.09261288e-02\n",
      " -2.71168817e-02 -1.64833770e-03 -7.86010101e-02  2.21535508e-02\n",
      "  2.27728542e-02 -7.81786889e-02 -4.53958921e-02 -2.52812151e-02\n",
      " -1.61554608e-02 -1.60243995e-02 -3.33075486e-02 -7.18192533e-02\n",
      " -3.55768874e-02  1.38197836e-04  2.26320922e-02  6.60716090e-03\n",
      "  1.12220477e-02  2.19996572e-02 -1.02163563e-02  6.73651993e-02\n",
      " -6.05756268e-02  1.05325155e-01  4.37143892e-02 -6.48564147e-03\n",
      "  1.81881990e-02 -3.20585780e-02  3.92653234e-02  4.31282399e-03\n",
      " -5.75722754e-02  9.81879607e-02 -2.57260986e-02  1.18376203e-02\n",
      "  3.93918976e-02 -1.16888145e-02 -6.75629377e-02 -6.54780865e-02\n",
      "  4.90322988e-03  9.83750597e-02  1.89386085e-02 -1.14147160e-02\n",
      " -7.27443099e-02 -3.52954455e-02  9.96270776e-03 -2.42876913e-02\n",
      "  3.81819531e-02 -1.42911833e-03  4.06645015e-02 -1.83045324e-02\n",
      " -1.33975782e-02  4.29213382e-02  4.80985548e-03 -2.41242517e-02\n",
      "  1.01373367e-01 -2.90144961e-02  2.17544362e-02  1.14272023e-02\n",
      "  2.92580388e-02 -5.21350503e-02  4.07293625e-02 -3.20466310e-02\n",
      "  1.98586229e-02 -5.83529808e-02  7.97565747e-03 -7.88913667e-02\n",
      " -5.17887920e-02 -4.09288108e-02 -5.36423512e-02  6.88369013e-03\n",
      "  1.09867454e-02  5.52038364e-02 -7.96607658e-02 -5.10764159e-02\n",
      "  2.41172053e-02  6.69669127e-03  3.60606313e-02  9.01244394e-03\n",
      " -1.74744669e-02  1.65497568e-02 -1.93523336e-02  4.45868224e-02\n",
      "  1.71294417e-02 -1.90815143e-02 -1.80165451e-02  9.49393865e-03\n",
      "  1.03442073e-02 -7.52062574e-02 -3.78942341e-02  2.71298476e-02\n",
      "  4.29151282e-02  6.11749943e-03  7.11465254e-02 -3.59038524e-02\n",
      " -3.41157056e-02  4.49460112e-02  1.27722267e-02 -3.50714698e-02\n",
      "  2.61237361e-02 -1.09268390e-02  1.43386200e-02 -5.65083474e-02\n",
      "  1.30678630e-02 -1.59905069e-02  1.12654800e-02 -2.54021809e-02\n",
      "  8.02352838e-03  4.37819138e-02  3.98928672e-02  1.32178841e-02\n",
      " -5.44119030e-02 -5.86248077e-02  1.59983914e-02 -4.88557331e-02\n",
      " -4.30398136e-02  2.12666970e-02 -1.37660708e-02 -2.85869348e-03\n",
      " -9.37073976e-02  3.51297110e-02 -7.13547226e-03 -4.39215191e-02\n",
      " -1.77440699e-02 -4.28103982e-03 -4.58650803e-03 -3.72301340e-02\n",
      "  2.47469768e-02  6.85516521e-02 -6.21693544e-02 -7.35389628e-03\n",
      " -3.71867861e-03  2.04148497e-02  3.42106819e-02 -6.96840603e-03\n",
      " -3.74106667e-03  4.31439765e-02  1.79017894e-02 -4.04813420e-03\n",
      "  7.34758680e-04 -6.75009447e-04  1.16878264e-02  1.10743726e-02\n",
      "  3.34323421e-02 -6.68463632e-02  2.71700583e-02  3.84191680e-03\n",
      " -4.49036919e-02  6.27337098e-02 -1.31522212e-02 -2.23288704e-02\n",
      " -3.67486267e-03 -3.42806317e-02  4.38310653e-02 -3.40693407e-02\n",
      "  5.25650568e-03  2.45336294e-02  3.39337848e-02  4.05368023e-02\n",
      "  1.27802351e-02  5.28851058e-03 -1.74890570e-02  2.11984441e-02\n",
      " -2.23777462e-02  1.01537863e-03 -7.75141492e-02  3.26117640e-03\n",
      " -2.50199270e-02  3.34013775e-02  7.24781817e-03  9.72881273e-04\n",
      "  1.24598120e-03  1.43187605e-02 -4.73273546e-02  1.14977658e-02\n",
      " -2.92196646e-02  1.26599502e-02  8.77868291e-03  1.22458506e-02\n",
      "  1.74610168e-02 -5.94886392e-02  5.64167649e-02  6.17909152e-03\n",
      " -7.16853142e-02  2.85265176e-03  1.87976975e-02  2.98468098e-02\n",
      " -1.55859925e-02 -1.09791756e-02 -1.08146491e-02 -5.93945459e-02\n",
      "  3.62779126e-02 -6.40274435e-02  1.47659946e-02  5.25501557e-02\n",
      "  1.74148940e-02 -1.79984327e-02 -1.62573140e-02 -4.20104377e-02\n",
      " -1.22859608e-02 -3.52696031e-02  7.93191604e-03 -7.62610212e-02\n",
      "  2.12845905e-03  2.06637988e-03 -5.01622073e-03  5.07789366e-02\n",
      "  3.97171341e-02  1.95173733e-02  6.30217493e-02 -4.15246497e-04\n",
      "  3.89896557e-02 -7.60139618e-03 -1.20174279e-02 -1.39655685e-02\n",
      "  3.55325118e-02 -6.83424179e-04 -1.59205534e-02  1.90931037e-02\n",
      "  1.52063835e-02 -7.89900869e-02  9.59298108e-03  4.02888320e-02\n",
      "  4.62102853e-02 -3.15587502e-03 -3.90835153e-03  4.70784716e-02\n",
      "  6.92800991e-03  1.02967536e-02  8.64409953e-02 -6.14721933e-03\n",
      "  4.58153598e-02 -7.33062392e-03 -2.71873374e-04 -2.34046467e-02\n",
      "  2.92513650e-02 -3.58601213e-02  5.37800156e-02 -1.93161089e-02\n",
      "  3.73144671e-02 -4.49774135e-03 -6.49123592e-03 -4.54459600e-02\n",
      "  3.38811404e-03  4.03590640e-03  1.01454407e-02 -1.95511840e-02\n",
      " -1.05034979e-02  1.52459173e-02 -3.81945670e-02  4.29573804e-02\n",
      "  3.61978053e-03 -4.35573943e-02 -4.39289287e-02  9.38897356e-02\n",
      " -6.19876087e-02 -1.04596214e-02  4.28161398e-03  1.17470678e-02\n",
      "  4.99061448e-03 -1.22798467e-02  5.68797160e-03  5.38440496e-02\n",
      " -1.63445473e-02  2.97678616e-02  1.65455006e-02 -1.21843684e-02\n",
      "  7.10414676e-03  3.19588333e-02 -1.51297739e-02  3.53990234e-02\n",
      "  5.16489409e-02 -4.42692712e-02  2.34562382e-02  2.79628504e-02\n",
      "  1.71405673e-02 -4.60046227e-04 -1.29158255e-02  4.44831438e-02\n",
      "  4.82987687e-02 -7.61514949e-03 -4.10700068e-02 -3.79312374e-02\n",
      " -2.05529258e-02  2.38438509e-03  6.24959283e-02 -2.64068991e-02\n",
      " -2.36876949e-04 -1.79177504e-02  1.33443996e-02 -3.06371283e-02\n",
      " -8.41427073e-02 -5.25661092e-03 -2.15982236e-02  3.27200703e-02\n",
      "  2.84425560e-02 -7.35473447e-03  2.88453624e-02 -7.49379164e-03\n",
      " -1.78212784e-02 -1.40953409e-02  4.46038842e-02 -3.95580344e-02\n",
      "  1.66229960e-02  3.08321230e-02 -2.76660025e-02  4.49127937e-03\n",
      " -6.02306575e-02  3.04986592e-02  4.11146395e-02  6.96263369e-03\n",
      " -2.65229177e-02 -3.50042582e-02 -5.58389947e-02 -2.29872204e-02\n",
      "  1.08738178e-02 -2.19426001e-03 -9.42736305e-03  5.53239509e-03\n",
      " -1.38010858e-02  2.10793614e-02  4.13151737e-03 -8.70909821e-03\n",
      " -6.03573434e-02  1.17390864e-01 -2.14535389e-02 -3.79899540e-03\n",
      "  7.67155141e-02  2.26545390e-02 -5.93619607e-02 -1.16131350e-03\n",
      " -2.97306571e-02 -9.39253904e-03 -4.07408699e-02  8.57337266e-02\n",
      " -4.20205519e-02 -6.29779473e-02 -2.35068705e-02 -2.18479875e-02\n",
      " -2.05521341e-02 -2.71465722e-02 -9.58782062e-03  6.36660904e-02\n",
      "  1.52647803e-02  4.19336446e-02  5.15136495e-02 -1.63780227e-02\n",
      " -1.05248857e-02  5.36684096e-02  3.31993699e-02  2.88626552e-02\n",
      "  4.27957214e-02  2.85309888e-02  8.34287424e-03 -1.76903885e-02\n",
      "  6.69285879e-02 -4.98739518e-02 -6.71996847e-02 -1.03315036e-03\n",
      " -4.12875228e-02 -5.46945855e-02  6.78889379e-02  6.14245236e-02\n",
      "  2.74481401e-02 -5.09627275e-02 -5.29168807e-02  4.86863106e-02\n",
      "  1.35616148e-02  1.73227323e-04  2.89344112e-03  2.25522654e-05\n",
      " -7.23234117e-02  6.51353272e-03 -2.18479186e-02 -3.15575581e-03\n",
      "  5.81254512e-02 -1.16037382e-02  3.08346841e-02  6.73101400e-04\n",
      " -1.04043158e-02 -2.16118153e-02 -5.06330393e-02  4.34187911e-02\n",
      "  2.63047451e-03 -3.06619401e-03  2.84230366e-04  1.98841356e-02\n",
      "  2.60986798e-02  1.63624920e-02 -2.32910328e-02  3.06294542e-02\n",
      "  1.58752054e-02 -5.82722202e-02  6.22080974e-02  9.29626375e-02\n",
      "  1.57141127e-02 -2.54178215e-02 -4.33426239e-02 -4.83524539e-02\n",
      "  7.42995832e-03  6.63533481e-03  3.80434841e-02  2.30913907e-02\n",
      "  2.86016110e-02 -9.27383080e-03  3.53168696e-02 -6.21482283e-02\n",
      "  2.57540885e-02 -1.52124220e-03  6.12052623e-03 -9.60376987e-04\n",
      "  8.66786297e-03 -1.71348304e-02  3.44652496e-02  9.09447484e-03\n",
      "  1.21925548e-02 -2.12888885e-02  8.19058716e-03 -1.20529411e-02\n",
      " -5.00703678e-02 -5.69866821e-02  3.21256407e-02 -1.91880520e-02\n",
      "  5.91764711e-02  8.60036258e-03 -1.09348707e-02 -4.37611341e-02\n",
      " -2.57801302e-02  3.20603512e-02 -1.00977477e-02  4.64921258e-02\n",
      " -2.06920598e-03  4.93489504e-02  1.69440228e-02 -2.49490254e-02\n",
      " -1.24202278e-02 -4.92830724e-02 -3.11362352e-02  1.10380389e-02\n",
      " -3.90875824e-02  1.29109666e-01  1.81439053e-02 -5.85707277e-02\n",
      "  1.33008708e-03  2.78328210e-02  4.29810025e-02 -1.56340729e-02\n",
      " -4.09074239e-02 -3.03135291e-02  1.85556512e-03 -3.07139810e-02\n",
      "  6.72171041e-02  2.81571243e-02 -3.11283190e-02 -7.97195360e-03\n",
      "  7.18796486e-03 -6.88957572e-02  2.72047799e-02 -2.80911084e-02\n",
      " -6.54577790e-03 -2.12920289e-02 -9.76123288e-03 -4.99457978e-02\n",
      "  6.51107430e-02  2.51633581e-02 -1.42327920e-02 -1.92829985e-02\n",
      " -3.03968638e-02  9.71561577e-03  2.93186400e-02 -1.11449361e-02\n",
      " -3.00777610e-03 -6.15728125e-02  3.44085954e-02 -1.14368228e-02\n",
      "  9.11082886e-03  5.23227686e-03  4.12273332e-02  4.03453745e-02\n",
      " -5.35161942e-02 -4.89423312e-02 -1.50131974e-02  2.20726547e-03\n",
      "  3.79190557e-02  5.49071468e-02  1.96146900e-33 -2.19645742e-02\n",
      "  4.16761125e-03 -1.10492259e-02  3.56590748e-02  6.23922143e-03\n",
      " -1.32971928e-02  3.37554850e-02  7.63571705e-04  4.20450568e-02\n",
      "  6.39708759e-03  1.96152590e-02  3.62785608e-02 -4.54359362e-03\n",
      "  9.66309104e-03 -9.64657683e-03 -4.56818826e-02 -3.98486704e-02\n",
      "  5.30101471e-02 -4.36068326e-03  9.73617099e-03  4.59568226e-04\n",
      " -2.32505463e-02  7.46989623e-02 -7.88044557e-03 -3.84603185e-03\n",
      " -3.75637598e-03 -8.72893911e-03 -3.20462547e-02  7.66592752e-03\n",
      "  1.65601205e-02 -2.92172446e-03  8.99545103e-03 -5.87034458e-03\n",
      " -2.70173065e-02 -2.50395276e-02 -2.68910471e-02 -4.78132889e-02\n",
      " -9.69189126e-03  3.06099653e-02  6.37396872e-02  3.22448313e-02\n",
      "  4.34311815e-02  4.59276838e-03 -1.65440794e-02 -3.82582508e-02\n",
      "  8.31083767e-03 -2.46847104e-02  3.15137990e-02 -4.67736740e-03\n",
      " -6.99140597e-03 -2.44365353e-02  3.37581113e-02  4.36644368e-02\n",
      " -3.52899656e-02 -3.81676033e-02  5.13059348e-02  4.05260995e-02\n",
      "  2.72041820e-02  6.39677942e-02  7.23076984e-02  3.47029045e-02\n",
      " -9.99074988e-03  2.91952994e-02 -2.48727463e-02 -1.09459609e-01\n",
      "  3.61446664e-02  6.52754307e-02  1.90428877e-03 -2.06614900e-02\n",
      "  3.19191478e-02 -3.37370299e-02  1.39720412e-02  5.72276348e-03\n",
      " -7.41463061e-03  6.34118170e-02  7.91258365e-02 -4.82490141e-04\n",
      " -1.61614490e-03  2.10266933e-02 -2.67816689e-02  1.56706125e-02\n",
      " -1.97279416e-02 -2.15714909e-02  4.06080782e-02  1.98916905e-02\n",
      " -3.26035917e-02  4.89075035e-02  6.01660609e-02  7.21849278e-02\n",
      "  4.01844271e-02  1.12890147e-01 -9.10681579e-03 -1.07979365e-02\n",
      "  9.58714445e-05 -3.57160419e-02  1.89743675e-02 -6.07034266e-02\n",
      " -3.92846949e-02 -1.86862499e-02 -1.19475890e-02 -8.54716916e-03\n",
      "  1.20472105e-03  1.53368227e-02 -2.05733851e-02 -1.12162251e-02\n",
      "  2.16416339e-03  4.05089073e-02  9.20544751e-03  1.82919297e-02\n",
      " -5.48366308e-02 -1.08562158e-02  4.30798195e-02 -5.41272201e-02\n",
      " -3.94558087e-02  4.23342548e-03 -9.77823045e-03 -5.00141419e-02\n",
      " -9.82264336e-03  2.15956010e-02  2.58817757e-03 -1.77251901e-02\n",
      "  2.47332063e-02  4.67579626e-02  6.77175745e-02 -2.26824861e-02\n",
      "  2.78415028e-02 -4.15456556e-02  7.33882040e-02  2.03456748e-02\n",
      "  2.55577378e-02 -1.26612894e-02 -1.02006048e-02 -7.58157810e-03\n",
      "  1.49521390e-02 -2.60276385e-02 -7.87929259e-03  2.07499526e-02\n",
      "  1.67159792e-02  1.16993459e-02 -5.34797600e-03 -5.18575683e-03\n",
      "  2.04152595e-02  3.71050788e-03  3.11703999e-02 -3.51871401e-02\n",
      " -6.96811778e-03 -6.39502406e-02  9.49779991e-03 -3.01080644e-02\n",
      "  1.76533200e-02  1.10145900e-02  2.57795565e-02  2.48333905e-02\n",
      "  1.96341760e-02  1.77973453e-02  8.53687059e-03 -5.72359338e-02\n",
      "  7.87549168e-02 -3.27238883e-03 -3.59622613e-02 -1.12784309e-02\n",
      " -1.90719275e-03 -2.46230140e-02 -4.02159207e-02 -5.35711087e-03\n",
      "  5.21604717e-02  4.32058014e-02  4.64817509e-02  1.33563252e-02\n",
      "  3.75774391e-02 -1.96694545e-02 -1.74744260e-02  1.42601097e-03\n",
      "  5.56794107e-02  4.01503183e-02 -4.06827889e-02  3.66287865e-02\n",
      "  4.04857211e-02 -5.32001965e-02 -8.40041637e-02  2.64976174e-03\n",
      " -5.58047965e-02 -3.93060502e-03 -2.11393423e-02  3.37587819e-02\n",
      " -2.62645055e-02  2.10639425e-02 -2.20663305e-02 -2.53170151e-02\n",
      " -2.25023925e-02 -1.14467926e-02  2.16869339e-02 -1.54950200e-02\n",
      " -7.29033817e-03  4.79715178e-03 -6.44843802e-02  1.62935480e-02\n",
      "  1.12122046e-02 -3.21637802e-02 -4.01290283e-02 -5.77260144e-02\n",
      "  5.12447767e-02  2.57027689e-02 -1.99324097e-02 -4.97193588e-03\n",
      " -2.35844050e-02 -1.07710240e-02 -3.70196812e-02 -4.71807271e-02\n",
      " -4.33887243e-02  4.61778510e-03  5.79151250e-02 -3.10254842e-02\n",
      "  1.33855622e-02  4.82191594e-04 -4.50102729e-04  5.45415394e-02\n",
      "  2.28798985e-02  5.44311618e-03  2.62301490e-02 -5.05142435e-02\n",
      "  8.23888422e-06  1.39496708e-02 -3.37011479e-02 -3.17831188e-02\n",
      " -5.08098416e-02 -6.02801666e-02  5.87724261e-02  5.44314214e-04\n",
      "  3.07394145e-03 -1.95792090e-04  2.89458539e-02 -4.23617512e-02\n",
      " -1.73597001e-02 -1.28306188e-02  6.98157726e-03  2.64882036e-02\n",
      "  3.28202732e-02  9.47437659e-02 -4.47073113e-03  1.17253743e-01\n",
      "  3.39710750e-02  3.00531331e-02 -7.27051031e-03 -4.39194851e-02\n",
      "  8.41725245e-03 -7.51756318e-03  2.26023775e-02 -8.73232819e-03\n",
      "  2.36799195e-02 -1.39164068e-02 -1.98966339e-02 -8.87863524e-03\n",
      " -2.24909261e-02 -8.76107905e-03  5.82054816e-02  3.85198817e-02\n",
      " -4.78317142e-02 -4.73025702e-02 -4.50039282e-02 -4.65447865e-02\n",
      "  3.02283466e-02 -6.18410064e-03 -1.10683450e-03 -3.51431742e-02\n",
      " -1.81022454e-02  2.76126135e-02  8.27683788e-03  8.99322052e-03\n",
      "  9.22542363e-02 -6.15586378e-02 -6.03839271e-02 -3.24200466e-02]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding: [-1.27362064e-03 -1.54341934e-02 -8.66567064e-03 -1.83742475e-02\n",
      " -5.69552891e-02 -4.16346528e-02 -5.75635172e-02 -6.95571536e-03\n",
      " -4.35472317e-02 -1.01444386e-01 -1.86654720e-02  6.85101887e-03\n",
      "  3.06317341e-02  5.66019639e-02 -6.20234851e-03  5.37179187e-02\n",
      " -7.50226527e-03 -4.65109199e-02 -1.75817013e-02 -1.07620265e-02\n",
      " -2.84414701e-02 -4.90557961e-03 -5.69019206e-02 -3.53091918e-02\n",
      " -9.12379567e-03  2.42871325e-03  1.14618922e-02  1.19835678e-02\n",
      " -3.03620044e-02  1.37243671e-02 -6.96566328e-02  4.71940264e-03\n",
      " -4.40859422e-02  3.65208462e-02  2.89269499e-02  4.89480942e-02\n",
      "  4.07518484e-02 -4.36624624e-02  1.95373464e-02 -1.62816793e-02\n",
      " -1.12342285e-02 -1.99471344e-03 -3.12269088e-02 -4.72739488e-02\n",
      "  2.23609172e-02 -2.92920065e-03 -5.95299937e-02 -8.84137973e-02\n",
      " -2.40843855e-02  3.87748182e-02 -8.31167959e-03  4.95167300e-02\n",
      " -6.23732358e-02 -9.05024633e-03  4.48902622e-02  1.05241523e-03\n",
      "  1.51566127e-02  5.12214638e-02  5.37964925e-02  1.77612696e-02\n",
      " -6.29934743e-02  3.53279449e-02 -1.73390508e-02 -3.74631360e-02\n",
      "  7.95784220e-02  5.20433187e-02  7.21564377e-03  4.23880666e-03\n",
      " -1.38940581e-03 -2.02069916e-02  3.49659398e-02  1.99903417e-02\n",
      " -2.53464263e-02 -2.93021034e-02  5.77128232e-02 -5.62125109e-02\n",
      " -3.69553417e-02 -6.34371713e-02 -5.08899726e-02 -1.70162246e-02\n",
      " -1.66935902e-02 -6.45801499e-02 -7.30899945e-02 -4.70271073e-02\n",
      "  9.49142908e-04  1.33875478e-02  2.39530746e-02  3.15555115e-03\n",
      " -2.96470486e-02 -1.38123259e-02 -4.16273028e-02  1.37374382e-02\n",
      "  5.08991331e-02 -9.41795576e-03  3.63903530e-02 -5.69861196e-02\n",
      "  3.56107056e-02 -8.13319907e-02 -8.39568488e-03  1.76614779e-03\n",
      "  5.14752083e-02  4.55307923e-02  2.92409603e-02  1.86265782e-02\n",
      "  7.38073932e-03  4.27583084e-02 -4.92686266e-03 -7.43663013e-02\n",
      "  3.24523821e-02  1.92142874e-02 -5.25369868e-03 -5.68218715e-02\n",
      "  1.77262276e-02  5.82253421e-03 -3.00542358e-03 -5.00561632e-02\n",
      " -2.93329693e-02 -2.41048425e-03 -2.94579007e-03  2.80014873e-02\n",
      " -3.24892588e-02 -1.27681224e-02 -2.55539734e-02 -4.07880060e-02\n",
      " -2.68235318e-02 -2.68613044e-02  3.86876194e-03 -2.12315260e-03\n",
      " -4.98712435e-02  2.28559058e-02 -3.62256132e-02 -3.75633426e-02\n",
      " -2.04534791e-02  2.92124003e-02 -7.72747351e-03  1.27874571e-03\n",
      "  8.96972325e-03  2.43819840e-02 -5.76097704e-02 -1.80398673e-02\n",
      " -9.82231554e-03 -1.08060529e-02  4.40571979e-02 -1.60764027e-02\n",
      "  2.93245213e-03  2.72541009e-02  2.06259433e-02  2.35220734e-02\n",
      "  1.09668577e-03 -1.63700061e-05  5.01896604e-04 -3.72250751e-02\n",
      "  2.43283971e-03 -3.73724848e-02 -1.23432055e-02  5.71158379e-02\n",
      " -2.06795079e-03  3.30931246e-02  6.42836690e-02 -2.40922682e-02\n",
      "  1.36497458e-02 -4.72871251e-02 -2.28622388e-02  2.55047297e-03\n",
      " -2.46368884e-03 -2.65180953e-02  3.74127366e-02  1.35779679e-02\n",
      "  3.23007628e-03  2.75751296e-02  3.50741968e-02  2.03507226e-02\n",
      " -1.56163760e-02  4.32741120e-02  3.04548237e-02  1.82568096e-03\n",
      " -2.51468122e-02  7.52793029e-02  4.36129700e-03  2.72825593e-03\n",
      "  1.38339093e-02 -6.59173308e-03 -5.86360507e-02  3.91121656e-02\n",
      " -1.52467275e-02 -3.52510856e-03 -2.66035348e-02  2.74840705e-02\n",
      "  6.16145581e-02 -1.76431611e-02 -1.50416954e-03  6.02777824e-02\n",
      " -2.06826534e-02 -1.31394463e-02  2.66498793e-02 -3.24712545e-02\n",
      " -2.21466348e-02 -4.11989428e-02  9.90800094e-03  1.96388997e-02\n",
      "  5.61811887e-02 -3.94120105e-02  1.08117824e-02  3.09972800e-02\n",
      "  5.36297560e-02  1.75601486e-02  1.83835197e-02 -7.25810826e-02\n",
      " -4.78347763e-02  1.93291402e-03 -3.11306175e-02 -4.79600057e-02\n",
      "  1.65493041e-02 -2.63989773e-02  9.98059101e-03  4.17170674e-02\n",
      "  1.99524686e-02  4.98292558e-02  8.88863355e-02  4.42877784e-02\n",
      "  5.29035134e-03  2.82974318e-02 -5.05881459e-02 -2.09721550e-02\n",
      " -3.56762409e-02  2.15620045e-02 -6.64992677e-03 -9.93970502e-03\n",
      "  9.60838050e-03 -4.81883809e-02  2.63234526e-02  1.66827347e-02\n",
      "  4.61210124e-02  4.79878411e-02 -1.44554174e-03  5.63640520e-02\n",
      " -2.16559321e-02 -3.15963179e-02  9.13035423e-02  1.66695975e-02\n",
      " -1.36835072e-02 -1.05257779e-02  3.92308347e-02 -1.25357555e-02\n",
      "  4.25589718e-02 -2.16689017e-02  4.12736535e-02 -3.92410532e-02\n",
      " -6.01732843e-02 -7.45622516e-02 -2.05468573e-02 -4.82865348e-02\n",
      " -2.38446929e-02  2.41863634e-03  1.30868377e-02 -4.77632619e-02\n",
      "  1.88787468e-02  1.74356215e-02 -2.26105060e-02  4.85587008e-02\n",
      "  1.98856518e-02 -2.80565303e-02 -5.62787950e-02  6.57102093e-02\n",
      " -6.37453943e-02 -6.35114312e-02 -4.69829813e-02  7.56338006e-03\n",
      " -6.48957416e-02  4.22970988e-02  2.08776779e-02  2.76161451e-03\n",
      " -8.87909206e-04  1.43224970e-02  3.30637731e-02 -7.45571591e-03\n",
      "  2.44107880e-02  1.02035599e-02 -2.58671287e-02  1.94708556e-02\n",
      "  8.15286711e-02 -2.98610795e-02  5.98778464e-02  2.80829463e-02\n",
      " -2.51866225e-02  8.55933037e-03 -1.85661670e-02  4.11858521e-02\n",
      "  4.05376405e-02 -3.74305956e-02  6.74326345e-03 -1.77999269e-02\n",
      " -2.40240339e-02 -2.42125504e-02  3.34647149e-02 -3.71127762e-02\n",
      " -1.40796723e-02  1.53745839e-03 -2.21663043e-02 -2.18037218e-02\n",
      " -5.86122088e-02 -2.33848188e-02  2.29033064e-02 -7.14422911e-02\n",
      "  4.22133654e-02 -8.10800772e-03  5.33788232e-03  1.16637284e-02\n",
      " -3.99143025e-02 -3.14993188e-02  3.54596935e-02  2.28698999e-02\n",
      " -3.55821811e-02  5.98325506e-02 -5.87501703e-03 -3.36615182e-03\n",
      " -1.48331914e-02  2.93912785e-03  2.51703002e-02 -1.79073159e-02\n",
      " -2.29480267e-02 -2.07362603e-02 -2.99669080e-03  5.10530882e-02\n",
      "  1.89011020e-03  1.75525472e-02  6.18197657e-02  7.88339647e-04\n",
      "  2.66012680e-02  3.43304761e-02 -1.57850441e-02  1.81231147e-03\n",
      " -4.29937877e-02  4.53398302e-02 -4.34956327e-02 -4.36819904e-02\n",
      " -2.82557029e-03  2.71338895e-02  1.73402149e-02 -1.26127787e-02\n",
      " -3.33879441e-02 -2.08824649e-02 -2.64400784e-02 -8.70595314e-03\n",
      "  2.63013020e-02 -2.13165190e-02 -2.48070769e-02  6.53882418e-03\n",
      "  1.02486387e-02 -1.06097199e-02 -5.02407588e-02  2.66965758e-02\n",
      " -1.69663448e-02  4.02012877e-02  3.61632854e-02 -1.58026796e-02\n",
      " -1.88067928e-03  3.10005210e-02 -1.22341292e-03  1.11500975e-02\n",
      "  2.58466881e-02  3.62872854e-02  4.44972888e-02  2.87195621e-03\n",
      "  3.56289707e-02 -3.27184759e-02 -8.88136476e-02  2.47899313e-02\n",
      " -2.87285037e-02 -5.38368570e-03  7.90464878e-03  5.05467132e-02\n",
      "  3.50590446e-04 -8.32358301e-02  1.03852269e-03  1.85216609e-02\n",
      "  4.89885956e-02  3.52009237e-02 -5.05210832e-02 -1.80323012e-02\n",
      " -6.75277086e-03  7.22748367e-03 -4.42312621e-02  5.35137355e-02\n",
      "  6.87053055e-02 -2.10968088e-02 -8.84411391e-03 -2.43200287e-02\n",
      " -5.73814511e-02 -2.17334870e-02  6.03747256e-02  1.59222521e-02\n",
      "  2.87384796e-03  3.00619397e-02 -6.48647025e-02 -3.56271975e-02\n",
      "  3.41966137e-04  5.97507926e-03 -2.78856009e-02  8.21759775e-02\n",
      " -4.34630513e-02 -2.21294723e-02  6.11384623e-02  1.34110361e-01\n",
      "  2.61387718e-03 -6.66510314e-02 -1.08557818e-02 -2.27634441e-02\n",
      "  6.56756982e-02  5.04081100e-02  2.60738675e-02  2.96083745e-02\n",
      "  2.96403770e-03 -2.68989541e-02  3.25817429e-02 -4.77534495e-02\n",
      "  2.91177165e-02  1.21364053e-02  3.61824129e-03  3.34559679e-02\n",
      " -1.87795376e-03  1.39309661e-02 -3.87431751e-03 -2.63950024e-02\n",
      "  5.21587543e-02  7.95530807e-03 -3.25279161e-02  5.50867105e-03\n",
      " -8.02455842e-02  2.54014805e-02  2.42111403e-02 -6.88694566e-02\n",
      "  6.80361837e-02  3.49501526e-04  9.00671538e-03 -2.77494714e-02\n",
      " -1.52270123e-02  4.65111248e-02 -2.67316680e-02  1.13789532e-02\n",
      "  4.28157896e-02  2.64182109e-02 -1.14518385e-02 -1.58912055e-02\n",
      " -1.81100685e-02 -2.91360319e-02 -5.04318736e-02  3.48882973e-02\n",
      " -4.88506481e-02  8.51980224e-02 -2.40058172e-02 -4.74805245e-04\n",
      "  1.08796954e-02 -1.95390563e-02  1.62021648e-02 -3.42027023e-02\n",
      " -3.78363132e-02 -3.74191403e-02  2.08850447e-02  6.25741261e-04\n",
      "  4.09244262e-02 -7.96503201e-03 -3.07211597e-02 -2.86010131e-02\n",
      "  9.19208210e-03 -8.83709043e-02  3.94571722e-02 -3.70749496e-02\n",
      " -1.16303870e-02 -4.02609892e-02 -1.66663807e-02 -4.54829559e-02\n",
      "  7.62197450e-02 -2.10805312e-02 -1.15806088e-02  2.26928145e-02\n",
      " -1.55341253e-02 -6.81816321e-03  5.35224602e-02 -4.68827710e-02\n",
      " -8.04427487e-04  1.05660791e-02 -2.78787711e-03 -1.66233201e-02\n",
      " -1.61674730e-02 -4.92973626e-03 -2.02183127e-02 -5.66957938e-03\n",
      " -2.95121465e-02 -5.26972190e-02  1.90177914e-02  6.66106353e-04\n",
      " -1.29930340e-02  5.33003956e-02  2.68755845e-33 -3.21150385e-02\n",
      " -2.11053574e-03 -5.07444069e-02  4.78233099e-02  3.72024253e-02\n",
      " -2.56987587e-02 -1.19759534e-02 -5.76877557e-02  5.49337603e-02\n",
      " -2.13820022e-02  1.91575568e-02  6.34295167e-03 -4.75406786e-03\n",
      "  2.04408541e-02  1.07866973e-02 -8.51493403e-02  1.47765568e-02\n",
      "  6.25373274e-02  1.04148062e-02  1.56272631e-02 -1.14781260e-02\n",
      " -5.61868586e-02  4.21102569e-02  3.52333188e-02  3.41385254e-03\n",
      " -4.67980690e-02 -1.80712845e-02 -6.14240952e-03 -2.17472669e-02\n",
      "  2.03627255e-02 -2.80616432e-02  2.13502832e-02 -4.08370756e-02\n",
      " -1.21226795e-02 -2.88871191e-02 -1.02638733e-02  1.64860282e-02\n",
      " -4.48425184e-04  2.42318101e-02  3.66818421e-02 -1.95624642e-02\n",
      "  6.60977000e-03  1.03314281e-01  3.88364941e-02 -7.43389055e-02\n",
      "  5.18580116e-02 -1.37119070e-02  2.13431045e-02  2.23212373e-02\n",
      " -4.46436666e-02  2.64632944e-02  8.55354890e-02 -1.78489590e-03\n",
      " -3.54052149e-02 -5.94411008e-02  2.44842120e-03  3.77057381e-02\n",
      "  2.52434378e-03  3.28403488e-02  7.18777105e-02 -4.21842560e-03\n",
      " -4.10485500e-03  7.12756440e-02 -1.18006235e-02 -2.61001959e-02\n",
      "  4.89613041e-02  7.80019015e-02  1.58549100e-02 -2.10710522e-02\n",
      " -1.64562091e-02  3.93063901e-03  3.91908363e-02  1.92021914e-02\n",
      " -5.17402515e-02  3.25031467e-02  5.53387366e-02  2.49874890e-02\n",
      "  2.76886839e-02 -3.56663167e-02 -3.41968238e-02  2.06229594e-02\n",
      "  3.26713324e-02 -4.29041311e-02  2.62591359e-03  4.12291475e-02\n",
      " -8.74129757e-02  9.52266157e-02  8.62808246e-03  4.67638038e-02\n",
      "  4.27409336e-02  1.16691053e-01  1.32856928e-02 -2.50164121e-02\n",
      "  1.73932826e-03 -8.60073231e-03  1.24992686e-03 -4.61972542e-02\n",
      " -4.03604209e-02 -3.22943293e-02 -2.30257586e-02 -4.63813497e-03\n",
      " -2.45359093e-02  5.35283647e-02  4.19440772e-03  1.01437494e-02\n",
      "  3.06844488e-02  2.80387532e-02  4.09806930e-02 -2.69035585e-02\n",
      "  3.79183120e-03  2.15080418e-02  4.28831689e-02 -4.30176258e-02\n",
      " -6.86156377e-02 -2.72122771e-02  7.68235559e-03 -6.22538552e-02\n",
      "  3.81456898e-03  2.61061955e-02 -3.46263833e-02  1.28830243e-02\n",
      "  5.93077741e-04  4.99908999e-02  1.69820320e-02  4.29438725e-02\n",
      "  1.70877390e-02 -2.87876260e-02 -1.17119849e-02  4.55158427e-02\n",
      "  3.51275802e-02 -3.13966163e-02  3.07939574e-02 -1.50829158e-03\n",
      "  2.24454552e-02 -1.85893551e-02 -3.23756933e-02  3.66898403e-02\n",
      "  1.88710052e-03 -1.24026490e-02  5.96230337e-03  2.03220248e-02\n",
      "  3.16509455e-02 -1.78939700e-02 -7.09910830e-03  1.12109892e-02\n",
      " -9.62416269e-03 -2.60764174e-02 -2.47969441e-02 -1.65694412e-02\n",
      " -2.81837080e-02 -1.77687835e-02  3.18063758e-02  1.11796306e-02\n",
      "  1.32764848e-02  7.54720997e-03  4.30178903e-02 -1.54438615e-02\n",
      "  8.06555748e-02 -4.25643567e-03  3.52593628e-03 -3.78522873e-02\n",
      "  4.92975153e-02 -5.03921025e-02 -2.93354969e-02  8.58211890e-03\n",
      "  2.68631596e-02  4.03082184e-02  2.95627154e-02  5.57620674e-02\n",
      "  6.31305203e-02 -4.87381779e-02 -2.46596755e-03  1.05649699e-02\n",
      "  2.14446522e-02  5.63113391e-02 -2.21867803e-02  1.07072331e-01\n",
      " -2.56065633e-02 -3.33635099e-02 -3.34037505e-02 -2.75849588e-02\n",
      "  4.55308182e-04 -5.01803420e-02 -4.82621156e-02 -3.02250553e-02\n",
      " -3.23276296e-02  6.16881112e-03  3.42307910e-02  7.63496244e-03\n",
      " -1.77230164e-02  6.71720039e-03 -2.70274002e-02  8.23468268e-02\n",
      " -4.38831002e-02 -2.93297153e-02 -2.81882752e-02  4.12143469e-02\n",
      "  2.18016673e-02 -2.48112455e-02 -4.80811335e-02 -1.10460408e-01\n",
      "  3.70273702e-02  6.90882094e-03 -3.24926674e-02  2.99199149e-02\n",
      " -1.48300864e-02  2.06583962e-02 -7.07678730e-03 -1.09593058e-02\n",
      "  2.53681634e-02  5.11374790e-03  4.53540273e-02  3.71298604e-02\n",
      "  5.42714773e-03  2.59081759e-02 -2.15023216e-02  2.77117640e-02\n",
      "  1.00831408e-02  4.54039387e-02  2.31525786e-02 -2.60809623e-02\n",
      "  5.59955202e-02 -2.77955327e-02 -3.13352421e-02 -2.57340204e-02\n",
      " -5.71258403e-02 -6.03812141e-03  2.88834162e-02 -9.78067517e-03\n",
      " -2.77344603e-02  5.54856518e-03  1.62502378e-02 -3.11377132e-03\n",
      " -1.92464031e-02  6.45407662e-03  4.94345725e-02 -1.83710847e-02\n",
      " -1.19027859e-02  2.15626992e-02  1.79084986e-02  9.95923057e-02\n",
      "  7.63741732e-02  1.50433108e-02 -2.91355848e-02 -3.89983058e-02\n",
      " -2.95918081e-02 -4.13562283e-02 -8.54154676e-03 -1.89884994e-02\n",
      "  8.57208949e-03  8.77251755e-03 -8.42606835e-03  1.20966863e-02\n",
      " -9.61464271e-03 -5.13859168e-02 -1.11315241e-02  5.11423834e-02\n",
      " -6.29280647e-03 -5.16711473e-02 -2.12988034e-02 -1.11759817e-02\n",
      "  2.49515306e-02 -1.45137254e-02 -3.58739495e-03  4.16563675e-02\n",
      "  5.15723228e-03  1.59327388e-02 -6.01370111e-02 -3.45670544e-02\n",
      "  4.29583266e-02 -2.85811592e-02 -6.64402992e-02 -6.28364235e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requires !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-distilroberta-v1\", \n",
    "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That's a lot of numbers.\n",
    "\n",
    "How about we do just once sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Yo! How cool are embeddings?\n",
      "Embedding:\n",
      "[ 1.08260913e-02  2.41692699e-02 -5.50694950e-03 -8.78381729e-02\n",
      "  4.02133204e-02  5.20530120e-02 -2.10508443e-02 -3.94526729e-03\n",
      " -2.29506586e-02 -3.05871833e-02 -2.15222649e-02  4.00971174e-02\n",
      " -4.41348702e-02 -6.95999414e-02  4.64823935e-03  5.68864420e-02\n",
      "  3.78482905e-03 -2.37432551e-02 -8.03357363e-03 -4.99030091e-02\n",
      "  4.06385995e-02  1.21118426e-02  3.38867493e-02 -4.95439395e-02\n",
      " -1.42667750e-02 -2.26875320e-02  3.49207260e-02 -1.09987622e-02\n",
      " -3.37442160e-02  3.29349227e-02  1.40444702e-02  4.57680486e-02\n",
      "  1.81821864e-02  3.24518345e-02 -4.85767163e-02  2.39300914e-02\n",
      "  1.60664283e-02 -3.34798321e-02  7.98382144e-03 -4.71427478e-03\n",
      "  2.84693576e-03 -5.78676932e-04 -1.88540611e-02 -1.59974489e-02\n",
      "  2.75714919e-02  1.45352373e-04  1.30773792e-02 -9.23535377e-02\n",
      "  1.74976271e-02  3.60502005e-02  1.59503389e-02  1.34556750e-02\n",
      " -4.19778302e-02  9.27382708e-03  2.27063205e-02  6.86729923e-02\n",
      "  1.84429679e-02  2.15068497e-02  1.29141044e-02  3.44659500e-02\n",
      " -2.89168973e-02  1.48917073e-02 -2.10672487e-02 -4.40885574e-02\n",
      "  4.29354534e-02  1.47787565e-02  5.72202727e-03  1.47168292e-02\n",
      " -2.14426611e-02 -1.17971422e-02  3.27390097e-02 -1.22982347e-02\n",
      " -1.11593446e-02 -4.21808548e-02  2.13266183e-02  1.57495905e-02\n",
      " -8.22773855e-03 -3.85033749e-02 -2.94417422e-02  6.00807518e-02\n",
      "  1.69122629e-02  3.92937660e-02 -5.07948771e-02 -4.75694016e-02\n",
      " -2.78106285e-03  2.24482771e-02  5.07322326e-02  1.66234616e-02\n",
      "  3.69281927e-03  5.89906145e-03 -3.15623097e-02  2.76781600e-02\n",
      "  8.49933177e-03 -7.73044899e-02  3.68934199e-02  4.32453603e-02\n",
      "  1.39048435e-02 -7.95404240e-02 -1.71953924e-02 -3.66013832e-02\n",
      "  6.48010597e-02  1.92784350e-02  4.72230744e-03 -4.89871763e-02\n",
      "  2.66537406e-02  4.85890545e-02 -5.18371947e-02 -4.23862450e-02\n",
      "  2.69119050e-02 -2.14077570e-02  2.88797263e-02 -1.01784162e-01\n",
      " -7.71638239e-03 -3.64925563e-02 -1.38570117e-02 -5.57462312e-02\n",
      "  2.67055966e-02 -1.03652794e-02  4.58762869e-02  3.56889479e-02\n",
      "  4.08880301e-02 -3.27138081e-02 -2.02358118e-03 -4.29364517e-02\n",
      " -7.04241395e-02 -1.33122494e-02 -3.54776867e-02  5.17461859e-02\n",
      " -3.63688357e-02  7.45037720e-02  3.55739333e-03 -3.70833836e-02\n",
      " -1.71278752e-02  4.13968749e-02  4.04369570e-02 -6.06693514e-03\n",
      "  4.64594690e-03  3.09206359e-02 -5.45153394e-02 -5.93561260e-03\n",
      " -9.94842872e-03 -1.58128049e-02  3.69674973e-02  3.72255221e-02\n",
      "  1.45935183e-02  6.60967035e-03  4.96151065e-03  2.22660936e-02\n",
      "  2.49482784e-03  1.57933272e-02 -3.05037107e-02  5.77565841e-02\n",
      "  5.40358648e-02 -3.16023943e-03 -3.33364308e-02  7.14947982e-03\n",
      "  7.61846546e-03  1.20741678e-02 -2.73434278e-02  5.95168993e-02\n",
      "  2.04747356e-02 -5.18700667e-02 -2.90808380e-02 -3.42983101e-03\n",
      " -1.23356916e-02  1.11330599e-02  4.42048050e-02  6.53859135e-03\n",
      "  4.72902181e-03  1.86977088e-02 -2.90878154e-02  4.65991162e-03\n",
      "  2.57558003e-03 -2.43274625e-02 -1.91747062e-02  9.68340226e-03\n",
      " -5.38327321e-02  7.72943571e-02 -1.23340674e-02 -1.39941252e-03\n",
      " -1.91598162e-02  3.37586626e-02 -5.80420755e-02  6.47749146e-03\n",
      "  5.88047598e-03 -1.25942240e-03  2.36867759e-02  3.25877517e-02\n",
      "  3.55001837e-02 -3.32353823e-02  4.32332382e-02  1.14252465e-02\n",
      " -1.37306973e-02  5.58508676e-04 -4.38374951e-02 -4.31177430e-02\n",
      "  2.27002371e-02 -1.08064497e-02  1.22234961e-02 -2.27974858e-02\n",
      "  2.69942936e-02 -8.72965455e-02  4.23120596e-02 -1.23364506e-02\n",
      "  3.68462391e-02 -2.24115630e-03 -8.61508027e-03 -6.66196495e-02\n",
      " -2.16259677e-02  9.63442959e-03  3.96033190e-02 -9.06255543e-02\n",
      " -2.02792082e-02 -1.83419231e-02 -4.50078165e-03  3.80039625e-02\n",
      " -7.03426311e-03  1.38224475e-02  4.45462167e-02 -5.29578365e-02\n",
      "  3.25749745e-04 -1.89781375e-02 -8.82708207e-02 -2.16181781e-02\n",
      " -5.52318618e-03  3.37858871e-02 -3.95878889e-02 -7.04384502e-03\n",
      "  2.81860661e-02 -9.84685123e-03 -1.43244211e-02  2.55991984e-02\n",
      "  6.75271824e-02  6.46527708e-02 -5.11129387e-02  5.19175492e-02\n",
      " -1.57853272e-02 -1.56200235e-03  6.26118779e-02 -9.34498012e-03\n",
      " -8.84149849e-05 -3.51231135e-02  3.46243642e-02 -4.31087986e-03\n",
      "  2.52958741e-02 -7.84893036e-02  1.53837800e-02 -7.33629754e-03\n",
      "  1.32750068e-02 -1.15827676e-02  1.16930958e-02 -7.16161430e-02\n",
      " -3.20236087e-02 -1.08003113e-02  2.07232498e-03 -6.85923314e-03\n",
      " -2.37986948e-02  3.67761627e-02 -3.39296013e-02  4.02844772e-02\n",
      "  2.84563210e-02 -9.47130993e-02 -1.70994587e-02  3.03932987e-02\n",
      " -9.26400200e-02 -2.23155133e-02 -1.05461292e-03 -4.77502868e-03\n",
      "  5.02519011e-02 -2.67287642e-02 -1.97344720e-02 -2.41351128e-03\n",
      "  4.77260277e-02 -2.05750554e-03  7.83092529e-03 -2.56457496e-02\n",
      "  5.09714410e-02  4.13870737e-02  3.19360644e-02  7.84981158e-03\n",
      "  7.58009478e-02 -3.54966596e-02  1.01488270e-01 -2.52832230e-02\n",
      " -6.39281468e-03 -3.49854156e-02 -3.69307213e-02  5.06416373e-02\n",
      "  1.36472266e-02 -4.13364060e-02  3.31079364e-02  1.34848189e-02\n",
      " -3.93462414e-03 -5.26181720e-02 -2.15408076e-02 -5.63620403e-02\n",
      " -9.46792192e-04 -9.18579753e-03  8.57119784e-02 -4.43336479e-02\n",
      " -1.39774475e-02 -4.29182574e-02  3.82948644e-03 -1.01560280e-02\n",
      "  2.43306905e-02  2.79308786e-03  2.53226757e-02 -1.11908773e-02\n",
      " -1.11031026e-01  1.00716166e-02  3.37308720e-02 -3.19051766e-03\n",
      "  2.34519597e-02  3.25720124e-02 -4.81830761e-02 -1.66555345e-02\n",
      " -1.74819753e-02  5.36616109e-02  1.76263731e-02  4.94872630e-02\n",
      " -3.96546610e-02 -4.84619662e-02 -6.52604699e-02  3.18873152e-02\n",
      " -7.80272996e-03  5.44865280e-02  9.67995450e-03  4.42935154e-02\n",
      "  2.64919680e-02  4.36910382e-03 -1.15235724e-01 -4.74707820e-02\n",
      " -3.58013771e-02  6.38522953e-02 -1.25456266e-02  1.97242349e-02\n",
      "  2.35997457e-02  3.06572001e-02 -1.24634970e-02 -2.38370616e-02\n",
      " -9.10773035e-03 -2.05568131e-03 -6.00743399e-04  6.75559789e-02\n",
      "  4.05675508e-02 -5.64920157e-02 -9.92758572e-03 -9.63817351e-03\n",
      "  4.16152067e-02 -1.22759268e-02 -7.89963081e-02  3.67506631e-02\n",
      " -4.94728312e-02  5.03591187e-02  1.18724573e-02  6.74811676e-02\n",
      " -2.86566862e-03  1.68700516e-02  7.31362775e-03  2.60172877e-02\n",
      "  1.58470813e-02  6.29823282e-02  3.02402470e-02  1.99539438e-02\n",
      "  1.28335999e-02 -6.25512376e-02 -7.22569823e-02  3.83264311e-02\n",
      " -5.37748914e-03  2.94202939e-03  6.61651567e-02  4.53490950e-03\n",
      "  8.68646428e-03 -3.37273479e-02 -1.90926250e-02  6.05575033e-02\n",
      "  3.10163014e-02 -8.83600302e-03  4.05556597e-02  1.39546860e-02\n",
      " -4.25420962e-02 -8.38431297e-04 -3.23137492e-02 -2.25256686e-03\n",
      "  3.98349613e-02 -2.94450112e-02 -2.62961667e-02  3.25073041e-02\n",
      " -4.40992005e-02 -2.45306175e-02  1.00516062e-02  5.79220690e-02\n",
      " -3.27765308e-02 -2.93654315e-02 -3.95729318e-02 -4.18349653e-02\n",
      "  7.85630569e-03 -4.33267467e-02  2.57533565e-02  2.68054456e-02\n",
      " -6.66377274e-03 -4.73842137e-02  6.39312249e-03  4.41065468e-02\n",
      "  1.56671666e-02  5.44472365e-03 -3.62123512e-02 -4.10402678e-02\n",
      "  6.23032963e-03  1.95667278e-02 -2.18551792e-02  1.47492746e-02\n",
      " -1.70415044e-02 -1.90135296e-02 -7.92981009e-04 -3.69191058e-02\n",
      " -5.57348086e-03 -6.59317300e-02 -1.26405628e-02  1.65136363e-02\n",
      " -2.73985881e-02 -1.00930789e-02  3.60466130e-02  1.42403748e-02\n",
      "  5.45407459e-02 -1.75216980e-02 -6.13529272e-02  4.35160547e-02\n",
      " -3.46017182e-02 -6.04250655e-02  4.65894714e-02 -2.12061629e-02\n",
      "  9.17034522e-02  3.40809207e-03 -2.51403376e-02 -1.90434046e-02\n",
      "  6.32268097e-03  5.90704978e-02 -4.47293855e-02  4.29635420e-02\n",
      " -1.89008322e-02 -1.51639599e-02  2.63488255e-02  3.66073251e-02\n",
      " -1.13447336e-02 -2.83496045e-02 -2.78483350e-02  1.93125091e-03\n",
      " -3.16300467e-02  1.29435465e-01 -3.21113318e-02 -3.62546928e-02\n",
      " -2.17480026e-02 -3.75396758e-02  5.98449223e-02 -6.26928825e-03\n",
      " -4.33079489e-02 -3.18766907e-02 -3.86201846e-03  1.90646295e-02\n",
      "  5.81042804e-02 -1.06653431e-02 -2.37339688e-03 -1.19262924e-02\n",
      " -1.02159297e-02 -5.43072261e-02  3.86321209e-02 -2.44073048e-02\n",
      " -2.80684829e-02 -9.00446903e-03  1.16183236e-02 -6.98793828e-02\n",
      " -1.16777150e-02  3.22118625e-02  3.39050479e-02  4.55598384e-02\n",
      " -2.12054010e-02  8.16518720e-03  5.79289421e-02 -3.81503478e-02\n",
      " -4.43043094e-03  1.35337107e-03  1.10745169e-02 -2.10747700e-02\n",
      " -9.47950501e-03 -1.34912115e-02 -3.65765765e-03  4.97881956e-02\n",
      " -3.00230365e-02 -6.24600314e-02 -6.97974414e-02  1.93837881e-02\n",
      "  5.20540029e-02  3.08731049e-02  2.24625564e-33  9.80735291e-04\n",
      "  5.50294807e-03 -4.54840176e-02 -2.57982034e-03  5.53516671e-03\n",
      " -2.92656035e-03 -2.64072958e-02  2.00803187e-02  1.00955525e-02\n",
      " -1.00492714e-02  6.91963285e-02 -1.08639263e-02 -3.42691354e-02\n",
      "  1.46769136e-02 -6.40992522e-02 -3.62574086e-02  7.67651014e-03\n",
      "  4.40374948e-02  5.81924571e-03  7.66832288e-03 -2.25624572e-02\n",
      " -3.20991129e-02  7.86383152e-02 -1.35174394e-02  1.40205892e-02\n",
      " -2.88673514e-03  1.98903307e-02  1.72270220e-02  5.56149147e-03\n",
      " -5.56690851e-03 -1.97010413e-02  6.62108362e-02 -3.32453549e-02\n",
      " -7.76921958e-02  2.96486430e-02 -2.33435836e-02 -4.63043898e-03\n",
      "  4.05721813e-02  2.41771918e-02  4.01919335e-02 -1.88218318e-02\n",
      "  2.93938145e-02  1.52132721e-04  1.07136350e-02 -2.52685528e-02\n",
      "  5.90888644e-03  3.23564447e-02  9.34945513e-03 -3.90718784e-03\n",
      " -7.38908872e-02 -1.01162307e-02  3.66592593e-02 -4.23666947e-02\n",
      " -3.58617902e-02 -6.24363907e-02 -2.63717677e-02  1.18380487e-02\n",
      " -9.94323939e-03  9.82738379e-03  3.09555847e-02  3.50524555e-04\n",
      " -2.64366250e-02  4.91941422e-02  2.46406533e-02 -6.54783323e-02\n",
      " -9.38120205e-03  3.92585360e-02  5.31138619e-04  5.42595444e-05\n",
      " -2.70135738e-02  1.63600594e-02  1.97196919e-02  3.40841077e-02\n",
      "  3.38979028e-02 -3.54994118e-04  9.89753306e-02 -3.95267867e-02\n",
      "  4.47720252e-02 -2.20412761e-02 -4.98319929e-03  2.81845294e-02\n",
      " -3.76559347e-02 -3.21509279e-02 -9.70000587e-03  2.57118549e-02\n",
      " -5.88521548e-02  8.01836327e-02  8.49111006e-03  9.23721213e-03\n",
      "  1.74626708e-03  6.42001629e-02  6.52225837e-02  8.76695570e-03\n",
      "  1.72040449e-03 -1.04399752e-02 -2.06091739e-02 -7.64427707e-02\n",
      " -4.34605516e-02  9.45790485e-03  8.01980868e-03  7.25219995e-02\n",
      " -5.29142236e-03  2.94390861e-02 -1.78188588e-02  1.70220342e-02\n",
      " -1.51242304e-03 -1.51023148e-02  3.52623612e-02  1.94637533e-02\n",
      " -2.31502131e-02  1.07675018e-02 -5.00552682e-03 -1.44519377e-02\n",
      " -3.32182124e-02  6.20088279e-02  2.69659813e-02 -2.86766998e-02\n",
      "  2.92606074e-02 -4.01196033e-02 -1.31186005e-02 -3.34521681e-02\n",
      " -3.49293016e-02  5.56185246e-02  9.72344652e-02  5.45154745e-03\n",
      "  6.41311035e-02 -6.03784025e-02  4.45316284e-04  1.98593736e-02\n",
      " -2.06964882e-03  5.98608342e-04 -2.28845496e-02 -9.25562158e-03\n",
      "  3.43566798e-02  3.78562696e-02 -1.90134998e-02  1.34337712e-02\n",
      " -8.56055412e-03 -2.03252602e-02 -5.80456853e-02 -4.44501191e-02\n",
      "  1.63507974e-03 -4.48045991e-02 -1.19424034e-02 -3.38625088e-02\n",
      " -1.80710237e-02 -8.19313247e-03 -1.93339062e-03 -2.00767405e-02\n",
      " -3.53051946e-02 -1.53205253e-03 -3.26330471e-03 -5.33324778e-02\n",
      "  3.20943557e-02 -1.85389221e-02  3.17645632e-02  1.44379381e-02\n",
      "  7.38357753e-02 -7.68864341e-03 -1.10554351e-02  2.80031804e-02\n",
      "  5.94301336e-02 -2.12085415e-02 -3.47336456e-02  4.52917442e-02\n",
      "  6.40740106e-03 -1.46802757e-02  5.43359816e-02  4.42648456e-02\n",
      "  5.18207162e-05  5.64107671e-03  9.62776039e-03  1.64936930e-02\n",
      "  4.85213511e-02  7.52792358e-02 -3.76901925e-02  3.03656105e-02\n",
      "  9.27514024e-03 -2.65770108e-02 -5.47038205e-02 -7.40861346e-04\n",
      " -4.13001981e-03 -3.90815623e-02 -3.94909456e-02  3.02188341e-02\n",
      " -5.57051860e-02 -2.89235227e-02  5.76529885e-03 -2.58935266e-03\n",
      " -3.77671956e-03 -5.43502122e-02  6.19166810e-03  3.05840019e-02\n",
      " -2.51964829e-03 -3.82508077e-02 -1.11505864e-02  2.72627198e-03\n",
      "  9.51769948e-03 -4.39575426e-02 -1.25080987e-03 -4.34929691e-02\n",
      "  7.37353042e-03 -7.62937218e-03 -8.97918269e-02 -1.56955682e-02\n",
      " -2.55346261e-02  2.24405080e-02 -1.02870737e-03 -4.20513563e-02\n",
      "  1.38474144e-02  4.52309754e-03  3.81816961e-02  1.47458240e-02\n",
      "  4.98733446e-02 -4.20057622e-04 -3.25220055e-03  2.64411159e-02\n",
      " -2.03602016e-02  1.64234582e-02  1.23465266e-02 -2.38930341e-02\n",
      "  3.35369669e-02  2.92663602e-03 -1.94603913e-02 -2.28902845e-05\n",
      " -7.32492330e-03  7.77280657e-03  1.34164542e-02 -1.38965501e-02\n",
      " -2.82231029e-02 -3.17339785e-03  6.02337532e-02  5.67685366e-02\n",
      " -6.63361140e-03 -2.28223968e-02  4.38112468e-02  1.01254229e-02\n",
      "  3.78390774e-02  5.23660369e-02  1.65555123e-02  8.97497907e-02\n",
      " -5.78293391e-02 -1.89209655e-02 -2.17298102e-02 -2.32200846e-02\n",
      " -1.33678894e-02  3.87906097e-02  6.46334738e-02  2.98444182e-02\n",
      "  1.30476337e-02 -7.83440247e-02 -3.41035463e-02  1.34989358e-02\n",
      "  3.43383104e-02 -4.79843430e-02  8.87369215e-02  4.88414057e-02\n",
      " -5.89989312e-02 -4.80948761e-02 -2.57466938e-02 -4.63863537e-02\n",
      " -1.43878506e-02  5.75019931e-03 -1.98178878e-03  3.82988937e-02\n",
      " -1.79926306e-02  4.21510674e-02  7.12391734e-03  6.54197335e-02\n",
      "  7.99310505e-02  4.33803350e-02 -1.09676495e-02 -5.29075339e-02]\n",
      "Embedding size: (768,)\n"
     ]
    }
   ],
   "source": [
    "single_sentence = \"Yo! How cool are embeddings?\"\n",
    "single_embedding = embedding_model.encode(single_sentence)\n",
    "print(f\"Sentence: {single_sentence}\")\n",
    "print(f\"Embedding:\\n{single_embedding}\")\n",
    "print(f\"Embedding size: {single_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've now got a way to numerically represent each of our chunks.\n",
    "\n",
    "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n",
    "\n",
    "> **Note:** No matter the size of the text input to our `all-mpnet-base-v2` model, it will be turned into an embedding size of `(768,)`. This value is fixed. So whether a sentence is 1 token long or 1000 tokens long, it will be truncated/padded with zeros to size 384 and then turned into an embedding vector of size `(768,)`. Of course, other embedding models may have different input/output shapes.\n",
    "\n",
    "How about we add an embedding field to each of our chunk items?\n",
    "\n",
    "Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Uncomment to see how long it takes to create embeddings on CPU\n",
    "# # Make sure the model is on the CPU\n",
    "# embedding_model.to(\"cpu\")\n",
    "\n",
    "# # Embed each chunk one by one\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok not too bad... but this would take a *really* long time if we had a larger dataset.\n",
    "\n",
    "Now let's see how long it takes to create the embeddings with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:06<00:00, 34.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.23 s\n",
      "Wall time: 6.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Send the model to the GPU\n",
    "embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n",
    "\n",
    "# Create embeddings one by one on the GPU\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Looks like the embeddings get created much faster (~10x faster on my machine) on the GPU!\n",
    "\n",
    "You'll likely notice this trend with many of your deep learning workflows. If you have access to a GPU, especially a NVIDIA GPU, you should use one if you can.\n",
    "\n",
    "But what if I told you we could go faster again?\n",
    "\n",
    "You see many modern models can handle batched predictions.\n",
    "\n",
    "This means computing on multiple samples at once.\n",
    "\n",
    "Those are the types of operations where a GPU flourishes!\n",
    "\n",
    "We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text chunks into a single list\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.05 s\n",
      "Wall time: 4.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0100, -0.0654, -0.0008,  ..., -0.0387, -0.0191,  0.0307],\n",
       "        [ 0.0183, -0.0897, -0.0003,  ..., -0.0708, -0.0366, -0.0291],\n",
       "        [ 0.0208, -0.0895,  0.0151,  ..., -0.0447, -0.0238, -0.0097],\n",
       "        ...,\n",
       "        [-0.0184, -0.0806,  0.0060,  ...,  0.0044, -0.0658, -0.0105],\n",
       "        [-0.0105, -0.0870,  0.0179,  ...,  0.0564, -0.0461,  0.0054],\n",
       "        [ 0.0092, -0.0777,  0.0093,  ...,  0.0333, -0.0162,  0.0147]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Embed all texts in batches\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what I'm talking about!\n",
    "\n",
    "A ~4x improvement (on my GPU) in speed thanks to batched operations.\n",
    "\n",
    "So the tip here is to use a GPU when you can and use batched operations if you can too.\n",
    "\n",
    "Now let's save our chunks and their embeddings so we could import them later if we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings to file\n",
    "\n",
    "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make sure it imports nicely by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...</td>\n",
       "      <td>2359</td>\n",
       "      <td>440</td>\n",
       "      <td>589.75</td>\n",
       "      <td>[ 9.96456947e-03 -6.54456615e-02 -7.66414392e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>something peculiar — a cat reading a map. For ...</td>\n",
       "      <td>1468</td>\n",
       "      <td>282</td>\n",
       "      <td>367.00</td>\n",
       "      <td>[ 1.82670932e-02 -8.97018537e-02 -3.11918731e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>But then it struck Mr. Dursley that this was p...</td>\n",
       "      <td>1413</td>\n",
       "      <td>263</td>\n",
       "      <td>353.25</td>\n",
       "      <td>[ 2.07877830e-02 -8.94869491e-02  1.50764538e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>“ — yes, their son, Harry —”    Mr. Dursley st...</td>\n",
       "      <td>1629</td>\n",
       "      <td>308</td>\n",
       "      <td>407.25</td>\n",
       "      <td>[-1.19488453e-02 -7.12201223e-02 -1.57541279e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>And the old man hugged Mr. Dursley around the ...</td>\n",
       "      <td>1053</td>\n",
       "      <td>204</td>\n",
       "      <td>263.25</td>\n",
       "      <td>[-9.28694103e-03 -4.73219007e-02 -1.84328258e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            4  CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...   \n",
       "1            5  something peculiar — a cat reading a map. For ...   \n",
       "2            5  But then it struck Mr. Dursley that this was p...   \n",
       "3            6  “ — yes, their son, Harry —”    Mr. Dursley st...   \n",
       "4            6  And the old man hugged Mr. Dursley around the ...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0              2359               440             589.75   \n",
       "1              1468               282             367.00   \n",
       "2              1413               263             353.25   \n",
       "3              1629               308             407.25   \n",
       "4              1053               204             263.25   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 9.96456947e-03 -6.54456615e-02 -7.66414392e-...  \n",
       "1  [ 1.82670932e-02 -8.97018537e-02 -3.11918731e-...  \n",
       "2  [ 2.07877830e-02 -8.94869491e-02  1.50764538e-...  \n",
       "3  [-1.19488453e-02 -7.12201223e-02 -1.57541279e-...  \n",
       "4  [-9.28694103e-03 -4.73219007e-02 -1.84328258e-...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and embedding questions\n",
    "\n",
    "> **Which embedding model should I use?**\n",
    "\n",
    "This depends on many factors. My best advice is to experiment, experiment, experiment! \n",
    "\n",
    "If you want the model to run locally, you'll have to make sure it's feasible to run on your own hardware. \n",
    "\n",
    "A good place to see how different models perform on a wide range of embedding tasks is the [Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "> **What other forms of text chunking/splitting are there?**\n",
    "\n",
    "There are a fair few options here too. We've kept it simple with groups of sentences.\n",
    "\n",
    "For more, [Pinecone has a great guide on different kinds of chunking](https://www.pinecone.io/learn/chunking-strategies/) including for different kinds of data such as markdown and LaTeX.\n",
    "\n",
    "Libraries such as [LangChain also have a good amount of in-built text splitting options](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
    "\n",
    "> **What should I think about when creating my embeddings?**\n",
    "\n",
    "Our model turns text inputs up to 384 tokens long in embedding vectors of size 768.\n",
    "\n",
    "Generally, the larger the vector size, the more information that gets encoded into the embedding (however, this is not always the case, as smaller, better models can outperform larger ones).\n",
    "\n",
    "Though with larger vector sizes comes larger storage and compute requirements.\n",
    "\n",
    "Our model is also relatively small (420MB) in size compared to larger models that are available.\n",
    "\n",
    "Larger models may result in better performance but will also require more compute.\n",
    "\n",
    "So some things to think about:\n",
    "* Size of input - If you need to embed longer sequences, choose a model with a larger input capacity.\n",
    "* Size of embedding vector - Larger is generally a better representation but requires more compute/storage.\n",
    "* Size of model - Larger models generally result in better embeddings but require more compute power/time to run.\n",
    "* Open or closed - Open models allow you to run them on your own hardware whereas closed models can be easier to setup but require an API call to get embeddings.\n",
    "\n",
    "> **Where should I store my embeddings?**\n",
    "\n",
    "If you've got a relatively small dataset, for example, under 100,000 examples (this number is rough and only based on first hand experience), `np.array` or `torch.tensor` can work just fine as your dataset.\n",
    "\n",
    "But if you've got a production system and want to work with 100,000+ embeddings, you may want to look into a [vector database]( https://en.wikipedia.org/wiki/Vector_database) (these have become very popular lately and there are many offerings).\n",
    "\n",
    "### Document Ingestion and Embedding Creation Extensions\n",
    "\n",
    "One major extension to the workflow above would to functionize it.\n",
    "\n",
    "Or turn it into a script.\n",
    "\n",
    "As in, take all the functionality we've created and package it into a single process (e.g. go from document -> embeddings file).\n",
    "\n",
    "So you could input a document on one end and have embeddings come out the other end. The hardest part of this is knowing what kind of preprocessing your text may need before it's turned into embeddings. Cleaner text generally means better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG - Search and Answer\n",
    "\n",
    "We discussed RAG briefly in the beginning but let's quickly recap.\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
    "\n",
    "Let's breakdown each step:\n",
    "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
    "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
    "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
    "\n",
    "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
    "\n",
    "This is an incredibly helpful tool.\n",
    "\n",
    "Let's say you had 1000s of customer support documents.\n",
    "\n",
    "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
    "\n",
    "Or you were an insurance company with large chains of claims emails.\n",
    "\n",
    "You could use RAG to answer questions about the emails with sources.\n",
    "\n",
    "One helpful analogy is to think of LLMs as calculators for words.\n",
    "\n",
    "With good inputs, the LLM can sort them into helpful outputs.\n",
    "\n",
    "How? \n",
    "\n",
    "It starts with better search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search\n",
    "\n",
    "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
    "\n",
    "If this sounds like woo, woo. It's not.\n",
    "\n",
    "Perhaps searching via *meaning* is a better analogy.\n",
    "\n",
    "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
    "\n",
    "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
    "\n",
    "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
    "\n",
    "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with: \n",
    ">\n",
    ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n",
    "> \n",
    "> as the first result. How cool!\n",
    "\n",
    "If you've ever used Google, you know this kind of workflow.\n",
    "\n",
    "But now we'd like to perform that across our own data.\n",
    "\n",
    "Let's import our embeddings we created earlier (tk -link to embedding file) and prepare them for use by turning them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([232, 768])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...</td>\n",
       "      <td>2359</td>\n",
       "      <td>440</td>\n",
       "      <td>589.75</td>\n",
       "      <td>[0.00996456947, -0.0654456615, -0.000766414392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>something peculiar — a cat reading a map. For ...</td>\n",
       "      <td>1468</td>\n",
       "      <td>282</td>\n",
       "      <td>367.00</td>\n",
       "      <td>[0.0182670932, -0.0897018537, -0.000311918731,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>But then it struck Mr. Dursley that this was p...</td>\n",
       "      <td>1413</td>\n",
       "      <td>263</td>\n",
       "      <td>353.25</td>\n",
       "      <td>[0.020787783, -0.0894869491, 0.0150764538, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>“ — yes, their son, Harry —”    Mr. Dursley st...</td>\n",
       "      <td>1629</td>\n",
       "      <td>308</td>\n",
       "      <td>407.25</td>\n",
       "      <td>[-0.0119488453, -0.0712201223, -0.0157541279, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>And the old man hugged Mr. Dursley around the ...</td>\n",
       "      <td>1053</td>\n",
       "      <td>204</td>\n",
       "      <td>263.25</td>\n",
       "      <td>[-0.00928694103, -0.0473219007, -0.0184328258,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            4  CHAPTER ONE  THE BOY WHO LIVED     M r. and Mr...   \n",
       "1            5  something peculiar — a cat reading a map. For ...   \n",
       "2            5  But then it struck Mr. Dursley that this was p...   \n",
       "3            6  “ — yes, their son, Harry —”    Mr. Dursley st...   \n",
       "4            6  And the old man hugged Mr. Dursley around the ...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0              2359               440             589.75   \n",
       "1              1468               282             367.00   \n",
       "2              1413               263             353.25   \n",
       "3              1629               308             407.25   \n",
       "4              1053               204             263.25   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.00996456947, -0.0654456615, -0.000766414392...  \n",
       "1  [0.0182670932, -0.0897018537, -0.000311918731,...  \n",
       "2  [0.020787783, -0.0894869491, 0.0150764538, -0....  \n",
       "3  [-0.0119488453, -0.0712201223, -0.0157541279, ...  \n",
       "4  [-0.00928694103, -0.0473219007, -0.0184328258,...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.9646e-03, -6.5446e-02, -7.6641e-04,  8.0540e-03,  4.0280e-02,\n",
       "         3.6609e-02,  1.2012e-02,  4.3946e-02, -8.2864e-03,  6.1500e-02,\n",
       "        -1.6467e-03, -2.7692e-03,  1.2955e-02,  6.8629e-02, -4.3708e-02,\n",
       "         2.7635e-02,  1.2209e-02, -6.3836e-02, -3.7870e-02, -2.4288e-02,\n",
       "         5.2287e-02, -2.0759e-02,  8.9562e-03,  4.9065e-03, -1.0097e-02,\n",
       "        -1.4059e-02,  4.6908e-02,  3.8605e-02, -3.4930e-02,  3.6044e-02,\n",
       "         7.9476e-03, -2.8331e-02,  1.4332e-02,  3.7054e-02, -1.2305e-02,\n",
       "        -1.5795e-02,  5.3710e-02, -2.1542e-02, -6.0529e-03, -1.5297e-02,\n",
       "        -4.1673e-02,  1.5524e-02, -9.2679e-03,  1.5649e-02,  2.7002e-02,\n",
       "        -1.5635e-02, -2.7723e-02,  1.5001e-02,  4.5760e-02,  2.3063e-02,\n",
       "         5.4805e-02, -2.0287e-02,  6.6997e-03,  3.8526e-02, -1.4890e-02,\n",
       "         4.3006e-02,  2.4123e-02,  9.6037e-02, -2.4829e-02, -4.0788e-02,\n",
       "         4.8152e-02,  1.5958e-03, -6.5783e-03,  6.2719e-02,  2.3705e-04,\n",
       "        -2.6114e-02,  1.6223e-03, -2.9285e-02,  8.5217e-03, -8.7613e-03,\n",
       "        -1.9648e-02, -5.3003e-02, -1.4029e-02,  2.7799e-03, -3.6864e-02,\n",
       "        -1.3870e-02, -2.7945e-02,  4.6257e-03, -5.2803e-02,  1.0339e-02,\n",
       "        -2.1799e-02,  6.1327e-02,  3.0443e-02, -4.4226e-03, -4.1352e-04,\n",
       "         2.3038e-02, -3.1976e-03, -6.1652e-02,  2.0697e-02,  4.4739e-02,\n",
       "        -6.4635e-03,  1.0421e-01, -5.6072e-03,  1.0906e-01, -1.7395e-02,\n",
       "        -3.9237e-03, -5.2378e-02,  8.9094e-02,  1.1240e-03, -9.0799e-03,\n",
       "         2.2414e-02, -9.6976e-03, -4.8621e-02, -2.8294e-02, -4.7761e-03,\n",
       "        -2.4586e-02, -2.1087e-02,  6.2507e-03, -3.6955e-02,  3.0511e-03,\n",
       "        -2.4387e-02,  2.5662e-02, -9.0178e-03,  1.0669e-02, -2.1745e-02,\n",
       "         1.6605e-02,  2.0843e-02, -2.0756e-02,  2.2451e-02, -6.5314e-03,\n",
       "         4.6846e-02, -3.2075e-02, -1.2371e-02,  4.9015e-02,  5.2761e-02,\n",
       "         4.5168e-02,  3.3655e-02, -5.8796e-02, -6.5627e-04,  1.0475e-02,\n",
       "        -5.2613e-02,  1.2944e-04,  4.3277e-02, -6.9570e-02, -2.9778e-02,\n",
       "         2.9477e-02, -3.3363e-03,  2.5001e-02,  3.3897e-02,  4.9360e-02,\n",
       "        -1.7613e-02,  4.5422e-02,  5.4504e-02, -5.9408e-03, -2.7420e-02,\n",
       "         2.0611e-02,  7.4773e-03,  4.6916e-02, -3.1066e-02,  1.0817e-01,\n",
       "         6.9110e-02,  4.2710e-04, -3.6638e-02,  2.2300e-03,  1.9722e-03,\n",
       "         2.1818e-03, -1.4050e-02,  2.3859e-02,  1.6091e-02,  2.5956e-02,\n",
       "         3.8186e-03,  3.0027e-02,  4.9866e-02,  6.2093e-03, -3.2561e-02,\n",
       "        -9.6343e-03,  3.4216e-02,  1.1028e-02,  6.5613e-02,  2.8520e-03,\n",
       "        -1.8420e-02,  1.2661e-02,  3.1870e-02, -2.5377e-03,  1.8344e-02,\n",
       "         4.6540e-02,  1.4278e-02, -9.7950e-03,  3.3746e-03,  2.4568e-02,\n",
       "         5.3647e-03,  1.4534e-02,  1.8638e-02,  6.2657e-03, -1.3464e-02,\n",
       "         2.1245e-02, -2.0886e-02, -2.4494e-03, -6.7039e-02,  3.3780e-02,\n",
       "        -4.1758e-02,  2.2803e-02,  3.4039e-02,  2.2634e-02,  5.3047e-02,\n",
       "         2.4955e-02,  6.6032e-02,  8.8168e-03,  2.4656e-02, -7.1176e-02,\n",
       "         2.5668e-02, -4.2083e-02, -1.0484e-02, -6.8508e-02, -7.5945e-02,\n",
       "        -1.9741e-02,  1.1175e-03, -2.6916e-04, -1.0441e-03,  1.4004e-02,\n",
       "        -4.2184e-02, -4.4034e-02, -3.6235e-02, -5.4681e-03,  1.9908e-02,\n",
       "        -1.8830e-02,  3.7478e-02, -9.5872e-03, -2.6176e-02,  2.5133e-03,\n",
       "         2.4328e-02,  4.4975e-03, -3.0102e-02,  3.8063e-02, -4.5508e-02,\n",
       "        -6.9667e-02, -3.7183e-02, -1.2730e-02, -1.7117e-02,  8.3482e-02,\n",
       "        -2.4682e-02,  2.3902e-03, -3.6361e-02, -9.2184e-02,  1.9572e-02,\n",
       "        -8.8869e-02,  2.4625e-03, -1.6958e-02, -7.7532e-03,  3.9017e-02,\n",
       "         1.1877e-02, -1.0673e-02,  3.6510e-02, -2.6976e-02,  1.2138e-02,\n",
       "        -2.1468e-02,  4.8249e-02,  2.2283e-02,  5.6898e-02, -1.8943e-02,\n",
       "         1.9273e-02,  8.2758e-03, -2.5732e-02, -2.4097e-02,  4.8519e-02,\n",
       "         1.2131e-02,  4.2202e-03,  3.3806e-02,  9.8717e-02,  4.2130e-02,\n",
       "        -5.0595e-03,  2.5094e-02, -3.3536e-02,  1.0910e-02, -4.7007e-02,\n",
       "         1.9919e-04, -4.5621e-02, -2.6981e-02, -4.6764e-02, -3.5246e-03,\n",
       "         1.4456e-02, -1.3544e-02, -8.4006e-03,  4.8713e-03, -1.6603e-03,\n",
       "         3.7271e-02,  8.2526e-03,  4.2643e-02,  5.7690e-02, -2.7793e-02,\n",
       "        -1.3677e-02, -3.6334e-02, -1.5026e-02, -8.1217e-03, -1.7957e-02,\n",
       "         2.4726e-02, -1.4639e-02,  1.7700e-03,  1.1758e-02,  7.8834e-03,\n",
       "        -2.0675e-02, -6.0025e-03,  5.6211e-02,  4.6006e-03, -4.4024e-03,\n",
       "         4.2349e-02,  1.1532e-02, -7.1301e-02,  6.9117e-03, -3.3929e-02,\n",
       "        -3.7517e-03, -4.8076e-02, -1.2383e-01,  4.7566e-03, -2.3302e-02,\n",
       "        -8.4716e-03,  4.1677e-02, -1.8608e-03, -2.8208e-03,  1.1243e-02,\n",
       "        -5.8882e-02, -2.5987e-02, -1.0097e-02,  2.3796e-02, -6.1404e-02,\n",
       "        -2.7364e-02, -4.2993e-03, -2.5299e-02,  5.0027e-02,  1.3883e-02,\n",
       "         8.3079e-03, -1.2113e-02, -4.0204e-02, -1.0825e-02,  1.5247e-02,\n",
       "        -5.9642e-02,  8.8385e-03,  1.1133e-02, -1.4892e-02, -7.7578e-03,\n",
       "        -6.4924e-02, -2.0093e-02, -4.1222e-02, -8.7033e-03,  3.3911e-02,\n",
       "         2.5507e-02,  2.2478e-02, -2.3255e-02,  3.5905e-02,  2.0959e-02,\n",
       "        -4.1894e-02,  2.3378e-02, -4.5856e-02,  1.0853e-02,  5.8942e-03,\n",
       "         2.6121e-02,  8.5895e-04,  2.6634e-02, -2.5450e-02, -1.7807e-04,\n",
       "         2.4642e-02, -1.4781e-02, -6.9981e-03,  4.9517e-02, -4.9463e-02,\n",
       "        -3.8395e-03, -3.5294e-02, -2.8795e-02, -2.9091e-02, -5.4136e-02,\n",
       "        -3.2570e-02,  3.3296e-02,  4.5412e-03, -7.9784e-03,  2.4898e-02,\n",
       "        -8.2986e-02, -1.9197e-02,  7.2841e-02,  1.3721e-02, -2.3765e-02,\n",
       "        -3.0048e-02,  2.6802e-02,  4.7206e-02, -4.8804e-02, -6.4868e-03,\n",
       "         8.0953e-03, -3.2517e-02, -8.5788e-02,  6.6307e-02, -1.2815e-02,\n",
       "         1.1337e-02, -1.9493e-02, -4.9629e-02, -2.1185e-02,  3.0387e-02,\n",
       "        -2.3710e-03,  1.1244e-02,  2.7290e-02,  7.5076e-02,  1.4914e-02,\n",
       "        -7.6517e-03, -4.1022e-03, -3.1264e-02, -5.2334e-03,  6.5097e-02,\n",
       "         7.1670e-04, -1.8658e-02, -5.9021e-03,  1.3032e-02,  2.1579e-02,\n",
       "         1.1661e-02,  4.4725e-02,  2.6777e-02, -1.7417e-02, -4.4513e-02,\n",
       "        -2.6020e-02,  9.1717e-03, -1.2520e-03, -3.9426e-02, -9.2618e-02,\n",
       "         5.7379e-02,  5.9283e-02, -6.1073e-03, -8.9587e-02,  4.9581e-02,\n",
       "        -3.7265e-02, -3.9795e-02, -8.5143e-03,  6.9592e-03, -3.2145e-02,\n",
       "         3.9665e-02,  4.5516e-02, -6.1604e-02,  2.4288e-02, -1.3495e-02,\n",
       "        -1.3596e-02, -2.2429e-03, -2.2699e-02, -8.8803e-03, -4.8245e-02,\n",
       "        -1.7736e-02, -9.2959e-03,  8.1046e-03, -2.2761e-02, -1.9506e-03,\n",
       "         1.5534e-02,  4.8609e-02, -3.1177e-02, -1.0076e-02, -3.9403e-03,\n",
       "         1.5792e-02,  1.7498e-02,  4.3439e-02, -9.1314e-02, -5.5280e-04,\n",
       "         3.5783e-02,  2.3441e-02, -9.8514e-03,  1.6092e-02,  2.6935e-02,\n",
       "        -3.2048e-02, -5.9968e-02,  8.8307e-03, -1.5000e-01, -2.8514e-02,\n",
       "        -1.4719e-03, -1.2279e-02, -1.4061e-02, -3.6193e-02, -6.6030e-03,\n",
       "         1.9902e-03,  1.2858e-02, -8.8166e-03,  6.1413e-02,  2.2991e-02,\n",
       "        -1.1403e-02, -2.1954e-02, -2.9824e-02,  1.9600e-02,  3.8448e-02,\n",
       "        -1.4185e-02,  4.8215e-02, -1.3338e-02, -2.9070e-02,  1.7701e-02,\n",
       "         7.7155e-02, -1.6135e-03,  3.6636e-02, -9.0451e-02,  1.7280e-02,\n",
       "        -7.8706e-03,  8.0347e-04, -1.4583e-02, -7.0584e-03, -3.8020e-02,\n",
       "         9.4198e-03, -4.4949e-02,  1.1071e-01, -2.6986e-03, -3.4317e-02,\n",
       "         7.6416e-02, -3.3860e-04,  2.7878e-02,  2.2096e-02,  3.6001e-33,\n",
       "        -4.4719e-02, -6.7421e-02,  6.5984e-03,  4.4078e-03, -2.6280e-02,\n",
       "        -4.6809e-03, -2.5260e-02,  3.0835e-02,  1.9412e-02,  5.1312e-03,\n",
       "        -3.0985e-02,  3.4385e-02,  1.0731e-02,  2.2091e-02, -1.2211e-02,\n",
       "        -7.3512e-03,  8.7754e-02, -4.8498e-02,  1.1021e-02, -1.8160e-02,\n",
       "        -2.5977e-02,  5.4836e-02, -5.7885e-02,  5.5121e-02,  8.9629e-04,\n",
       "         2.2965e-02,  1.7878e-02, -7.9737e-03, -4.1922e-02,  2.6096e-02,\n",
       "         1.1168e-02,  4.5961e-03,  5.4723e-03, -2.1687e-02, -1.1354e-01,\n",
       "        -6.0310e-02,  2.1318e-02,  1.4071e-02, -3.9277e-02, -6.5190e-02,\n",
       "        -2.3060e-03,  1.8596e-02, -1.7021e-02,  3.5484e-02, -5.5903e-02,\n",
       "        -1.7263e-05, -1.0426e-02, -2.9772e-02,  1.5753e-02, -1.4569e-02,\n",
       "         7.4480e-02,  4.9310e-02, -9.1279e-04, -3.1635e-02,  3.2648e-02,\n",
       "         7.2507e-02,  9.2430e-02, -6.1917e-02,  2.8038e-02,  1.5958e-02,\n",
       "         1.6092e-03,  2.4817e-02, -4.8839e-03,  3.4813e-03, -1.9319e-02,\n",
       "         1.5377e-02, -1.4757e-02, -1.6091e-02, -3.5858e-03, -5.5945e-02,\n",
       "         4.7757e-02, -6.6444e-02,  5.6456e-02,  1.8449e-02, -4.5466e-03,\n",
       "         1.0812e-03, -3.5594e-02, -3.1919e-02,  7.9951e-02, -2.1573e-02,\n",
       "        -5.6515e-02, -2.9349e-02,  4.4261e-02,  3.8139e-02, -4.8784e-02,\n",
       "         1.1022e-02,  2.6352e-02,  2.6385e-02, -7.0678e-02, -2.4575e-02,\n",
       "        -4.5390e-02, -2.7656e-02,  1.2478e-02,  1.9274e-03, -5.2622e-03,\n",
       "         2.4415e-02,  1.0133e-02,  1.4438e-02, -2.0448e-02, -1.7892e-02,\n",
       "        -7.4662e-02, -3.4051e-02, -5.5482e-03,  9.5378e-03,  1.3838e-02,\n",
       "         1.6390e-02, -1.6444e-02,  2.7216e-03,  1.5290e-02, -5.5879e-02,\n",
       "         4.2360e-02,  8.7497e-02,  4.1657e-02, -1.0934e-01,  5.2568e-02,\n",
       "        -6.6187e-03, -3.8888e-02,  5.0737e-03,  8.8728e-03,  4.2200e-02,\n",
       "        -3.0073e-02, -3.0152e-02, -1.2447e-02, -3.6140e-02,  5.6924e-02,\n",
       "         1.7334e-02, -3.1657e-02, -2.5273e-02, -4.9243e-02, -7.9343e-02,\n",
       "        -5.1444e-02,  7.4495e-03, -2.6109e-02, -4.3226e-02, -4.0828e-03,\n",
       "        -7.5642e-03,  6.5974e-02, -2.1232e-02, -3.2629e-02, -3.6573e-02,\n",
       "        -4.1798e-03, -1.4119e-03, -1.4885e-03,  3.8742e-02, -3.7592e-03,\n",
       "        -1.9728e-02, -3.6379e-02, -2.6745e-02, -4.3382e-02,  5.8838e-04,\n",
       "        -8.6037e-03, -4.3272e-02,  3.7092e-02, -8.3047e-05, -2.8758e-02,\n",
       "        -9.5483e-03, -2.6148e-02,  1.9655e-02, -4.1397e-02,  4.7218e-02,\n",
       "        -5.3613e-02, -8.2998e-04,  1.8618e-02, -3.1082e-02, -6.9423e-04,\n",
       "         2.9317e-02,  2.4653e-02, -5.3467e-02,  2.3387e-02,  5.0410e-02,\n",
       "        -6.3680e-02, -3.2228e-02,  1.2695e-02,  2.5761e-02,  5.9658e-02,\n",
       "        -1.4898e-03, -3.9541e-03, -1.5056e-02, -9.2197e-03, -7.0601e-02,\n",
       "         2.9640e-04,  1.3806e-02, -5.5681e-02, -3.1804e-02, -3.5382e-02,\n",
       "        -2.2986e-02, -3.8609e-02,  2.5017e-02, -3.5623e-02,  2.2924e-03,\n",
       "         4.3738e-03, -7.0463e-03,  1.7251e-02, -1.1860e-02,  3.4433e-02,\n",
       "         1.7359e-02, -1.1037e-02,  3.8099e-02, -6.9809e-04,  2.8880e-02,\n",
       "         1.3194e-02, -1.2898e-02, -1.2347e-02, -2.6434e-02,  2.4941e-02,\n",
       "         4.4321e-02,  1.7078e-02, -9.9983e-03,  7.9478e-03,  1.8746e-02,\n",
       "        -7.0104e-02,  4.9333e-02, -7.8914e-02, -2.0352e-03, -2.8901e-03,\n",
       "        -2.5382e-02,  1.0635e-02,  5.5542e-02, -3.3942e-02, -2.6644e-02,\n",
       "        -1.8727e-02, -7.7930e-03, -3.2077e-03, -6.2867e-02,  2.9460e-02,\n",
       "         7.6479e-02, -1.6908e-02, -1.2907e-02, -5.9960e-03, -3.2878e-03,\n",
       "        -3.4736e-02,  7.1373e-02,  1.5819e-02,  1.7024e-02,  9.1185e-03,\n",
       "         1.5177e-02,  3.8700e-02,  1.2585e-02,  6.3980e-02,  5.6475e-02,\n",
       "         5.0359e-02,  3.8269e-02,  2.9580e-02, -1.9578e-02,  1.0877e-02,\n",
       "         3.1789e-02, -6.8094e-02,  7.1462e-02, -7.2759e-02,  6.1430e-02,\n",
       "         3.8818e-02,  1.2735e-02, -1.1902e-02,  8.3482e-03,  1.5092e-02,\n",
       "        -9.9723e-03,  6.9971e-02, -3.2728e-02,  2.1362e-04, -1.5316e-02,\n",
       "        -6.2919e-02, -3.0231e-02,  3.6047e-02, -3.3485e-02,  5.3378e-02,\n",
       "        -2.7520e-02, -5.2569e-02, -5.1949e-03,  3.3137e-02,  1.7866e-03,\n",
       "        -3.8718e-02, -1.9067e-02,  3.0663e-02], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding model ready!\n",
    "\n",
    "Time to perform a semantic search.\n",
    "\n",
    "Let's say you were studying the macronutrients.\n",
    "\n",
    "And wanted to search your textbook for \"macronutrients functions\".\n",
    "\n",
    "Well, we can do so with the following steps:\n",
    "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
    "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
    "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
    "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts. \n",
    "\n",
    "Easy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "Time take to get scores on 232 embeddings: 0.00016 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0877, 0.0780, 0.0749, 0.0704, 0.0684], device='cuda:0'),\n",
       "indices=tensor([113, 134,  99, 214, 146], device='cuda:0'))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!! Now that was fast!\n",
    "\n",
    "~0.00008 seconds to perform a dot product comparison across 1680 embeddings on my machine (NVIDIA RTX 4090 GPU).\n",
    "\n",
    "GPUs are optimized for these kinds of operations.\n",
    "\n",
    "So even if you we're to increase our embeddings by 100x (1680 -> 168,000), an exhaustive dot product operation would happen in ~0.008 seconds (assuming linear scaling).\n",
    "\n",
    "Heck, let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([23200, 768])\n",
      "Time take to get scores on 23200 embeddings: 0.00060 seconds.\n"
     ]
    }
   ],
   "source": [
    "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
    "\n",
    "# Perform dot product across 168,000 embeddings\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. That's quick!\n",
    "\n",
    "That means we can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
    "\n",
    "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Let's check the results of our original similarity search.\n",
    "\n",
    "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
    "\n",
    "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
    "\n",
    "We can use those indicies to map back to our text chunks.\n",
    "\n",
    "First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n",
    "\n",
    "Sounds like a lot but we can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'macronutrients functions'\n",
      "\n",
      "Results:\n",
      "Score: 0.0877\n",
      "Text:\n",
      "out at odd angles.   “Stick out your right hand over your broom,” called Madam\n",
      "Hooch at the front, “and say ‘Up!’”   “UP” everyone shouted.   Harry’s broom\n",
      "jumped into his hand at once, but it was one of the few that did. Hermione\n",
      "Granger’s had simply rolled over on the ground, and Neville’s hadn’t moved at\n",
      "all. Perhaps brooms, like horses, could tell when you were afraid, thought\n",
      "Harry; there was a quaver in Neville’s voice that said only too clearly that he\n",
      "wanted to keep his feet on the ground.   Madam Hooch then showed them how to\n",
      "mount their brooms without sliding off the end, and walked up and down the rows\n",
      "correcting their grips. Harry and Ron were delighted when she told Malfoy he’d\n",
      "been doing it wrong for years.   “Now, when I blow my whistle, you kick off from\n",
      "the ground, hard,” said Madam Hooch. “Keep your brooms steady, rise a few feet,\n",
      "and then come straight back down by leaning forward slightly. On my whistle —\n",
      "three — two —”    But Neville, nervous and jumpy and frightened of being left on\n",
      "the ground, pushed off hard before the whistle had touched Madam Hooch’s lips.\n",
      "“Come back, boy!”she shouted, but Neville was rising straight up like a cork\n",
      "shot out of a bottle — twelve feet — twenty feet. Harry saw his scared white\n",
      "face look down at the ground falling away, saw him gasp, slip sideways off the\n",
      "broom and —    WHAM — a thud and a nasty crack and Neville lay facedown on the\n",
      "grass in a heap. His broomstick was still rising higher and higher, and started\n",
      "to drift lazily toward the forbidden forest and out of sight.   Madam Hooch was\n",
      "bending over Neville, her face as white as his.   “Broken wrist,” Harry heard\n",
      "her mutter. “Come on, boy — it’s all right, up you get.”   She turned to the\n",
      "rest of the class.   “None of you is to move while I take this boy to the\n",
      "hospital wing!\n",
      "Page number: 106\n",
      "\n",
      "\n",
      "Score: 0.0780\n",
      "Text:\n",
      "Flushed with their victory, they started to run back up the passage, but as they\n",
      "reached the corner they heard something that made their hearts stop — a high,\n",
      "petrified scream — and it was coming from the chamber they’d just chained up.\n",
      "“Oh, no,” said Ron, pale as the Bloody Baron.   “It’s the girls’ bathroom!”Harry\n",
      "gasped.   “Hermione!”they said together.   It was the last thing they wanted to\n",
      "do, but what choice did they have?Wheeling around, they sprinted back to the\n",
      "door and turned the key, fumbling in their panic. Harry pulled the door open and\n",
      "they ran inside.   Hermione Granger was shrinking against the wall opposite,\n",
      "looking as if she was about to faint. The troll was advancing on her, knocking\n",
      "the sinks off the walls as it went.   “Confuse it!”Harry said desperately to\n",
      "Ron, and, seizing a tap, he threw it as hard as he could against the wall.   The\n",
      "troll stopped a few feet from Hermione. It lumbered around, blinking stupidly,\n",
      "to see what had made the noise. Its mean little eyes saw Harry. It\n",
      "Page number: 125\n",
      "\n",
      "\n",
      "Score: 0.0749\n",
      "Text:\n",
      "Harry looked over at the Slytherin table and saw a horrible ghost sitting there,\n",
      "with blank staring eyes, a gaunt face, and robes stained with silver blood. He\n",
      "was right next to Malfoy who, Harry was pleased to see, didn’t look too pleased\n",
      "with the seating arrangements.   “How did he get covered in blood?”asked Seamus\n",
      "with great interest.   “I’ve never asked,” said Nearly Headless Nick delicately.\n",
      "When everyone had eaten as much as they could, the remains of the food faded\n",
      "from the plates, leaving them sparkling clean as before. A moment later the\n",
      "desserts appeared. Blocks of ice cream in every flavor you could think of, apple\n",
      "pies, treacle tarts, chocolate eclairs and jam doughnuts, trifle, strawberries,\n",
      "Jell-O, rice pudding…    As Harry helped himself to a treacle tart, the talk\n",
      "turned to their families.   “I’m half-and-half,” said Seamus. “Me dad’s a\n",
      "Muggle. Mom didn’t tell him she was a witch ’til after they were married. Bit of\n",
      "a nasty shock for him.”   The others laughed.   “What about you, Neville?”said\n",
      "Ron.   “Well, my gran brought me up and she’s a witch,” said Neville, “but the\n",
      "family thought I was all-Muggle for ages. My Great Uncle Algie kept trying to\n",
      "catch me off my guard and force some magic out of me — he pushed me off the end\n",
      "of Blackpool pier once, I nearly drowned — but nothing happened until I\n",
      "Page number: 89\n",
      "\n",
      "\n",
      "Score: 0.0704\n",
      "Text:\n",
      "and up the next passageway.   “What if he’s —?”   “He’ll be all right,” said\n",
      "Harry, trying to convince himself. “What do you reckon’s next?”   “We’ve had\n",
      "Sprout’s, that was the Devil’s Snare; Flitwick must’ve put charms on the keys;\n",
      "McGonagall transfigured the chessmen to make them alive; that leaves Quirrell’s\n",
      "spell, and Snape’s.”   They had reached another door.   “All right?”Harry\n",
      "whispered.   “Go on.”   Harry pushed it open.   A disgusting smell filled their\n",
      "nostrils, making both of them pull their robes up over their noses. Eyes\n",
      "watering, they saw, flat on the floor in front of them, a troll even larger than\n",
      "the one they had tackled, out cold with a bloody lump on its head.   “I’m glad\n",
      "we didn’t have to fight that one,” Harry whispered as they stepped carefully\n",
      "over one of its massive legs. “Come on, I can’t breathe.”   He pulled open the\n",
      "next door, both of them hardly daring to look at what came next - but there was\n",
      "nothing very frightening in here, just a table with seven differently shaped\n",
      "bottles standing on it in a line.   “Snape’s,” said Harry. “What do we have to\n",
      "do?”   They stepped over the threshold, and immediately a fire sprang up behind\n",
      "them in the doorway. It wasn’t ordinary fire either; it was purple. At the same\n",
      "instant, black flames shot up in the doorway leading onward.\n",
      "Page number: 203\n",
      "\n",
      "\n",
      "Score: 0.0684\n",
      "Text:\n",
      "obviously hoping to catch him if he fell. Marcus Flint seized the Quaffle and\n",
      "scored five times without anyone noticing.   “Come on, Hermione,” Ron muttered\n",
      "desperately.   Hermione had fought her way across to the stand where Snape\n",
      "stood, and was now racing along the row behind him; she didn’t even stop to say\n",
      "sorry as she knocked Professor Quirrell headfirst into the row in front.\n",
      "Reaching Snape, she crouched down, pulled out her wand, and whispered a few,\n",
      "well-chosen words. Bright blue flames shot from her wand onto the hem of Snape’s\n",
      "robes.   It took perhaps thirty seconds for Snape to realize that he was on\n",
      "fire. A sudden yelp told her she had done her job. Scooping the fire off him\n",
      "into a little jar in her pocket, she scrambled back along the row — Snape would\n",
      "never know what had happened.   It was enough. Up in the air, Harry was suddenly\n",
      "able to clamber back on to his broom.   “Neville, you can look!”Ron said.\n",
      "Neville had been sobbing into Hagrid’s jacket for the last five minutes.   Harry\n",
      "was speeding toward the ground when the crowd saw him clap his hand to his mouth\n",
      "as though he was about to be sick — he hit the field on all fours — coughed —\n",
      "and something gold fell into his hand.   “I’ve got the Snitch!”he shouted,\n",
      "waving it above his head, and the game ended in complete confusion.   “He didn’t\n",
      "catch it, he nearly swallowed it,” Flint was still howling twenty minutes later,\n",
      "but it made no difference — Harry hadn’t broken any rules and Lee Jordan was\n",
      "still happily shouting the results — Gryffindor had won by one hundred and\n",
      "seventy points to sixty. Harry heard none of this, though. He was being made a\n",
      "cup of strong tea back in Hagrid’s hut, with Ron and Hermione.\n",
      "Page number: 137\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first result looks to have nailed it!\n",
    "\n",
    "We get a very relevant answer to our query `\"macronutrients functions\"` even though its quite vague.\n",
    "\n",
    "That's the power of semantic search!\n",
    "\n",
    "And even better, if we wanted to inspect the result further, we get the page number where the text appears.\n",
    "\n",
    "How about we check the page to verify?\n",
    "\n",
    "We can do so by loading the page number containing the highest result (page 5 but really page 5 + 41 since our PDF page numbers start on page 41)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Now we can do extra research if we'd like.\n",
    "\n",
    "We could repeat this workflow for any kind of query we'd like on our textbook.\n",
    "\n",
    "And it would also work for other datatypes too.\n",
    "\n",
    "We could use semantic search on customer support documents.\n",
    "\n",
    "Or email threads.\n",
    "\n",
    "Or company plans.\n",
    "\n",
    "Or our old journal entries.\n",
    "\n",
    "Almost anything!\n",
    "\n",
    "The workflow is the same:\n",
    "\n",
    "`ingest documents -> split into chunks -> embed chunks -> make a query -> embed the query -> compare query embedding to chunk embeddings`\n",
    "\n",
    "And we get relevant resources *along with* the source they came from!\n",
    "\n",
    "That's the **retrieval** part of Retrieval Augmented Generation (RAG).\n",
    "\n",
    "Before we get to the next two steps, let's take a small aside and discuss similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measures: dot product and cosine similarity \n",
    "\n",
    "Let's talk similarity measures between vectors.\n",
    "\n",
    "Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n",
    "\n",
    "Two of the most common you'll across are the dot product and cosine similarity.\n",
    "\n",
    "They are quite similar.\n",
    "\n",
    "The main difference is that cosine similarity has a normalization step.\n",
    "\n",
    "| Similarity measure | Description | Code |\n",
    "| ----- | ----- | ----- |\n",
    "| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) | \n",
    "| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n",
    "\n",
    "For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude. \n",
    "\n",
    "In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n",
    "\n",
    "To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n",
    "\n",
    "> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vector1 and vector2: tensor(14.)\n",
      "Dot product between vector1 and vector3: tensor(32.)\n",
      "Dot product between vector1 and vector4: tensor(-14.)\n",
      "Cosine similarity between vector1 and vector2: tensor(1.0000)\n",
      "Cosine similarity between vector1 and vector3: tensor(0.9746)\n",
      "Cosine similarity between vector1 and vector4: tensor(-1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Example tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n",
    "\n",
    "Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity). \n",
    "\n",
    "But comparing `vector1` and `vector4` the result is in the negative direction.\n",
    "\n",
    "This makes sense because `vector4` is the negative version of `vector1`.\n",
    "\n",
    "Whereas comparing `vector1` and `vector3` shows a different outcome.\n",
    "\n",
    "For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n",
    "\n",
    "However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n",
    "\n",
    "It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n",
    "\n",
    "And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n",
    "\n",
    "The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n",
    "\n",
    "So we can continue using the dot product (cosine similarity is dot product + normalization).\n",
    "\n",
    "With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionizing our semantic search pipeline\n",
    "\n",
    "Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now let's test our functions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 232 embeddings: 0.00006 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.0560, 0.0524, 0.0515, 0.0450, 0.0406], device='cuda:0'),\n",
       " tensor([225, 111, 227,  13,  15], device='cuda:0'))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"harry, ron and hermoine\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 232 embeddings: 0.00006 seconds.\n",
      "Query: harry, ron and hermoine\n",
      "\n",
      "Results:\n",
      "Score: 0.0560\n",
      "“But why couldn’t Quirrell touch me?”   “Your mother died to save you. If there\n",
      "is one thing Voldemort cannot understand, it is love. He didn’t realize that\n",
      "love as powerful as your mother’s for you leaves its own mark. Not a scar, no\n",
      "visible sign…to have been loved so deeply, even though the person who loved us\n",
      "is gone, will give us some protection forever. It is in your very skin.\n",
      "Quirrell, full of hatred, greed, and ambition, sharing his soul with Voldemort,\n",
      "could not touch you for this reason. It was agony to touch a person marked by\n",
      "something so good.”   Dumbledore now became very interested in a bird out on the\n",
      "windowsill, which gave Harry time to dry his eyes on the sheet. When he had\n",
      "found his voice again, Harry said, “And the invisibility cloak — do you know who\n",
      "sent it to me?”   “Ah — your father happened to leave it in my possession, and I\n",
      "thought you might like it.”Dumbledore’s eyes twinkled. “Useful things…your\n",
      "father used it mainly for sneaking off to the kitchens to steal food when he was\n",
      "here.”   “And there’s something else…”    “Fire away.”   “Quirrell said Snape —”\n",
      "“Professor Snape, Harry.”   “Yes, him — Quirrell said he hates me because he\n",
      "hated my father. Is that true?”   “Well, they did rather detest each other. Not\n",
      "unlike yourself and Mr. Malfoy. And then, your father did something Snape could\n",
      "never forgive.”\n",
      "Page number: 214\n",
      "\n",
      "\n",
      "Score: 0.0524\n",
      "CHAPTER NINE  THE MIDNIGHT DUEL  H arry had never believed he would meet a boy\n",
      "he hated more than Dudley, but that was before he met Draco Malfoy. Still,\n",
      "first-year Gryffindors only had Potions with the Slytherins, so they didn’t have\n",
      "to put up with Malfoy much. Or at least, they didn’t until they spotted a notice\n",
      "pinned up in the Gryffindor common room that made them all groan. Flying lessons\n",
      "would be starting on Thursday — and Gryffindor and Slytherin would be learning\n",
      "together.   “Typical,” said Harry darkly. “Just what I always wanted. To make a\n",
      "fool of myself on a broomstick in front of Malfoy.”   He had been looking\n",
      "forward to learning to fly more than anything else.   “You don’t know that\n",
      "you’ll make a fool of yourself,” said Ron reasonably. “Anyway, I know Malfoy’s\n",
      "always going on about how good he is at Quidditch, but I bet that’s all talk.”\n",
      "Malfoy certainly did talk about flying a lot. He complained loudly about first\n",
      "years never getting on the house Quidditch teams and told long, boastful stories\n",
      "that always seemed to end with him narrowly escaping Muggles in helicopters. He\n",
      "wasn’t the only one, though: the way Seamus Finnigan told it, he’d spent most of\n",
      "his childhood zooming around the countryside on his broomstick. Even Ron would\n",
      "tell anyone who’d listen about the time he’d almost hit a hang glider on\n",
      "Charlie’s old broom. Everyone from wizarding families talked about Quidditch\n",
      "constantly. Ron had already had a big argument with Dean Thomas, who shared\n",
      "their dormitory, about soccer. Ron couldn’t see what was exciting about a game\n",
      "with only one ball where no one was allowed to fly. Harry had caught Ron\n",
      "prodding Dean’s poster of West Ham soccer team, trying to make the players move.\n",
      "Neville had never been on a broomstick in his life, because his grandmother had\n",
      "never let him near one. Privately, Harry felt she’d had good reason, because\n",
      "Neville managed to have an extraordinary number of accidents even with both feet\n",
      "on the ground.\n",
      "Page number: 104\n",
      "\n",
      "\n",
      "Score: 0.0515\n",
      "“D’you think he meant you to do it?”said Ron. “Sending you your father’s cloak\n",
      "and everything?”   “Well, ” Hermione exploded, “if he did — I mean to say that’s\n",
      "terrible — you could have been killed.”   “No, it isn’t,” said Harry\n",
      "thoughtfully. “He’s a funny man, Dumbledore. I think he sort of wanted to give\n",
      "me a chance. I think he knows more or less everything that goes on here, you\n",
      "know. I reckon he had a pretty good idea we were going to try, and instead of\n",
      "stopping us, he just taught us enough to help. I don’t think it was an accident\n",
      "he let me find out how the mirror worked. It’s almost like he thought I had the\n",
      "right to face Voldemort if I could….”   “Yeah, Dumbledore’s off his rocker, all\n",
      "right,” said Ron proudly. “Listen, you’ve got to be up for the end-of-year feast\n",
      "tomorrow. The points are all in and Slytherin won, of course — you missed the\n",
      "last Quidditch match, we were steamrollered by Ravenclaw without you — but the\n",
      "food’ll be good.”   At that moment, Madam Pomfrey bustled over.   “You’ve had\n",
      "nearly fifteen minutes, now OUT” she said firmly. After a good night’s sleep,\n",
      "Harry felt nearly back to normal.   I want to go to the feast,” he told Madam\n",
      "Pomfrey as she straightened his many candy boxes. I can, can’t I?”   “Professor\n",
      "Dumbledore says you are to be allowed to go,” she said stiffly, as though in her\n",
      "opinion Professor Dumbledore didn’t realize how risky feasts could be. “\n",
      "Page number: 216\n",
      "\n",
      "\n",
      "Score: 0.0450\n",
      "“Yes,” said Professor McGonagall. “And I don’t suppose you’re going to tell me\n",
      "why you’re here, of all places?”   “I’ve come to bring Harry to his aunt and\n",
      "uncle. They’re the only family he has left now.”   “You don’t mean – you can’t\n",
      "mean the people who live here?”cried Professor McGonagall, jumping to her feet\n",
      "and pointing at number four. “Dumbledore — you can’t. I’ve been watching them\n",
      "all day. You couldn’t find two people who are less like us. And they’ve got this\n",
      "son — I saw him kicking his mother all the way up the street, screaming for\n",
      "sweets. Harry Potter come and live here!”   “It’s the best place for him,” said\n",
      "Dumbledore firmly. “His aunt and uncle will be able to explain everything to him\n",
      "when he’s older. I’ve written them a letter.”   “A letter?”repeated Professor\n",
      "McGonagall faintly, sitting back down on the wall. “Really, Dumbledore, you\n",
      "think you can explain all this in a letter?These people will never understand\n",
      "him!He’ll be famous — a legend — I wouldn’t be surprised if today was known as\n",
      "Harry Potter day in the future — there will be books written about Harry — every\n",
      "child in our world will know his name!”   “Exactly.”\n",
      "Page number: 11\n",
      "\n",
      "\n",
      "Score: 0.0406\n",
      "“Even if I could, I wouldn’t. Scars can come in handy. I have one myself above\n",
      "my left knee that is a perfect map of the London Underground. Well — give him\n",
      "here, Hagrid — we’d better get this over with.”   Dumbledore took Harry in his\n",
      "arms and turned toward the Dursleys’ house.   “Could I — could I say good-bye to\n",
      "him, sir?”asked Hagrid. He bent his great, shaggy head over Harry and gave him\n",
      "what must have been a very scratchy, whiskery kiss. Then, suddenly, Hagrid let\n",
      "out a howl like a wounded dog.   “Shhh!”hissed Professor McGonagall, “You’ll\n",
      "wake the Muggles!”   “S-s-sorry,” sobbed Hagrid, taking out a large, spotted\n",
      "handkerchief and burying his face in it. “But I c-c-can’t stand it —Lily an’\n",
      "James dead — an’ poor little Harry off ter live with Muggles —”    “Yes, yes,\n",
      "it’s all very sad, but get a grip on yourself, Hagrid, or we’ll be found,”\n",
      "Professor McGonagall whispered, patting Hagrid gingerly on the arm as Dumbledore\n",
      "stepped over the low garden wall and walked to the front door. He laid Harry\n",
      "gently on the doorstep, took a letter out of his cloak, tucked it inside Harry’s\n",
      "blankets, and then came back to the other two. For a full minute the three of\n",
      "them stood and looked at the little bundle; Hagrid’s shoulders shook, Professor\n",
      "McGonagall blinked furiously, and the twinkling light that usually shone from\n",
      "Dumbledore’s eyes seemed to have gone out.   “Well,” said Dumbledore finally,\n",
      "“that’s that. We’ve no business staying here. We may as well go and join the\n",
      "celebrations.”   “Yeah,” said Hagrid in a very muffled voice, “I’ll be takin’\n",
      "Sirius his bike back. G’night, Professor McGonagall — Professor Dumbledore,\n",
      "sir.”\n",
      "Page number: 13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search/vector search extensions \n",
    "\n",
    "We've covered an exmaple of using embedding vector search to find relevant results based on a query.\n",
    "\n",
    "However, you could also add to this pipeline with traditional keyword search.\n",
    "\n",
    "Many modern search systems use keyword and vector search in tandem.\n",
    "\n",
    "Our dataset is small and allows for an exhaustive search (comparing the query to *every* possible result) but if you start to work with large scale datasets with hundred of thousands, millions or even billions of vectors, you'll want to implement an index.\n",
    "\n",
    "You can think of an index as sorting your embeddings before you search through them.\n",
    "\n",
    "So it narrows down the search space.\n",
    "\n",
    "For example, it would be inefficient to search every word in the dictionary to find the word \"duck\", instead you'd go straight to the letter D, perhaps even straight to the back half of the letter D, find words close to \"duck\" before finding it.\n",
    "\n",
    "That's how an index can help search through many examples without comprimising too much on speed or quality (for more on this, check out [nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)).\n",
    "\n",
    "One of the most popular indexing libraries is [Faiss](https://github.com/facebookresearch/faiss). \n",
    "\n",
    "Faiss is open-source and was originally created by Facebook to deal with internet-scale vectors and implements many algorithms such as [HNSW](https://arxiv.org/abs/1603.09320) (Hierarchical Naviganle Small Worlds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an LLM for local generation\n",
    "\n",
    "We're got our retrieval pipeline ready, let's now get the generation side of things happening.\n",
    "\n",
    "To perform generation, we're going to use a Large Language Model (LLM).\n",
    "\n",
    "LLMs are designed to generate an output given an input.\n",
    "\n",
    "In our case, we want our LLM to generate and output of text given a input of text.\n",
    "\n",
    "And more specifically, we want the output of text to be generated based on the context of relevant information to the query.\n",
    "\n",
    "The input to an LLM is often referred to as a prompt.\n",
    "\n",
    "We'll augment our prompt with a query as well as context from our textbook related to that query.\n",
    "\n",
    "> **Which LLM should I use?**\n",
    "\n",
    "There are many LLMs available.\n",
    "\n",
    "Two of the main questions to ask from this is:\n",
    "1. Do I want it to run locally? \n",
    "2. If yes, how much compute power can I dedicate?\n",
    "\n",
    "If you're after the absolute best performance, you'll likely want to use an API (not running locally) such as GPT-4 or Claude 3. However, this comes with the tradeoff of sending your data away and then awaiting a response.\n",
    "\n",
    "For our case, since we want to set up a local pipeline and run it on our own GPU, we'd answer \"yes\" to the first question and then the second question will depend on what hardware we have available.\n",
    "\n",
    "To find open-source LLMs, one great resource is the [Hugging Face open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "The leaderboard compares many of the latest and greatest LLMs on various benchmarks.\n",
    "\n",
    "Another great resource is [TheBloke on Hugging Face](https://huggingface.co/TheBloke), an account which provides an extensive range of quantized (models that have been made smaller) LLMs.\n",
    "\n",
    "A rule of thumb for LLMs (and deep learning models in general) is that the higher the number of parameters, the better the model performs. \n",
    "\n",
    "It may be tempting to go for the largest size model (e.g. a 70B parameter model rather than a 7B parameter model) but a larger size model may not be able to run on your available hardware.\n",
    "\n",
    "The following table gives an insight into how much GPU memory you'll need to load an LLM with different sizes and different levels of [numerical precision](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n",
    "\n",
    "They are based on the fact that 1 float32 value (e.g. `0.69420`) requires 4 bytes of memory and 1GB is approximately 1,000,000,000 (one billion) bytes.\n",
    "\n",
    "| Model Size (Billion Parameters) | Float32 VRAM (GB) | Float16 VRAM (GB) | 8-bit VRAM (GB) | 4-bit VRAM (GB) |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| 1B                              | ~4                | ~2                | ~1              | ~0.5            |\n",
    "| 7B (e.g., [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b), [Gemma 7B](https://huggingface.co/google/gemma-7b-it), [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1))             | ~28               | ~14               | ~7              | ~3.5            |\n",
    "| 10B                             | ~40               | ~20               | ~10             | ~5              |\n",
    "| 70B (e.g, Llama 2 70B)          | ~280              | ~140              | ~70             | ~35             |\n",
    "| 100B                            | ~400              | ~200              | ~100            | ~50             |\n",
    "| 175B                            | ~700              | ~350              | ~175            | ~87.5           |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Note:** Loading a model in a lower precision (e.g. 8-bit instead of float16) generally lowers performance. Lower precision can help to reduce computing requirements, however sometimes the performance degradation in terms of model output can be substantial. Finding the right speed/performance tradeoff will often require many experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    "\n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
    "\n",
    "> **Note:** You can also check this with the `!nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok wonderful!\n",
    "\n",
    "I'm running this notebook with a NVIDIA RTX 4090, so I've got 24GB of VRAM available.\n",
    "\n",
    "However, this may be different on your end.\n",
    "\n",
    "Looking at the table above, it seems we can run a ~7-10B parameter model in float16 precision pretty comfortably.\n",
    "\n",
    "But we could also run a smaller one if we'd like.\n",
    "\n",
    "Let's try out the recently released (at the time of writing, March 2024) LLM from Google, [Gemma](https://huggingface.co/blog/gemma).\n",
    "\n",
    "Specifically, we'll use the `gemma-7b-it` version which stands for Gemma 7B Instruction-Tuned.\n",
    "\n",
    "Instruction tuning is the process of tuning a raw language model to follow instructions.\n",
    "\n",
    "These are the kind of models you'll find in most chat-based assistants such as ChatGPT, Gemini or Claude.\n",
    "\n",
    "The following table shows different amounts of GPU memory requirements for different verions of the Gemma LLMs with varying levels of precision.\n",
    "\n",
    "| Model             | Precision | Min-Memory (Bytes) | Min-Memory (MB) | Min-Memory (GB) | Recommended Memory (GB) | Hugging Face ID |\n",
    "|-------------------|-----------|----------------|-------------|-------------| ----- | ----- |\n",
    "| [Gemma 2B](https://huggingface.co/google/gemma-2b-it)          | 4-bit     | 2,106,749,952  | 2009.15     | 1.96        | ~5.0 | [`gemma-2b`](https://huggingface.co/google/gemma-2b) or [`gemma-2b-it`](https://huggingface.co/google/gemma-2b-it) for instruction tuned version | \n",
    "| Gemma 2B          | Float16   | 5,079,453,696  | 4844.14     | 4.73        | ~8.0 | Same as above |\n",
    "| [Gemma 7B](https://huggingface.co/google/gemma-7b-it)          | 4-bit     | 5,515,859,968  | 5260.33     | 5.14        | ~8.0 | [`gemma-7b`](https://huggingface.co/google/gemma-7b) or [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) for instruction tuned version |\n",
    "| Gemma 7B          | Float16   | 17,142,470,656 | 16348.33    | 15.97       | ~19 | Same as above |\n",
    "\n",
    "> **Note:** `gemma-7b-it` means \"instruction tuned\", as in, a base LLM (`gemma-7b`) has been fine-tuned to follow instructions, similar to [`Mistral-7B-v0.1`](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [`Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
    "> \n",
    "> There are also further quantized and smaller variants of Gemma (and other LLMs) available in various formats such as GGUF. You can see many of these on [TheBloke account on Hugging Face](https://huggingface.co/TheBloke).\n",
    "> \n",
    "> The version of LLM you choose to use will be largely based on project requirements and experimentation.\n",
    "\n",
    "Based on the table above, let's write a simple if/else statement which recommends which Gemma variant we should look into using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8 | Recommended model: Gemma 2B in 4-bit precision.\n",
      "use_quantization_config set to: True\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally\n",
    "\n",
    "Alright! Looks like `gemma-7b-it` it is (for my local machine with an RTX 4090, change the `model_id` and `use_quantization_config` values to suit your needs)! \n",
    "\n",
    "There are plenty of examples of how to load the model on the `gemma-7b-it` [Hugging Face model card](https://huggingface.co/google/gemma-7b-it).\n",
    "\n",
    "Good news is, the Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n",
    "\n",
    "To load our LLM, we're going to need a few things:\n",
    "1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the [`transformers.BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/quantization#transformers.BitsAndBytesConfig) class (requires installing the [`bitsandbytes` library](https://github.com/TimDettmers/bitsandbytes)).\n",
    "2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-7b-it`.\n",
    "3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n",
    "4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n",
    "\n",
    "As a bonus, we'll check if [Flash Attention 2](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) is available using `transformers.utils.is_flash_attn_2_available()`. Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the [GitHub repo](https://github.com/Dao-AILab/flash-attention). \n",
    "\n",
    "> **Note:** Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with [`torch.cuda.get_device_capability(0)`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html). \n",
    "\n",
    "> **Note:** To get access to the Gemma models, you will have to [agree to the terms & conditions](https://huggingface.co/google/gemma-7b-it) on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the [Hugging Face CLI/Hugging Face Hub `login()` function](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication). Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to the \"Secrets\" tab.\n",
    ">\n",
    "> Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got an LLM!\n",
    "\n",
    "Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ok a bunch of layers ranging from embedding layers to attention layers (see the `GemmaFlashAttention2` layers!) to MLP and normalization layers.\n",
    "\n",
    "The good news is that we don't have to know too much about these to use the model.\n",
    "\n",
    "How about we get the number of parameters in our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515268096"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, turns out that Gemma 7B is really Gemma 8.5B.\n",
    "\n",
    "It pays to do your own investigations!\n",
    "\n",
    "How about we get the models memory requirements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2039631872, 'model_mem_mb': 1945.14, 'model_mem_gb': 1.9}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with our LLM\n",
    "\n",
    "We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n",
    "\n",
    "The tokenized input comes from passing a string of text to our `tokenizer`.\n",
    "\n",
    "It's important to note that you should use a tokenizer that has been paired with a model.\n",
    "\n",
    "Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n",
    "\n",
    "For some LLMs, there's a specific template you should pass to them for ideal outputs.\n",
    "\n",
    "For example, the `gemma-7b-it` model has been trained in a dialogue fashion (instruction tuning).\n",
    "\n",
    "In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model.\n",
    "\n",
    "Let's try it out.\n",
    "\n",
    "> **Note:** The following demo has been modified from the Hugging Face model card for [Gemma 7B](https://huggingface.co/google/gemma-7b-it). Many similar demos of usage are available on the model cards of similar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n",
    "\n",
    "Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n",
    "\n",
    "We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n",
    "\n",
    "Let's generate some text! \n",
    "\n",
    "We'll time it for fun with the `%%time` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "         184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "            573,   3515,   2971, 235336,    107,    108,    106,   2516,    108]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "        184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "           573,   3515,   2971, 235336,    107,    108,    106,   2516,    108,\n",
      "         21404, 235269,   1517, 235303, 235256,    476,  25497,    576,    573,\n",
      "        186809, 184592,    578,   1024,  16065,    575,    573,   3515,   2971,\n",
      "        235292,    109,    688,  12298,   1695, 184592,  66058,    109, 235287,\n",
      "          5231, 156615,  56227,  66058,   5626,   2971,   7177,  72780,    604,\n",
      "          4134, 235265,   2365,    708,    573,   7920,   9719,    604,   1167,\n",
      "          5999,    578,  29703, 235265,    108, 235287,   5231,  49471,  66058,\n",
      "         33849,    603,   8727,    604,   4547,    578,  68808,  29703, 235269,\n",
      "          3547,  44760, 235269,    578,  17839,  53186, 235265,    108, 235287,\n",
      "          5231,  33690,  66058,  22904,   6572,   4134, 235269,   7154,  33398,\n",
      "         48765, 235269,    578,   7154,    577,   2029,   7459,    573,   2971,\n",
      "        235265,    109,    688,  12298,   1695,   7208,    579, 152614,  66058,\n",
      "           109, 235287,   5231, 156615,  56227,  66058,   5626,  13266,   1476,\n",
      "          2449, 235248, 235310, 235308, 235290, 235318, 235308, 235358,    576,\n",
      "          1167,   3051,  34366,    774,  72780, 235265,    108, 235287,   5231,\n",
      "         49471,  66058,   1448,   1476,   2449, 235248, 235274, 235265, 235318,\n",
      "        235290, 235284, 235265, 235284,  27491,    576,   9646,    842,  77180,\n",
      "           576,   2971,   5171,    842,   1744, 235265,    108, 235287,   5231,\n",
      "         33690,  66058,   1448,   1476,   2449, 235248, 235284, 235276, 235290,\n",
      "        235304, 235276, 235358,    576,   1167,   3051,  34366,    774,   6181,\n",
      "        235265,    109,    688,   2299,  97586, 184592,   5624,  32119,  66058,\n",
      "           109,  12298,   1695, 184592,   1160,   3584,    577,   3658,    573,\n",
      "          2971,    675,    573,   4134,    578,   4547,  13854,    665,   4026,\n",
      "           577,   1411,  10338, 235265,   1699,   3287, 235292,    109, 235287,\n",
      "        110165,  56227,    708,  10127,   1706,   1280,  30859, 235269,    948,\n",
      "           603,   1671,    731,   5999,    604,   4134, 235265,    108, 235287,\n",
      "         33849,    603,   1671,    577,   2500,    578,  12158,  29703, 235269,\n",
      "          7872,  44760, 235269,    578,   7872,  53186, 235265,    108, 235287,\n",
      "         22904,    603,   1671,    577,   4659,   4134,    578,   1707,    577,\n",
      "          2029,   7459,    573,   2971], device='cuda:0')\n",
      "\n",
      "CPU times: total: 2.33 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We just generated some text on our local GPU!\n",
    "\n",
    "Well not just yet...\n",
    "\n",
    "Our LLM accepts tokens in and sends tokens back out.\n",
    "\n",
    "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:** Our body uses carbohydrates for energy. They are the primary fuel for our cells and tissues.\n",
      "* **Protein:** Protein is essential for building and repairing tissues, making enzymes, and producing hormones.\n",
      "* **Fat:** Fat provides energy, helps absorb vitamins, and helps to insulate the body.\n",
      "\n",
      "**Macronutrient Ratios:**\n",
      "\n",
      "* **Carbohydrates:** Our bodies need around 45-65% of our total calories from carbohydrates.\n",
      "* **Protein:** We need around 1.6-2.2 grams of protein per kilogram of body weight per day.\n",
      "* **Fat:** We need around 20-30% of our total calories from fat.\n",
      "\n",
      "**How Macronutrients Work Together:**\n",
      "\n",
      "Macronutrients work together to provide the body with the energy and building blocks it needs to function properly. For example:\n",
      "\n",
      "* Carbohydrates are broken down into glucose, which is used by cells for energy.\n",
      "* Protein is used to build and repair tissues, produce enzymes, and produce hormones.\n",
      "* Fat is used to store energy and help to insulate the body\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That looks like a pretty good answer.\n",
    "\n",
    "But notice how the output contains the prompt text as well?\n",
    "\n",
    "How about we do a little formatting to replace the prompt in the output text?\n",
    "\n",
    "> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Output text:\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:** Our body uses carbohydrates for energy. They are the primary fuel for our cells and tissues.\n",
      "* **Protein:** Protein is essential for building and repairing tissues, making enzymes, and producing hormones.\n",
      "* **Fat:** Fat provides energy, helps absorb vitamins, and helps to insulate the body.\n",
      "\n",
      "**Macronutrient Ratios:**\n",
      "\n",
      "* **Carbohydrates:** Our bodies need around 45-65% of our total calories from carbohydrates.\n",
      "* **Protein:** We need around 1.6-2.2 grams of protein per kilogram of body weight per day.\n",
      "* **Fat:** We need around 20-30% of our total calories from fat.\n",
      "\n",
      "**How Macronutrients Work Together:**\n",
      "\n",
      "Macronutrients work together to provide the body with the energy and building blocks it needs to function properly. For example:\n",
      "\n",
      "* Carbohydrates are broken down into glucose, which is used by cells for energy.\n",
      "* Protein is used to build and repair tissues, produce enzymes, and produce hormones.\n",
      "* Fat is used to store energy and help to insulate the body\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is that!\n",
    "\n",
    "We just officially generated text from an LLM running locally.\n",
    "\n",
    "So we've covered the R (retrieval) and G (generation) of RAG.\n",
    "\n",
    "How about we check out the last step?\n",
    "\n",
    "Augmentation.\n",
    "\n",
    "First, let's put together a list of queries we can try out with our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"Who is Harry Potter?\",\n",
    "    \"Describe Hermione Granger.\",\n",
    "    \"What are the characteristics of Ron Weasley?\",\n",
    "    \"What house does Harry get sorted into, and why?\",\n",
    "    \"Describe the character of Severus Snape.\",\n",
    "    \"How does Harry discover he is a wizard?\",\n",
    "    \"Describe Harry's first experience at Platform 9 3/4.\",\n",
    "    \"What is the significance of Harry's lightning-shaped scar?\",\n",
    "    \"What is the Mirror of Erised?\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"What are the characteristics of a Nimbus 2000 broomstick?\",\n",
    "    \"How does Harry become the Seeker for the Gryffindor Quidditch team?\",\n",
    "    \"What challenges do Harry, Ron, and Hermione face to reach the Philosopher's Stone?\",\n",
    "    \"How do they get past Fluffy?\",\n",
    "    \"Describe the encounter with the Devil's Snare.\",\n",
    "    \"What happens in the life-sized wizard chess game?\",\n",
    "    \"How does Harry confront Professor Quirrell and Voldemort?\",\n",
    "    \"How is friendship portrayed in the book?\",\n",
    "    \"What does the story suggest about bravery?\",\n",
    "    \"Who is Fluffy, and what role does he play?\",\n",
    "    \"How do Harry, Ron, and Hermione become friends?\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check if our `retrieve_relevant_resources()` function works with our list of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe Harry's first experience at Platform 9 3/4.\n",
      "[INFO] Time taken to get scores on 232 embeddings: 0.00009 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.1204, 0.1073, 0.0917, 0.0910, 0.0859], device='cuda:0'),\n",
       " tensor([158,  29,  35,  15,  30], device='cuda:0'))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful!\n",
    "\n",
    "Let's augment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting our prompt with context items\n",
    "\n",
    "What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n",
    "\n",
    "In essence, we start with a base prompt and update it with context text.\n",
    "\n",
    "Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n",
    "\n",
    "We'll apply the dialogue and chat template to our prompt before returning it as well.\n",
    "\n",
    "> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"In response to the following prompt, provide relevant information retrieved from the documents: \"{context}\"\n",
    "\n",
    "Prompt: {query}\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try our function out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does Harry become the Seeker for the Gryffindor Quidditch team?\n",
      "[INFO] Time taken to get scores on 232 embeddings: 0.00006 seconds.\n",
      "<bos><start_of_turn>user\n",
      "In response to the following prompt, provide relevant information retrieved from the documents: \"- he whispered. “Dad?”   They just looked at him, smiling. And slowly, Harry looked into the faces of the other people in the mirror, and saw other pairs of green eyes like his, other noses like his, even a little old man who looked as though he had Harry’s knobbly knees — Harry was looking at his family, for the first time in his life.   The Potters smiled and waved at Harry and he stared hungrily back at them, his hands pressed flat against the glass as though he was hoping to fall right through it and reach them. He had a powerful kind of ache inside him, half joy, half terrible sadness.   How long he stood there, he didn’t know. The reflections did not fade and he looked and looked until a distant noise brought him back to his senses. He couldn’t stay here, he had to find his way back to bed. He tore his eyes away from his mother’s face, whispered, “I’ll come back,” and hurried from the room. “You could have woken me up,” said Ron, crossly.   “You can come tonight, I’m going back, I want to show you the mirror.   “I’d like to see your mom and dad,” Ron said eagerly.\n",
      "- fading footsteps.   “He’s heading for the third floor,” Harry said, but Ron held up his hand.   “Can you smell something?”   Harry sniffed and a foul stench reached his nostrils, a mixture of old socks and the kind of public toilet no one seems to clean.   And then they heard it — a low grunting, and the shuffling footfalls of gigantic feet. Ron pointed — at the end of a passage to the left, something huge was moving toward them. They shrank into the shadows and watched as it emerged into a patch of moonlight.   It was a horrible sight. Twelve feet tall, its skin was a dull, granite gray, its great lumpy body like a boulder with its small bald head perched on top like a coconut. It had short legs thick as tree trunks with flat, horny feet. The smell coming from it was incredible. It was holding a huge wooden club, which dragged along the floor because its arms were so long.   The troll stopped next to a doorway and peered inside. It waggled its long ears, making up its tiny mind, then slouched slowly into the room.   “The keys in the lock,” Harry muttered. “We could lock it in.”   “Good idea,” said Ron nervously.   They edged toward the open door, mouths dry, praying the troll wasn’t about to come out of it. With one great leap, Harry managed to grab the key, slam the door, and lock it.   “Yes!”\n",
      "- Well, now — Mr. Potter. Let me see.”He pulled a long tape measure with silver markings out of his pocket. “Which is your wand arm?”   “Er — well, I’m right-handed,” said Harry.   “Hold out your arm. That’s it.”He measured Harry from shoulder to finger, then wrist to elbow, shoulder to floor, knee to armpit and round his head. As he measured, he said, “Every Ollivander wand has a core of a powerful magical substance, Mr. Potter. We use unicorn hairs, phoenix tail feathers, and the heartstrings of dragons. No two Ollivander wands are the same, just as no two unicorns, dragons, or phoenixes are quite the same. And of course, you will never get such good results with another wizard’s wand.”   Harry suddenly realized that the tape measure, which was measuring between his nostrils, was doing this on its own. Mr. Ollivander was flitting around the shelves, taking down boxes.   “That will do,” he said, and the tape measure crumpled into a heap on the floor. “Right then, Mr. Potter. Try this one. Beechwood and dragon heartstring. Nine inches. Nice and flexible.\n",
      "- Nevertheless, Harry, while you may only have delayed his return to power, it will merely take someone else who is prepared to fight what seems a losing battle next time — and if he is delayed again, and again, why, he may never return to power.”   Harry nodded, but stopped quickly, because it made his head hurt. Then he said, “Sir, there are some other things I’d like to know, if you can tell me… things I want to know the truth about.…”   “The truth.”Dumbledore sighed. “It is a beautiful and terrible thing, and should therefore be treated with great caution. However, I shall answer your questions unless I have a very good reason not to, in which case I beg you’ll forgive me. I shall not, of course, lie.”   “Well…Voldemort said that he only killed my mother because she tried to stop him from killing me. But why would he want to kill me in the first place?”   Dumbledore sighed very deeply this time.   “Alas, the first thing you ask me, I cannot tell you. Not today. Not now. You will know, one day…put it from your mind for now, Harry. When you are older…I know you hate to hear this…when you are ready, you will know.”   And Harry knew it would be no good to argue.\n",
      "- CHAPTER SEVEN  THE SORTING HAT  T he door swung open at once. A tall, black-haired witch in emerald-green robes stood there. She had a very stern face and Harry’s first thought was that this was not someone to cross.   “The firs’ years, Professor McGonagall,” said Hagrid.   “Thank you, Hagrid. I will take them from here.”   She pulled the door wide. The entrance hall was so big you could have fit the whole of the Dursleys’ house in it. The stone walls were lit with flaming torches like the ones at Gringotts, the ceiling was too high to make out, and a magnificent marble staircase facing them led to the upper floors.   They followed Professor McGonagall across the flagged stone floor. Harry could hear the drone of hundreds of voices from a doorway to the right — the rest of the school must already be here — but Professor McGonagall showed the first years into a small, empty chamber off the hall. They crowded in, standing rather closer together than they would usually have done, peering about nervously.   “Welcome to Hogwarts,” said Professor McGonagall. “The start-of-term banquet will begin shortly, but before you take your seats in the Great Hall, you will be sorted into your houses. The Sorting is a very important ceremony because, while you are here, your house will be something like your family within Hogwarts. You will have classes with the rest of your house, sleep in your house dormitory, and spend free time in your house common room.   “The four houses are called Gryffindor, Hufflepuff, Ravenclaw, and Slytherin. Each house has its own noble history and each has produced outstanding witches and wizards. While you are at Hogwarts, your triumphs will earn your house points, while any rulebreaking will lose house points. At the end of the year, the house with the most points is awarded the house cup, a great honor.\"\n",
      "\n",
      "Prompt: How does Harry become the Seeker for the Gryffindor Quidditch team?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a good looking prompt!\n",
    "\n",
    "We can tokenize this and pass it straight to our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does Harry become the Seeker for the Gryffindor Quidditch team?\n",
      "RAG answer:\n",
      "<bos>The context does not mention how Harry becomes the Seeker for the Gryffindor Quidditch team, so I cannot answer this question from the provided context.<eos>\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesssssss!!!\n",
    "\n",
    "Our RAG pipeline is complete!\n",
    "\n",
    "We just Retrieved, Augmented and Generated!\n",
    "\n",
    "And all on our own local GPU!\n",
    "\n",
    "How about we functionize the generation step to make it easier to use?\n",
    "\n",
    "We can put a little formatting on the text being returned to make it look nice too.\n",
    "\n",
    "And we'll make an option to return the context items if needed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a good looking function!\n",
    "\n",
    "The workflow could probably be a little refined but this should work!\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe the encounter with the Devil's Snare.\n",
      "[INFO] Time taken to get scores on 232 embeddings: 0.00006 seconds.\n",
      "Answer:\n",
      "\n",
      "The context does not provide any information about the Devil's Snare, so I\n",
      "cannot answer this question from the provided context.\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 5,\n",
       "  'sentence_chunk': 'something peculiar — a cat reading a map. For a second, Mr. Dursley didn’t realize what he had seen — then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn’t a map in sight. What could he have been thinking of?It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive — no, looking at the sign; cats couldn’t read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he thought of nothing except a large order of drills he was hoping to get that day.   But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn’t help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr. Dursley was enraged to see that a couple of them weren’t young at all; why, that man had to be older than he was, and wearing an emerald-green cloak!The nerve of him!',\n",
       "  'chunk_char_count': 1468,\n",
       "  'chunk_word_count': 282,\n",
       "  'chunk_token_count': 367.0,\n",
       "  'embedding': array([ 1.82670932e-02, -8.97018537e-02, -3.11918731e-04,  7.27453595e-03,\n",
       "          2.88019739e-02,  3.98980565e-02, -1.51766408e-02,  2.64417976e-02,\n",
       "         -1.99263375e-02, -4.57683392e-02, -4.59910892e-02, -2.27495730e-02,\n",
       "          1.22149270e-02, -1.97524223e-02, -3.40333059e-02,  6.14610128e-03,\n",
       "         -1.60432104e-02,  2.45331433e-02, -5.19772246e-02, -5.42646982e-02,\n",
       "          2.45133154e-02, -2.05490384e-02, -1.96577162e-02, -4.05974500e-02,\n",
       "         -1.75419543e-02, -7.28494162e-03,  3.97346728e-02,  7.19872303e-03,\n",
       "         -8.96507725e-02,  2.43061781e-02,  9.12155490e-03, -5.05734682e-02,\n",
       "          8.45343526e-03,  4.30626236e-02,  1.00075430e-03,  1.15247387e-02,\n",
       "          9.34877172e-02, -3.77198458e-02,  2.38428079e-02,  1.82260610e-02,\n",
       "         -4.13681045e-02, -3.12107913e-02, -4.15534675e-02, -1.09106246e-02,\n",
       "          4.60493714e-02,  2.39802692e-02, -6.46050721e-02,  2.11035535e-02,\n",
       "          6.75228536e-02, -1.41873173e-02, -3.82758235e-03,  1.04686087e-02,\n",
       "         -3.76225039e-02,  4.85663339e-02,  1.21383900e-02,  2.67987885e-02,\n",
       "          1.29936179e-02,  7.43828416e-02, -6.57412922e-03, -3.17631220e-03,\n",
       "          3.79012562e-02, -8.27284879e-04, -4.54803184e-03,  7.55178481e-02,\n",
       "          4.34973352e-02,  1.07066957e-02, -1.96153484e-03,  1.72465702e-03,\n",
       "         -1.36378519e-02, -2.24253088e-02,  5.84690366e-03, -4.91640083e-02,\n",
       "         -5.33871446e-03, -1.93795245e-02, -4.09811251e-02,  4.66484502e-02,\n",
       "          3.60395847e-04, -1.09258788e-02, -6.25787526e-02, -1.59250805e-03,\n",
       "         -2.09785588e-02, -3.06384475e-03,  3.88582125e-02,  4.74323239e-03,\n",
       "          8.21596757e-03,  2.51936447e-03,  1.36778492e-03, -3.13780196e-02,\n",
       "         -1.07489643e-03,  2.98880320e-02,  2.53101178e-02,  5.40954731e-02,\n",
       "         -1.61346775e-02,  3.96580175e-02, -1.39531158e-02,  3.79708386e-03,\n",
       "         -1.34918643e-02,  6.23212755e-02, -5.54728042e-03, -3.16950977e-02,\n",
       "          3.26852277e-02, -1.74675845e-02, -8.03745240e-02,  3.77448089e-03,\n",
       "          4.15062420e-02, -2.31544524e-02, -1.39196087e-02, -9.87056084e-03,\n",
       "         -3.58162932e-02, -2.69757304e-02, -5.17293774e-02,  1.13599608e-02,\n",
       "         -5.96796647e-02,  1.63745973e-02, -1.86457019e-02,  2.24643797e-02,\n",
       "          6.32773899e-03, -3.56385335e-02,  2.55720466e-02,  1.67658906e-02,\n",
       "          3.80620896e-03, -4.29558903e-02,  1.34287449e-02,  3.83548439e-02,\n",
       "          9.10166278e-03,  5.40988669e-02,  3.46780792e-02, -1.69977974e-02,\n",
       "         -1.77102331e-02,  2.08503785e-04, -8.32721740e-02,  6.15113741e-03,\n",
       "          2.53129788e-02, -6.84094951e-02, -1.73719712e-02,  1.99639555e-02,\n",
       "         -8.57560616e-03,  3.13788019e-02,  7.09203538e-03,  4.48426567e-02,\n",
       "          1.76734403e-02,  4.45518643e-02,  4.26357612e-02, -1.04661379e-02,\n",
       "         -2.38919053e-02,  2.97377687e-02,  1.64104868e-02,  5.43662533e-02,\n",
       "         -1.00066625e-02,  5.02554812e-02,  6.00478575e-02, -7.95398839e-03,\n",
       "         -1.64140537e-02, -4.95599732e-02,  1.69233978e-02,  3.58086489e-02,\n",
       "         -2.39476208e-02,  3.13904770e-02,  3.72992903e-02, -7.63800181e-03,\n",
       "          8.94087934e-05, -2.31315428e-03,  6.14624470e-03, -2.71495618e-02,\n",
       "          4.72282292e-03,  8.40467401e-03,  1.73765998e-02,  2.64581759e-02,\n",
       "          1.80009510e-02, -5.33015318e-02,  5.63808950e-03,  2.85765938e-02,\n",
       "         -4.38386714e-03,  5.09694871e-03, -3.13373003e-03,  4.19871733e-02,\n",
       "          1.16941957e-02, -6.24042600e-02,  3.97965200e-02,  1.94481425e-02,\n",
       "          3.01156640e-02,  2.22913437e-02,  5.95159642e-03,  4.77853185e-03,\n",
       "          8.31748545e-03,  3.37554552e-02, -1.03620114e-02, -3.14599164e-02,\n",
       "         -9.06302929e-02, -3.96816013e-03, -5.63673861e-02,  2.08803080e-02,\n",
       "         -7.28653837e-03, -7.72826979e-03,  3.44667546e-02,  2.67256591e-02,\n",
       "          3.90997641e-02,  2.17643566e-02,  1.07114958e-02, -1.05371371e-01,\n",
       "          2.16426905e-02, -4.19603288e-02,  4.13331129e-02, -6.60170242e-02,\n",
       "         -1.75505560e-02,  2.24244315e-03, -7.32697174e-02, -3.50938598e-03,\n",
       "          1.00297276e-02,  4.35525598e-03, -5.46939299e-03,  1.75385997e-02,\n",
       "          2.33582221e-02,  4.80419807e-02,  6.82045221e-02, -4.70205657e-02,\n",
       "          1.63882915e-02,  7.49129383e-03,  1.27288699e-02,  2.42137117e-03,\n",
       "         -1.12393349e-02, -8.38103984e-03,  1.09055673e-03,  7.85212126e-03,\n",
       "         -4.08080369e-02, -8.89706537e-02,  1.59956925e-02, -1.09696547e-02,\n",
       "         -6.46572113e-02,  3.87123190e-02, -4.48636562e-02,  1.30178621e-02,\n",
       "         -3.73083800e-02, -7.11915046e-02,  3.65379341e-02, -3.83448065e-03,\n",
       "          8.05169344e-03, -2.54890881e-03,  3.70736830e-02,  4.45427895e-02,\n",
       "         -9.71655734e-03,  3.54236574e-03,  1.59031544e-02,  3.66220693e-03,\n",
       "          4.83540483e-02, -3.61961499e-02,  4.44989726e-02, -1.52117405e-02,\n",
       "          3.12108584e-02, -4.92071621e-02,  3.32911164e-02, -4.31250520e-02,\n",
       "         -6.16563484e-03, -1.05781052e-02,  1.29451612e-02, -1.54964793e-02,\n",
       "          2.71503702e-02,  1.80466305e-02,  6.53590485e-02,  6.13667555e-02,\n",
       "          8.59438255e-03,  3.27019505e-02, -6.27123341e-02,  8.17620661e-03,\n",
       "          4.77474090e-03, -3.39596719e-03, -2.02024896e-02, -7.44449068e-03,\n",
       "         -4.66278195e-02,  8.67306441e-03,  1.03719672e-02, -1.69686172e-02,\n",
       "          1.58032067e-02,  4.20861468e-02, -7.65845226e-03,  4.16307971e-02,\n",
       "         -3.62924933e-02,  3.90469655e-03, -1.58070214e-02, -2.53741983e-02,\n",
       "         -3.09790298e-02,  6.10099034e-03, -7.04587996e-03, -1.08085666e-02,\n",
       "          7.82434084e-03, -3.93025950e-02, -3.98655981e-03, -5.22067363e-04,\n",
       "         -3.54056712e-03, -2.19203867e-02, -4.48292457e-02, -2.32264474e-02,\n",
       "          8.80661532e-02,  1.41851725e-02,  2.90436149e-02,  4.76725865e-03,\n",
       "          1.50116170e-02, -7.90587291e-02, -1.54359648e-02, -2.19444726e-02,\n",
       "          1.56169431e-02, -4.10646647e-02, -4.78291400e-02,  2.08341479e-02,\n",
       "         -2.76132803e-02, -2.84312526e-03,  1.27153238e-02, -2.35525239e-02,\n",
       "          2.49095876e-02,  4.39232402e-02, -4.35864888e-02, -2.06508096e-02,\n",
       "         -2.33231410e-02, -1.78340375e-02, -5.88699430e-02, -1.64239537e-02,\n",
       "          9.47797031e-04,  8.21682066e-03,  7.14318454e-02,  6.16596714e-02,\n",
       "         -3.69143076e-02, -2.59354990e-02, -8.87840148e-03,  4.66325600e-03,\n",
       "          4.06379923e-02, -5.25226742e-02,  7.12738885e-03,  7.77081726e-03,\n",
       "          1.55612342e-02,  3.21088061e-02, -4.65435386e-02, -2.60082614e-02,\n",
       "         -1.17606521e-02, -9.84699838e-03,  3.69894728e-02,  1.99243091e-02,\n",
       "          2.28570681e-02,  4.15110355e-03,  7.64910597e-03,  2.63647437e-02,\n",
       "         -7.20816553e-02, -4.76420484e-03, -4.98112068e-02,  2.60533076e-02,\n",
       "         -3.09884306e-02, -3.11427899e-02, -3.82599682e-02,  1.99919008e-02,\n",
       "          7.36525469e-03, -2.66304482e-02,  1.58405583e-02,  3.86576937e-03,\n",
       "         -2.21406147e-02,  2.01890059e-02, -3.25826444e-02, -6.19618455e-03,\n",
       "         -6.50354102e-02, -3.79528888e-02, -4.92337113e-03, -6.03702180e-02,\n",
       "          1.77594752e-03,  3.10941264e-02,  4.31689583e-02, -1.67415477e-02,\n",
       "          3.97775993e-02, -1.11502677e-01, -2.29776390e-02,  1.46943023e-02,\n",
       "          1.25427381e-04, -1.13072013e-02, -2.69075371e-02,  5.57884239e-02,\n",
       "          3.81360054e-02, -4.94718701e-02,  2.58428883e-02,  1.58192143e-02,\n",
       "         -4.62199152e-02, -4.80294265e-02,  6.48992434e-02, -2.23664083e-02,\n",
       "         -2.45724972e-02, -2.53124814e-02, -2.32199803e-02, -1.43671911e-02,\n",
       "          7.38292874e-04, -1.52912075e-02,  6.27656206e-02,  5.64234629e-02,\n",
       "          1.05812423e-01,  1.73202939e-02,  2.37180444e-04,  2.12139897e-02,\n",
       "         -7.57160364e-03,  3.24644782e-02,  4.42402624e-02,  3.69095732e-03,\n",
       "         -2.10401742e-03, -1.58513393e-02,  1.56489313e-02,  6.39800653e-02,\n",
       "          1.18337674e-02,  2.75636539e-02,  6.70754910e-02, -4.14182656e-02,\n",
       "         -5.58729172e-02,  1.10613753e-03,  7.78123213e-04,  1.58061199e-02,\n",
       "         -4.37005013e-02,  1.31101450e-02,  5.95939271e-02,  9.51366946e-02,\n",
       "          3.64806340e-03, -3.87658477e-02,  1.17388899e-02, -3.10215354e-02,\n",
       "          4.77972161e-03,  7.13410042e-03,  4.06121090e-02,  1.79073028e-02,\n",
       "          5.23489676e-02,  5.67803942e-02, -6.71070814e-02, -1.57639645e-02,\n",
       "          3.55676748e-02, -7.77892675e-03, -1.90608632e-02, -4.84238714e-02,\n",
       "         -5.15083149e-02, -5.70750646e-02, -1.43104866e-02, -1.98014509e-02,\n",
       "         -2.46172715e-02, -3.62176187e-02, -2.60106958e-02,  1.08804759e-02,\n",
       "          1.30315609e-02, -2.87686195e-02, -2.64867255e-03,  4.61634584e-02,\n",
       "          5.07550547e-03,  1.25969918e-02,  4.96505834e-02, -7.42337778e-02,\n",
       "         -2.83619072e-02, -1.75267756e-02, -6.36097509e-03,  3.16394083e-02,\n",
       "          5.08459052e-03,  2.98296213e-02, -9.06445310e-02, -8.02776590e-03,\n",
       "         -6.52463511e-02, -1.60382256e-01, -2.04441845e-02,  2.61824648e-03,\n",
       "          3.49269691e-03, -6.53363094e-02, -1.18108382e-02, -1.35262765e-03,\n",
       "         -2.14973986e-02,  1.09135304e-02, -2.41276827e-02,  4.09787521e-02,\n",
       "          1.76899415e-02, -5.41592948e-03, -6.13140911e-02, -6.36634976e-02,\n",
       "          1.67370401e-02,  3.46190222e-02, -3.60014178e-02,  4.81800586e-02,\n",
       "         -1.49260077e-03, -3.57230566e-02, -2.72367243e-02,  4.80226539e-02,\n",
       "         -4.48387256e-03,  5.88856414e-02, -4.52730432e-02,  9.08729155e-03,\n",
       "         -6.48937188e-03, -7.70801352e-03, -1.44194653e-02,  1.16462084e-02,\n",
       "         -1.67053993e-04, -1.00506153e-02, -4.95218374e-02,  7.97371194e-02,\n",
       "         -3.66483927e-02,  3.94332372e-02,  2.55936161e-02,  3.41431238e-02,\n",
       "          2.91512534e-02,  3.45190056e-02,  3.90895842e-33, -4.37476300e-03,\n",
       "         -8.18190444e-03,  1.51818171e-02,  2.50192638e-02,  1.79530289e-02,\n",
       "         -2.61649070e-03, -1.72646753e-02, -9.76095628e-03,  1.13102486e-02,\n",
       "          1.14454348e-02, -6.83819354e-02,  1.18133519e-02,  5.54015161e-03,\n",
       "          1.44712720e-03,  2.83266813e-03, -3.13088112e-02,  1.21778302e-01,\n",
       "         -3.31647210e-02,  1.11971295e-03, -3.35922721e-03, -1.50797814e-02,\n",
       "          3.06623857e-02, -8.39544088e-02,  3.63172069e-02,  3.08121052e-02,\n",
       "         -2.19374597e-02,  2.91939965e-03, -1.07165787e-03, -3.16528454e-02,\n",
       "          7.54187023e-03,  4.26543877e-02, -8.29764176e-03,  2.37609409e-02,\n",
       "         -1.56252980e-02, -7.79529959e-02, -4.49837260e-02,  1.21446876e-02,\n",
       "          1.06468489e-02, -2.57551428e-02, -5.47469966e-02,  1.36202364e-03,\n",
       "          2.63762586e-02,  1.09686777e-02,  5.26241176e-02, -8.75243321e-02,\n",
       "         -1.81532893e-02,  1.56017346e-02, -4.32217456e-02, -3.63389440e-02,\n",
       "         -4.34787385e-02,  5.25123589e-02,  5.42455539e-02, -1.83311459e-02,\n",
       "         -3.76410969e-02,  3.62388194e-02,  9.94620994e-02,  1.58214882e-01,\n",
       "         -6.29014522e-02,  4.75460775e-02,  1.68028362e-02,  8.22657254e-03,\n",
       "          2.02932879e-02, -5.69640286e-03, -8.87755677e-03, -4.32625860e-02,\n",
       "          2.37098429e-02, -4.76370305e-02, -1.97163131e-02, -3.71073782e-02,\n",
       "         -1.46246152e-02,  2.27171201e-02, -6.84840307e-02,  6.06957711e-02,\n",
       "         -4.05324332e-04,  4.35509793e-02,  1.66323856e-02, -6.99505815e-03,\n",
       "         -3.78545150e-02,  1.56801026e-02, -1.51456855e-02, -2.63835825e-02,\n",
       "         -1.75688528e-02,  2.55661905e-02, -8.45559407e-03, -1.14207622e-03,\n",
       "          1.84852369e-02,  4.37980816e-02,  8.01480561e-03, -2.98118684e-02,\n",
       "         -2.54997369e-02,  1.85394380e-02, -8.03511590e-02, -3.47395893e-03,\n",
       "          2.28002528e-03, -1.76041126e-02,  1.44176055e-02,  1.17818015e-02,\n",
       "          3.22711095e-02, -2.87342742e-02, -4.06272411e-02, -5.28019965e-02,\n",
       "          1.77668799e-02,  6.00630343e-02,  2.98580118e-02, -1.09660551e-02,\n",
       "          2.38357633e-02, -4.71532904e-02, -2.87821088e-02, -2.15552971e-02,\n",
       "         -5.52864373e-02,  2.26042122e-02,  9.86847654e-02,  4.34241369e-02,\n",
       "         -4.58990932e-02, -3.73962075e-02,  3.38429101e-02, -2.56447718e-02,\n",
       "          2.07160376e-02,  4.98271473e-02,  5.03021143e-02, -7.60995457e-03,\n",
       "         -1.79037936e-02,  4.00004117e-03, -2.72248145e-02,  4.75007109e-02,\n",
       "          4.09913473e-02,  2.36315280e-03, -6.93263412e-02,  4.81008505e-03,\n",
       "         -5.56805842e-02, -3.81917916e-02,  3.61840352e-02, -7.32605681e-02,\n",
       "         -2.17551109e-03, -3.46025750e-02, -2.50822250e-02,  4.61138226e-02,\n",
       "         -2.96218321e-02,  3.49583151e-03, -1.06576765e-02,  2.34471224e-02,\n",
       "         -6.93379017e-03,  4.34530992e-03,  4.16717269e-02, -6.55319681e-03,\n",
       "          2.84491796e-02, -1.44208698e-02, -2.71050967e-02, -1.40259648e-02,\n",
       "         -7.42016360e-04, -3.92488466e-04, -2.07364336e-02,  1.19386250e-02,\n",
       "          4.36658971e-02, -1.76758654e-02, -2.30655074e-02, -2.23669014e-03,\n",
       "          2.11950429e-02, -7.55822510e-02,  3.14745158e-02, -9.82183218e-03,\n",
       "          1.03371628e-02,  2.81910114e-02,  1.43749015e-02, -1.02198832e-02,\n",
       "          1.17846057e-02,  4.75008078e-02, -3.72202434e-02,  7.09816441e-02,\n",
       "          2.75606886e-02, -6.57829419e-02, -2.79429518e-02, -1.98750403e-02,\n",
       "          3.05124354e-02,  6.14068769e-02,  5.21242097e-02,  5.87090524e-03,\n",
       "          1.67519711e-02, -4.38651219e-02, -5.25365248e-02, -2.30559204e-02,\n",
       "          2.88287532e-02, -2.36984566e-02, -3.54271792e-02, -3.59895341e-02,\n",
       "          2.09410600e-02, -3.12243979e-02,  3.71812284e-02, -3.31891812e-02,\n",
       "         -9.08789225e-03,  3.07108108e-02,  1.29559478e-02, -3.71010392e-03,\n",
       "         -8.29563290e-03, -2.49622278e-02,  4.02864674e-03, -3.10852528e-02,\n",
       "         -1.37591560e-03,  3.66944671e-02,  1.12339873e-02,  3.23578925e-03,\n",
       "          2.51958729e-03, -1.36434771e-02, -1.32009964e-02,  1.23356609e-02,\n",
       "          8.96115787e-03, -3.48258540e-02, -9.37919867e-06,  5.71084721e-03,\n",
       "          1.31859938e-02, -1.31351044e-02,  3.85274626e-02, -4.79253493e-02,\n",
       "          1.32503435e-02,  3.86077445e-03, -3.94692123e-02,  2.67303735e-02,\n",
       "          4.43338268e-02, -2.48076543e-02, -3.97938341e-02, -2.50835903e-02,\n",
       "         -2.75908560e-02,  6.90730661e-03, -5.82853816e-02,  7.17485929e-03,\n",
       "          8.84984285e-02, -3.10675502e-02,  1.62760988e-02,  1.11524630e-02,\n",
       "          4.12515514e-02, -4.69314605e-02,  2.30525341e-02, -8.18947144e-03,\n",
       "          1.81900766e-02, -1.94200501e-02,  2.49895658e-02,  3.33620422e-02,\n",
       "          2.20839903e-02,  5.39369322e-02,  5.27728945e-02,  9.66630224e-03,\n",
       "          5.42127900e-02,  6.81594685e-02,  1.21280281e-02,  3.94254588e-02,\n",
       "          7.09358156e-02, -5.98518848e-02,  3.09884883e-02, -8.67161825e-02,\n",
       "          2.55589914e-02,  7.35293403e-02,  4.47230559e-04, -1.47477640e-02,\n",
       "          3.51855136e-03, -7.35936165e-02, -2.96665560e-02,  2.47979667e-02,\n",
       "          1.13846380e-02,  1.94559377e-02,  5.64109394e-03, -4.30565886e-02,\n",
       "          2.71108113e-02,  1.29943034e-02, -1.77586894e-03,  3.71468328e-02,\n",
       "         -5.70361614e-02, -4.00959179e-02, -3.98038849e-02,  5.25778085e-02,\n",
       "         -1.93615835e-02, -7.08194301e-02, -3.65994498e-02, -2.90551260e-02]),\n",
       "  'score': tensor(0.0869)},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'But then it struck Mr. Dursley that this was probably some silly stunt —these people were obviously collecting for something…yes, that would be it. The traffic moved on and a few minutes later, Mr. Dursley arrived in the Grunnings parking lot, his mind back on drills.   Mr. Dursley always sat with his back to the window in his office on the ninth floor. If he hadn’t, he might have found it harder to concentrate on drills that morning. He didn’t see the owls swooping past in broad daylight, though people down in the street did; they pointed and gazed open-mouthed as owl after owl sped overhead. Most of them had never seen an owl even at nighttime. Mr. Dursley, however, had a perfectly normal, owl-free morning. He yelled at five different people. He made several important telephone calls and shouted a bit more. He was in a very good mood until lunchtime, when he thought he’d stretch his legs and walk across the road to buy himself a bun from the bakery.   He’d for gotten all about the people in cloaks until he passed a group of them next to the baker’s. He eyed them angrily as he passed. He didn’t know why, but they made him uneasy. This bunch were whispering excitedly, too, and he couldn’t see a single collecting tin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.   “The Potters, that’s right, that’s what I heard —”',\n",
       "  'chunk_char_count': 1413,\n",
       "  'chunk_word_count': 263,\n",
       "  'chunk_token_count': 353.25,\n",
       "  'embedding': array([ 2.07877830e-02, -8.94869491e-02,  1.50764538e-02, -5.03704660e-02,\n",
       "          4.68071885e-02,  4.84917536e-02, -1.21522928e-02,  2.41050571e-02,\n",
       "         -9.71514918e-03, -1.16688134e-02, -6.55490160e-03,  7.97902350e-04,\n",
       "          1.48833236e-02, -2.35283887e-03, -3.78336087e-02, -4.33558947e-04,\n",
       "         -4.71811043e-03, -3.03982385e-02, -8.12639073e-02, -3.37042920e-02,\n",
       "          1.93469711e-02, -4.09430563e-02,  8.35142378e-03, -1.46274678e-02,\n",
       "          4.88643721e-03,  1.49938939e-02,  7.96900615e-02,  3.51859257e-02,\n",
       "         -5.35380654e-02,  4.82049026e-02, -9.63992253e-03, -8.89039226e-03,\n",
       "          2.48632245e-02,  5.46242259e-02, -1.19590927e-02, -1.65826045e-02,\n",
       "          8.54391903e-02, -2.90685100e-03, -6.06940500e-03, -1.51874740e-02,\n",
       "         -6.60105795e-02, -2.90252059e-03, -3.76569517e-02,  1.39713790e-02,\n",
       "          4.33218665e-02,  2.97477171e-02, -5.57214506e-02,  4.28861976e-02,\n",
       "          2.19790228e-02,  7.97657296e-03,  2.12287940e-02,  8.46466143e-03,\n",
       "         -3.10459975e-02,  4.63536568e-02,  1.88313965e-02,  4.51106671e-03,\n",
       "          1.59782711e-02,  7.02652261e-02,  7.93184678e-04, -3.72562520e-02,\n",
       "          6.62696585e-02, -2.41226200e-02, -9.92244203e-03,  1.70143526e-02,\n",
       "          3.21612880e-02, -2.27997662e-03, -4.77447640e-03,  4.35190741e-03,\n",
       "         -1.39240166e-02, -3.32374759e-02, -2.39518937e-03, -6.40241951e-02,\n",
       "          2.36822921e-03,  1.97534519e-03, -6.06449321e-02,  7.79616321e-03,\n",
       "         -1.52454041e-02, -1.80079509e-02, -4.01570164e-02,  1.84943713e-02,\n",
       "         -1.42425643e-02,  4.84285541e-02, -3.44586303e-03,  2.70167571e-02,\n",
       "          3.44281718e-02, -8.93647783e-03,  3.74503230e-04,  1.89713575e-02,\n",
       "         -4.45956597e-03,  4.83731460e-03,  2.14030854e-02,  8.64564031e-02,\n",
       "         -2.87459679e-02,  7.24834651e-02, -1.96118448e-02, -1.67886335e-02,\n",
       "         -2.79394723e-02,  1.05547912e-01, -1.35125685e-02,  8.69798381e-03,\n",
       "          5.41504808e-02,  9.19265114e-03, -2.67637875e-02,  1.43963275e-02,\n",
       "          3.73285562e-02, -7.31220096e-03,  2.51944121e-02, -3.60837728e-02,\n",
       "         -3.97951640e-02, -2.50706393e-02, -3.61286215e-02,  9.27635748e-03,\n",
       "         -3.65028195e-02,  1.44834509e-02,  1.53676849e-02,  9.24189948e-03,\n",
       "          3.56111501e-04, -4.85288203e-02,  2.03304235e-02,  5.45785576e-02,\n",
       "          1.81064755e-02, -4.17379811e-02,  3.94899100e-02,  2.97823548e-02,\n",
       "          4.42658998e-02,  5.17125167e-02,  6.42823800e-02, -4.26806398e-02,\n",
       "         -4.51730937e-02,  9.85136162e-03, -5.52003533e-02,  2.24932674e-02,\n",
       "          3.05552911e-02, -6.62254617e-02, -3.34112719e-02,  6.51618652e-03,\n",
       "         -2.30825972e-02,  4.19476070e-02,  3.20282020e-02,  4.19601351e-02,\n",
       "          8.12183321e-03,  6.64658844e-02, -1.00275343e-02, -1.00280000e-02,\n",
       "         -2.83814054e-02,  1.14439186e-02, -2.13229582e-02,  4.13821079e-02,\n",
       "         -3.65206413e-02,  5.39202243e-02,  4.40330319e-02,  1.04004797e-02,\n",
       "         -2.30516419e-02, -1.29257515e-02,  1.27262631e-02,  3.32976840e-02,\n",
       "         -8.27548187e-03,  3.23738568e-02,  2.56364606e-02,  8.82205460e-03,\n",
       "          2.44778432e-02,  1.77862942e-02, -1.70418173e-02, -2.16627643e-02,\n",
       "         -4.41637933e-02,  1.03645409e-02, -7.10150041e-03,  4.17866856e-02,\n",
       "          7.45889619e-02, -1.26388595e-02,  7.25731486e-03,  3.28721688e-03,\n",
       "         -1.46212624e-02, -1.80290604e-04,  1.69429649e-02,  5.83104417e-02,\n",
       "         -1.35254208e-02, -6.00828342e-02,  1.03242220e-02,  5.09632193e-02,\n",
       "          1.37731675e-02,  3.50318179e-02,  9.09231789e-03,  1.01281339e-02,\n",
       "          2.25915876e-03, -2.08918890e-03, -1.28682638e-02, -2.02302542e-02,\n",
       "         -8.14157799e-02,  1.24839582e-02, -2.77609508e-02,  3.34051922e-02,\n",
       "          3.28418612e-02, -1.25146209e-04,  3.74425426e-02,  6.07852340e-02,\n",
       "          3.44413593e-02,  1.81772560e-02,  6.29865006e-02, -8.35315064e-02,\n",
       "         -2.75626732e-03, -5.26855513e-02, -2.56194789e-02, -9.59398970e-02,\n",
       "         -1.07954955e-02, -1.36384480e-02, -8.67186580e-03,  2.36074207e-03,\n",
       "          3.31012212e-04,  4.30915207e-02, -4.03611511e-02, -2.43075807e-02,\n",
       "         -3.05592325e-02,  3.48997936e-02,  4.63523306e-02, -1.21468585e-02,\n",
       "          4.42407168e-02,  9.96318832e-03,  1.23960311e-02, -2.89706560e-03,\n",
       "         -9.26857907e-03, -2.67965682e-02, -1.26278149e-02,  2.74919700e-02,\n",
       "         -7.59279495e-03, -7.89399669e-02, -9.70986951e-03, -5.84547501e-03,\n",
       "         -1.50849130e-02,  1.61307044e-02, -4.48119268e-02,  2.50893701e-02,\n",
       "         -6.71007559e-02, -7.08282292e-02,  1.51424771e-02, -7.50028864e-02,\n",
       "          3.76207232e-02, -3.19098085e-02,  3.52541320e-02,  3.02827312e-03,\n",
       "          1.35007100e-02,  6.93473150e-04,  5.11646681e-02,  1.15304412e-02,\n",
       "          8.47636629e-03, -1.94674935e-02,  2.00374462e-02, -6.12593303e-03,\n",
       "          2.51024552e-02, -2.02726312e-02,  3.65195982e-02, -4.52832095e-02,\n",
       "         -1.02657666e-02, -3.57044227e-02,  1.55199915e-02,  1.40044419e-03,\n",
       "          1.92319490e-02,  8.60908348e-03,  7.54105672e-02,  7.55151659e-02,\n",
       "          4.26153973e-04,  6.53857365e-03, -2.61466783e-02,  2.53940485e-02,\n",
       "          3.97282979e-03, -7.13684633e-02, -3.66669931e-02,  8.84204265e-03,\n",
       "         -6.38890937e-02, -6.42775977e-03, -5.41908899e-03, -1.78549755e-02,\n",
       "         -2.66472623e-03, -2.15575863e-02,  8.80835578e-03,  2.13171709e-02,\n",
       "         -5.21393046e-02,  9.65932477e-03,  2.01618206e-02, -3.78799736e-02,\n",
       "         -1.87419001e-02, -3.86646157e-03, -6.07313775e-03,  2.23750854e-03,\n",
       "          4.19420749e-03,  1.48644103e-02, -5.32584731e-03,  1.99103337e-02,\n",
       "          3.22204158e-02, -8.40571534e-04, -4.17991057e-02, -2.00471580e-02,\n",
       "          4.01955582e-02,  1.54984985e-02,  4.65795994e-02,  1.86203998e-02,\n",
       "          1.46804228e-02, -1.06932595e-01, -1.38246790e-02, -6.81045204e-02,\n",
       "         -1.57892630e-02, -5.90415299e-02, -9.49883536e-02,  2.28731483e-02,\n",
       "         -3.44561934e-02,  3.45765054e-03,  3.28743346e-02,  1.93851143e-02,\n",
       "          2.21210462e-03,  8.75677466e-02, -2.56324112e-02, -1.64011847e-02,\n",
       "         -3.41447145e-02,  7.42973480e-03, -5.67929074e-02, -1.90077834e-02,\n",
       "         -2.36534514e-02, -6.83674915e-03,  6.78667203e-02,  3.79229523e-02,\n",
       "         -2.04217266e-02, -2.99177915e-02, -3.84934023e-02, -8.08014069e-03,\n",
       "          2.84665581e-02, -5.64008653e-02, -6.79981476e-03, -1.51444448e-03,\n",
       "          1.48551464e-02,  2.68854126e-02, -5.81183061e-02, -1.67037342e-02,\n",
       "         -1.52223213e-02, -2.47375630e-02,  3.49165946e-02,  2.32588686e-03,\n",
       "         -1.08150896e-02, -1.20609663e-02, -2.38155969e-03,  1.40120657e-02,\n",
       "         -5.16615547e-02,  5.79812936e-03, -1.70642491e-02,  4.08648280e-03,\n",
       "          1.42677233e-03, -6.29078178e-03, -4.22300510e-02,  3.21078636e-02,\n",
       "         -5.65173430e-03,  3.60382535e-02,  2.84269992e-02, -7.46504823e-03,\n",
       "          6.86028181e-03,  9.31253005e-03, -2.33320706e-02,  3.90130207e-02,\n",
       "         -4.97311763e-02, -3.26808877e-02, -1.34628508e-02, -4.78538759e-02,\n",
       "         -5.36909066e-02,  5.73314615e-02,  2.09206156e-02,  3.02796339e-04,\n",
       "          1.57091618e-02, -8.80071819e-02, -2.74490528e-02,  5.21702915e-02,\n",
       "          3.80797200e-02, -1.78000871e-02, -2.51311045e-02,  3.68603617e-02,\n",
       "          5.61921708e-02, -1.87947191e-02,  3.31353396e-02,  5.54727577e-03,\n",
       "         -9.74850580e-02, -3.75213288e-02,  3.40318978e-02,  1.56432316e-02,\n",
       "          2.52456032e-03, -2.46586893e-02, -3.70613858e-02, -1.42047787e-02,\n",
       "          3.39242034e-02,  1.07595893e-02,  3.96825606e-03,  2.87993811e-02,\n",
       "          9.17460099e-02,  2.28763130e-02,  2.24853605e-02, -3.68385687e-02,\n",
       "         -3.80472988e-02,  7.02596596e-03,  4.15228270e-02, -1.23586142e-02,\n",
       "          3.35025275e-03, -2.45486014e-02, -2.74707726e-03,  4.27029841e-02,\n",
       "          1.05141220e-03,  2.37760749e-02,  5.75146750e-02, -2.29748823e-02,\n",
       "         -6.82938918e-02,  1.56795909e-03, -2.55433284e-02,  1.92346238e-02,\n",
       "         -6.98696375e-02, -2.24919468e-02,  3.80412564e-02,  1.29227236e-01,\n",
       "         -5.44560433e-04, -3.34570520e-02,  3.57626490e-02, -3.81744578e-02,\n",
       "         -8.48288164e-02, -9.27415583e-03,  1.17239971e-02,  1.91531982e-02,\n",
       "          1.94253132e-03,  6.11244477e-02, -8.04661661e-02,  1.04607651e-02,\n",
       "          1.21424263e-02, -2.72333045e-02, -4.10868879e-03, -6.57112002e-02,\n",
       "         -1.84059460e-02, -6.65179491e-02, -7.35573694e-02, -1.17430044e-02,\n",
       "         -1.87040195e-02, -4.49493788e-02, -4.74081142e-03,  1.41869555e-03,\n",
       "          1.28621673e-02, -1.18375514e-02, -2.79216655e-02,  1.84312612e-02,\n",
       "          2.37213820e-02,  2.97896024e-02,  2.50924584e-02, -5.55820651e-02,\n",
       "         -4.36266065e-02,  1.01465834e-02, -3.82986851e-03,  1.10773388e-02,\n",
       "         -1.61719527e-02,  7.59935826e-02, -8.11019093e-02, -3.70008238e-02,\n",
       "         -2.59742234e-02, -1.09163314e-01,  1.89193711e-02,  7.41114418e-05,\n",
       "         -2.44386438e-02, -3.66765857e-02, -1.39068300e-02,  1.03906691e-02,\n",
       "         -1.15548987e-02,  2.41763052e-03, -2.58670766e-02,  1.93139017e-02,\n",
       "          6.91829808e-03, -3.36135775e-02, -6.35603219e-02, -3.22275013e-02,\n",
       "          4.97784419e-03,  3.40178143e-03, -3.04151140e-02,  7.43762255e-02,\n",
       "         -4.04082760e-02,  2.52909549e-02,  2.09664786e-03,  3.30060199e-02,\n",
       "         -5.90690121e-04,  8.52461066e-03, -5.01879491e-02,  1.17713241e-02,\n",
       "         -4.15235311e-02,  1.65564604e-02, -4.50980291e-03,  7.85825402e-03,\n",
       "         -3.26922871e-02,  1.80209316e-02, -3.60687040e-02,  8.41566324e-02,\n",
       "         -3.96301895e-02, -6.63994346e-03,  2.97310553e-03,  2.58796066e-02,\n",
       "          1.33174807e-02,  4.26155422e-03,  3.80087282e-33, -1.08598070e-02,\n",
       "         -1.94677059e-02,  1.54828047e-03, -2.21641026e-02,  4.13463713e-05,\n",
       "         -3.53842182e-03, -3.59343849e-02,  3.06831021e-02, -8.85913428e-03,\n",
       "         -1.32547254e-02, -3.53717245e-02,  1.68270878e-02,  7.03031290e-03,\n",
       "          1.32197607e-02, -1.79706346e-02,  1.84351054e-03,  1.04814418e-01,\n",
       "         -1.36527484e-02, -1.32021755e-02, -1.56419501e-02, -3.01142987e-02,\n",
       "          3.51497494e-02, -6.96482211e-02,  4.84066159e-02, -4.04351903e-03,\n",
       "         -1.38156088e-02, -3.54719795e-02, -2.02887412e-02, -5.97357787e-02,\n",
       "          4.21305150e-02,  1.32758403e-02,  1.95663385e-02,  2.97833476e-02,\n",
       "         -2.39286870e-02, -9.75505561e-02, -5.27947396e-02,  7.64289964e-03,\n",
       "          2.76766401e-02, -1.85239371e-02, -1.30844219e-02, -2.19286773e-02,\n",
       "          1.87645517e-02, -1.18125165e-02,  3.95223610e-02, -7.01384991e-02,\n",
       "          1.43320234e-02,  2.52362546e-02, -4.13916707e-02, -6.67895097e-03,\n",
       "         -1.64075624e-02,  6.37261569e-02,  2.48172525e-02, -2.02719904e-02,\n",
       "         -6.13172501e-02,  7.34476745e-02,  1.09300941e-01,  1.50410905e-01,\n",
       "         -7.64772221e-02,  8.39704275e-02, -1.46477921e-02,  3.40272859e-03,\n",
       "          4.00458872e-02,  6.05906919e-03, -1.53307838e-03, -2.91426536e-02,\n",
       "          4.59111072e-02, -2.47258171e-02, -3.66357714e-03, -3.67707349e-02,\n",
       "         -5.06786965e-02,  7.41629023e-03, -5.56461774e-02,  4.54982147e-02,\n",
       "          3.81560847e-02,  1.89508609e-02,  3.22764628e-02, -4.00406905e-02,\n",
       "         -3.11314613e-02,  4.91125472e-02, -4.69850376e-02, -1.87819619e-02,\n",
       "          1.03788069e-02,  6.14597723e-02,  3.55194352e-04, -3.58588025e-02,\n",
       "          8.39217007e-03,  4.55866940e-02, -3.86308157e-03, -2.78415103e-02,\n",
       "         -1.71893742e-02,  7.99393374e-03, -6.00218512e-02, -1.13654304e-02,\n",
       "          2.68363673e-03, -9.60667245e-03,  4.62943390e-02,  1.42293796e-02,\n",
       "         -1.65657781e-03, -1.11685842e-02, -2.64592674e-02, -5.34921959e-02,\n",
       "         -1.72306728e-02,  2.49882378e-02,  3.81603874e-02, -9.84679558e-04,\n",
       "          2.72375289e-02, -3.96024957e-02,  6.96640054e-04, -1.92873944e-02,\n",
       "         -4.21495996e-02,  4.64276746e-02,  5.05266972e-02,  4.50374484e-02,\n",
       "         -6.29146099e-02,  1.26612401e-02,  3.20065357e-02, -3.75283435e-02,\n",
       "          1.41531555e-02,  5.45692481e-02,  2.98102275e-02, -1.62041541e-02,\n",
       "         -1.93950385e-02, -5.61814383e-03, -3.65556255e-02, -6.28808793e-03,\n",
       "          3.51356380e-02, -1.15721002e-02, -1.41350301e-02, -5.59207331e-03,\n",
       "         -7.35599920e-02, -3.35222296e-02,  3.69353816e-02, -3.38513590e-02,\n",
       "         -5.89311263e-03, -5.47667965e-03, -2.85832249e-02,  2.66214181e-02,\n",
       "         -1.37962811e-02,  1.03160851e-02, -2.16528773e-02,  2.16360185e-02,\n",
       "          8.97814054e-03, -3.80683830e-03,  3.94559763e-02,  3.59390606e-03,\n",
       "          1.38889151e-02, -4.60306332e-02, -4.56163660e-02, -2.52775066e-02,\n",
       "          2.06905901e-02,  1.09668383e-02, -3.58842649e-02,  4.49658260e-02,\n",
       "         -2.47240271e-02, -2.41831951e-02,  1.16387964e-03,  1.19823534e-02,\n",
       "          3.24944556e-02, -4.59780954e-02,  2.89016888e-02, -5.55569641e-02,\n",
       "          1.18392182e-03,  1.15508912e-02,  1.59716476e-02, -9.88817215e-03,\n",
       "          4.40618508e-02,  4.11627218e-02, -2.82348990e-02, -1.36847403e-02,\n",
       "          5.98294213e-02, -6.69818297e-02, -2.62064617e-02, -7.16951769e-03,\n",
       "          1.02763167e-02,  5.50837517e-02,  4.94858548e-02,  2.69962549e-02,\n",
       "          1.61033496e-02,  2.23412015e-03, -4.82905693e-02, -1.07490066e-02,\n",
       "          6.47630692e-02, -1.21893734e-02, -2.21912228e-02, -2.09970027e-02,\n",
       "         -4.24707495e-03, -3.67125683e-02,  4.39434871e-02, -4.20477465e-02,\n",
       "          4.98689413e-02,  1.23426877e-02, -2.53743056e-04,  1.25062531e-02,\n",
       "          1.57635976e-02,  4.34685238e-02,  1.64173674e-02,  9.82390717e-04,\n",
       "          4.74401191e-02,  1.84387695e-02,  1.96845587e-02, -5.34954853e-03,\n",
       "          2.60444377e-02, -6.41688844e-03, -5.54876344e-04,  1.02397669e-02,\n",
       "          1.43601745e-02, -4.27858271e-02,  3.95440310e-02,  1.64598674e-02,\n",
       "          3.56666371e-02, -8.89733247e-03,  5.21214008e-02, -2.83429846e-02,\n",
       "         -2.47939266e-02, -2.56559663e-02, -3.94198745e-02,  3.54227535e-02,\n",
       "          4.29825373e-02, -4.33771908e-02, -7.83215836e-02, -1.99304800e-02,\n",
       "         -1.55092273e-02,  2.03696564e-02, -7.54622817e-02,  1.53751494e-02,\n",
       "          3.46976146e-02, -3.93195450e-02,  7.05483509e-03, -2.47773919e-02,\n",
       "          3.04152686e-02, -5.19534983e-02,  7.07468763e-02,  4.77610435e-03,\n",
       "          2.46427786e-02, -2.34836303e-02,  2.58445628e-02,  7.86726736e-03,\n",
       "          1.25609031e-02,  3.30322459e-02,  4.44242992e-02,  1.35643985e-02,\n",
       "          2.66906302e-02,  8.58739093e-02,  9.75440815e-03, -2.07604859e-02,\n",
       "          3.09289973e-02, -6.00392222e-02,  4.44811955e-02, -8.38739797e-02,\n",
       "          5.09752743e-02,  3.85114253e-02,  1.08935470e-02,  2.90369634e-02,\n",
       "         -8.32993723e-03,  1.12799229e-03, -3.74831483e-02,  6.48108348e-02,\n",
       "         -1.86038185e-02, -2.47428520e-03,  3.98574024e-03, -5.95328957e-02,\n",
       "          2.66087148e-02,  1.82587432e-03, -1.46907857e-02,  1.00301532e-02,\n",
       "         -4.24422361e-02, -1.90152135e-02, -6.46198615e-02,  1.91943068e-02,\n",
       "         -1.62142906e-02, -4.47491668e-02, -2.38318555e-02, -9.71099176e-03]),\n",
       "  'score': tensor(0.0857)},\n",
       " {'page_number': 28,\n",
       "  'sentence_chunk': '“I want to read that letter,” he said loudly.   “I want to read it,” said Harry furiously, “as it’s mine.”   “Get out, both of you,” croaked Uncle Vernon, stuffing the letter back inside its envelope.   Harry didn’t move.   “I WANT MY LETTER!”he shouted.   “Let me see it!”demanded Dudley.   “OUT!”roared Uncle Vernon, and he took both Harry and Dudley by the scruffs of their necks and threw them into the hall, slamming the kitchen door behind them. Harry and Dudley promptly had a furious but silent fight over who would listen at the keyhole; Dudley won, so Harry, his glasses dangling from one ear, lay flat on his stomach to listen at the crack between door and floor.   “Vernon,” Aunt Petunia was saying in a quivering voice, “look at the address — how could they possibly know where he sleeps?You don’t think they’re watching the house?”   “Watching — spying — might be following us,” muttered Uncle Vernon wildly.   “But what should we do, Vernon?Should we write back?Tell them we don’t want —”    Harry could see Uncle Vernon’s shiny black shoes pacing up and down the kitchen.   “No,” he said finally. “No, we’ll ignore it. If they don’t get an answer… Yes, that’s best…we won’t do anything…”    “But —”    “I’m not having one in the house, Petunia!',\n",
       "  'chunk_char_count': 1260,\n",
       "  'chunk_word_count': 244,\n",
       "  'chunk_token_count': 315.0,\n",
       "  'embedding': array([-3.26919705e-02, -8.78237560e-02,  2.02282462e-02, -3.56909446e-02,\n",
       "          2.72317417e-02,  4.74114493e-02,  7.34679960e-03,  2.59647109e-02,\n",
       "          2.03177761e-02,  3.33728082e-02, -1.22947339e-03, -2.44965274e-02,\n",
       "         -2.76429448e-02, -4.85818163e-02, -1.01145683e-02, -6.17298037e-02,\n",
       "          5.49275614e-02, -3.36312577e-02, -5.79734221e-02, -2.41512042e-02,\n",
       "          3.42755578e-02, -2.25262120e-02, -3.43183801e-02, -5.07083628e-03,\n",
       "         -3.30032669e-02, -1.34964455e-02,  4.12790962e-02,  3.64882424e-02,\n",
       "         -1.05006397e-02,  9.04512242e-04,  3.74825206e-03, -3.29231136e-02,\n",
       "          3.85911576e-02,  2.55063996e-02, -9.65536758e-03,  5.95291611e-03,\n",
       "          3.81992571e-02, -1.47056831e-02,  1.86597593e-02, -1.75053459e-02,\n",
       "         -8.86573642e-02,  2.57765129e-02, -1.50743099e-02, -2.84098946e-02,\n",
       "          5.76897711e-03, -9.22315102e-03, -3.86401229e-02,  6.90279063e-03,\n",
       "          4.10518236e-03, -2.44915709e-02,  6.35362864e-02,  2.69650854e-02,\n",
       "         -3.31275426e-02,  2.94149318e-03,  1.05540814e-04,  4.62760851e-02,\n",
       "          2.60572713e-02,  2.45963912e-02, -1.96343902e-02, -5.38165532e-02,\n",
       "          5.75087517e-02,  8.15832056e-03,  1.02501875e-02,  1.25851519e-02,\n",
       "          3.12188286e-02, -4.46326844e-02, -4.77805287e-02, -3.41250142e-03,\n",
       "         -5.18350601e-02,  1.27524463e-02, -7.32679432e-03, -4.23224866e-02,\n",
       "         -3.83772254e-02,  4.28191423e-02, -1.92915369e-02, -2.53205020e-02,\n",
       "          2.73904689e-02,  9.99032054e-03,  2.19010953e-02, -4.28197702e-04,\n",
       "         -2.71886885e-02,  8.79869089e-02,  6.28445996e-03,  3.21885794e-02,\n",
       "         -3.39163584e-03,  1.10293180e-02,  2.31188461e-02, -8.09529945e-02,\n",
       "          2.17062887e-02, -9.87760909e-03,  3.85027975e-02,  6.82097897e-02,\n",
       "          2.78426949e-02,  9.11827981e-02, -7.01441756e-03,  1.30176153e-02,\n",
       "         -4.13145497e-02,  2.32850686e-02, -9.33659915e-03, -4.81141396e-02,\n",
       "          1.41403256e-02, -2.43674889e-02, -7.66461194e-02, -2.21325252e-02,\n",
       "          1.29354338e-03,  1.54473400e-03, -1.87210534e-02, -3.17250192e-02,\n",
       "         -5.07407412e-02, -4.14638966e-02, -1.10023087e-02,  1.77660529e-02,\n",
       "         -1.82927959e-02,  3.84743325e-02, -3.16087902e-03, -8.00928465e-05,\n",
       "          5.57866283e-02, -3.26065160e-02,  2.07004957e-02, -4.49134819e-02,\n",
       "          3.15882340e-02, -1.82146253e-03,  2.01857649e-02,  5.89574017e-02,\n",
       "         -2.32101697e-03,  1.67048275e-02,  2.49999389e-02, -4.47050519e-02,\n",
       "          5.29340981e-03,  2.90092584e-02, -9.00436118e-02,  1.15836328e-02,\n",
       "          4.02525403e-02, -5.72710596e-02, -4.19213511e-02,  7.42537621e-03,\n",
       "          5.58765419e-02,  5.54152317e-02,  1.37730939e-02,  3.45887132e-02,\n",
       "         -3.94570362e-03,  4.02464755e-02, -2.14173645e-02,  1.24101317e-03,\n",
       "          2.61919554e-02, -5.75945759e-03, -9.79074743e-03,  4.23265882e-02,\n",
       "         -4.56964225e-02,  1.13746695e-01,  4.96208034e-02, -1.18812919e-02,\n",
       "         -2.61337515e-02, -1.10049043e-02,  1.48423845e-02,  1.86637917e-03,\n",
       "         -1.63300484e-02,  4.91029397e-02, -2.24568113e-03,  1.60352560e-03,\n",
       "         -3.46140238e-03,  4.31442708e-02,  3.83792259e-02,  1.79908350e-02,\n",
       "         -1.29752625e-02,  1.11484025e-02, -5.64766536e-03,  1.55404042e-02,\n",
       "          3.37284878e-02, -1.37144504e-02, -5.25070773e-03, -1.36907306e-02,\n",
       "          2.07130276e-02,  4.33686972e-02, -2.61340700e-02,  1.30041996e-02,\n",
       "         -9.09749977e-03, -3.14893909e-02,  1.51346680e-02,  2.18386780e-02,\n",
       "         -6.69796318e-02,  2.10845172e-02, -1.02990028e-02, -4.41061221e-02,\n",
       "          2.48760786e-02, -1.64723136e-02, -4.51582409e-02, -4.66649979e-02,\n",
       "         -4.70920615e-02,  4.19454789e-03, -1.62781477e-02,  1.99116599e-02,\n",
       "          5.85918278e-02, -1.70631818e-02,  2.97183786e-02,  7.92518782e-04,\n",
       "         -3.35689485e-02,  5.65665076e-03,  2.84704566e-02, -3.27441916e-02,\n",
       "          2.46566739e-02, -6.37393147e-02, -2.60136109e-02, -8.72412995e-02,\n",
       "         -8.71748254e-02,  3.86480521e-03, -8.03740881e-03, -5.86149609e-03,\n",
       "          2.61484669e-03,  6.94335848e-02, -2.87837232e-03, -1.30391205e-02,\n",
       "         -7.09165260e-02, -6.70379912e-03,  4.26154360e-02, -2.87811668e-03,\n",
       "          2.63111026e-04, -8.37149750e-03, -2.10293792e-02,  1.74281485e-02,\n",
       "         -2.74921432e-02,  2.72492338e-02, -1.24810152e-02, -1.80122964e-02,\n",
       "          2.49610245e-02, -5.37590384e-02, -1.80423632e-02,  3.07968119e-03,\n",
       "         -3.74918990e-02,  8.18664357e-02, -4.48834486e-02,  5.33330860e-03,\n",
       "         -3.81365716e-02, -4.56836447e-02, -5.99416858e-03, -3.73320617e-02,\n",
       "         -1.38920918e-02,  1.09617850e-02,  2.05861405e-02,  5.17524220e-02,\n",
       "         -3.31210382e-02, -6.08524634e-03,  3.04266717e-02, -1.83631107e-02,\n",
       "          5.89247514e-03, -3.94500680e-02,  4.16845270e-02,  5.67639135e-02,\n",
       "          5.60576618e-02, -5.95940724e-02,  1.29566779e-02, -2.06518359e-03,\n",
       "          1.11343777e-02, -1.16448160e-02,  3.71095315e-02, -2.05051452e-02,\n",
       "         -3.92075104e-04,  5.31303836e-03,  6.24274164e-02,  3.28464396e-02,\n",
       "         -2.45229565e-02, -1.79195963e-02, -7.90857896e-02, -3.34671582e-03,\n",
       "         -3.32053229e-02, -6.00080797e-03, -6.25187308e-02, -8.57890863e-03,\n",
       "         -2.14583445e-02,  2.72069336e-03,  2.05665864e-02, -1.21243130e-02,\n",
       "         -1.00215459e-02,  4.03431207e-02,  1.92165002e-02,  5.36863804e-02,\n",
       "         -2.96936892e-02,  8.43524933e-02,  3.22768576e-02, -3.86905074e-02,\n",
       "          1.13482289e-02, -9.91286412e-02, -3.82332988e-02, -3.02179325e-02,\n",
       "         -8.44817888e-03,  2.43163249e-03,  4.98978654e-03,  6.81732371e-02,\n",
       "          3.20719811e-03,  4.90127550e-03, -1.57116763e-02,  4.93676141e-02,\n",
       "          4.99155596e-02,  3.96449193e-02, -1.86000653e-02,  4.02694009e-02,\n",
       "         -1.18608726e-02, -7.26167411e-02,  1.13788564e-02, -2.13499963e-02,\n",
       "          1.89339817e-02, -3.92908677e-02, -6.76761270e-02,  2.57303584e-02,\n",
       "         -6.03019819e-03,  1.66631136e-02, -4.83479723e-03,  1.30479047e-02,\n",
       "         -9.47275013e-03,  1.02069318e-01, -1.55422902e-02,  3.70848295e-03,\n",
       "         -1.38273817e-02,  5.80824800e-02, -6.56929016e-02, -6.40348624e-03,\n",
       "          1.75837595e-02, -6.23518415e-02,  3.40381190e-02,  8.68713949e-03,\n",
       "         -4.02190723e-03, -1.86545812e-02, -3.10479868e-02, -9.74860974e-03,\n",
       "          3.44225504e-02, -4.06206846e-02, -2.85000016e-04, -2.04569865e-02,\n",
       "         -1.82220303e-02,  4.25314158e-03, -6.74838200e-02,  9.03627370e-03,\n",
       "         -3.54312398e-02, -6.53348351e-03,  1.40178427e-02, -6.62259012e-03,\n",
       "          3.66860032e-02, -2.17276327e-02,  2.11865529e-02,  2.56762039e-02,\n",
       "         -2.72576604e-02,  2.64812540e-03, -5.69416545e-02,  7.41439387e-02,\n",
       "          2.21986230e-02, -9.95636359e-03, -7.28392899e-02,  3.80161442e-02,\n",
       "         -7.83804059e-03, -1.95980482e-02,  5.01249917e-03,  4.00784332e-03,\n",
       "         -3.14775966e-02,  4.82944585e-02, -3.17043401e-02,  5.90137765e-03,\n",
       "         -1.57088861e-02, -5.86798564e-02,  6.42283921e-05,  3.95430345e-03,\n",
       "         -2.25540008e-02,  2.85174958e-02,  1.84789747e-02, -1.28180254e-02,\n",
       "         -3.72483730e-02, -8.08173940e-02, -5.33599034e-02,  5.84275872e-02,\n",
       "          9.34775732e-03,  3.28625157e-03, -4.58307266e-02,  4.19669263e-02,\n",
       "          4.65863273e-02, -3.38338763e-02, -4.55953181e-02,  1.24769202e-02,\n",
       "         -8.36862475e-02, -4.52789441e-02,  1.03484638e-01,  5.13268122e-03,\n",
       "         -2.59826686e-02, -3.67977135e-02, -5.68861030e-02, -2.53791027e-02,\n",
       "          1.49222407e-02,  1.15073398e-02, -8.91255494e-03, -4.62533068e-03,\n",
       "          4.34289314e-02, -4.32393188e-03,  3.80746508e-03,  2.76760962e-02,\n",
       "          8.31729360e-03,  1.49059203e-02,  2.97663696e-02, -2.28084228e-03,\n",
       "         -8.43656890e-04,  4.71263304e-02,  5.11982813e-02,  2.59657446e-02,\n",
       "          2.10427213e-03,  4.60741110e-02, -1.73636794e-03, -2.45306529e-02,\n",
       "         -2.53452267e-02,  9.54203494e-03, -8.27871822e-03, -5.33841026e-04,\n",
       "         -5.18525355e-02, -5.00365235e-02,  3.20360065e-02,  6.98709562e-02,\n",
       "         -3.03979665e-02, -9.18228999e-02,  3.33484150e-02, -3.78940515e-02,\n",
       "          1.00197000e-02, -8.53519700e-03,  4.61903885e-02,  1.96104571e-02,\n",
       "          2.80840397e-02,  1.05390493e-02, -2.35970952e-02,  4.57771830e-02,\n",
       "         -4.62987646e-02, -1.79213583e-02, -9.24399402e-03, -8.23289007e-02,\n",
       "          1.51915345e-02, -5.50404079e-02, -7.51567185e-02, -2.39963923e-02,\n",
       "         -2.61266716e-02, -1.08922431e-02,  5.38460398e-03,  2.35150680e-02,\n",
       "          4.62028868e-02,  1.45722143e-02,  3.31198722e-02, -3.75108724e-03,\n",
       "         -1.77662652e-02,  3.47390659e-02,  6.70003612e-03, -6.38348907e-02,\n",
       "         -3.84617671e-02, -3.44752497e-03, -6.28927071e-03,  5.03459072e-04,\n",
       "         -7.69553101e-03, -3.48400585e-02, -5.59609383e-02, -1.79617628e-02,\n",
       "         -3.83820310e-02, -1.34868145e-01, -1.80667453e-02, -5.57744876e-03,\n",
       "          2.38958211e-03, -2.05240250e-02, -3.39077674e-02,  2.64994726e-02,\n",
       "         -5.63772954e-03,  1.21605713e-02,  1.95433665e-02, -4.67917649e-03,\n",
       "          4.61787097e-02, -2.24726256e-02, -4.67705466e-02, -6.91739842e-02,\n",
       "          1.00226756e-02,  4.67080697e-02, -5.90712717e-03,  1.41164185e-02,\n",
       "          3.34763490e-02, -2.78502069e-02,  2.71578636e-02,  4.39257175e-02,\n",
       "          8.09850637e-03,  2.68998891e-02, -4.21032496e-02,  4.50541824e-02,\n",
       "          1.97982695e-02,  5.00285067e-02, -4.52947291e-03, -3.34911309e-02,\n",
       "         -1.36679569e-02,  1.89723037e-02,  6.08483795e-03,  1.18860610e-01,\n",
       "         -2.81737372e-02, -1.77564658e-02,  4.79407310e-02, -2.21855212e-02,\n",
       "          2.41744313e-02,  8.50554928e-03,  3.80429792e-33, -2.54607550e-03,\n",
       "          2.15137731e-02,  3.25954668e-02,  1.27136791e-02, -1.29649304e-02,\n",
       "         -1.20504631e-03, -2.04762556e-02,  5.97064709e-03,  3.88875194e-02,\n",
       "          2.35121883e-02, -3.35665271e-02, -1.33401779e-02, -1.78812742e-02,\n",
       "         -2.78100534e-03,  1.26349349e-02,  1.79137588e-02,  7.02543929e-02,\n",
       "         -8.69139284e-03, -2.21186168e-02,  1.94523635e-03, -1.50645645e-02,\n",
       "          5.91596626e-02, -4.30304222e-02,  7.13863894e-02,  3.96111123e-02,\n",
       "          1.57884937e-02,  2.46410836e-02, -3.54236029e-02, -5.09111164e-03,\n",
       "          1.94088779e-02,  8.55525304e-03, -9.04191006e-03,  4.11163904e-02,\n",
       "         -7.28667155e-02, -8.50779340e-02, -2.40182094e-02,  2.06923541e-02,\n",
       "          9.97452252e-03, -2.82650366e-02,  1.03598682e-03,  1.67334263e-04,\n",
       "          5.39266318e-02, -3.39023843e-02, -2.52877083e-03, -3.49358283e-02,\n",
       "         -6.16361713e-03,  4.56105731e-02, -1.12265581e-02,  9.83033702e-03,\n",
       "         -1.39946239e-02,  5.08889854e-02,  7.76014104e-03, -2.64380593e-02,\n",
       "         -4.49869148e-02, -1.21614486e-02, -2.93759629e-03,  1.84634522e-01,\n",
       "         -4.55034003e-02,  2.14561261e-02, -7.34617934e-03,  5.25914365e-03,\n",
       "          1.82358995e-02,  3.14102415e-03,  2.83103287e-02, -3.44554186e-02,\n",
       "          1.98264956e-03, -4.80458438e-02,  1.02951825e-02,  3.89866382e-02,\n",
       "          1.66292079e-02,  1.37382643e-02, -3.55571695e-02,  7.55629549e-03,\n",
       "          1.35134971e-02, -3.87544483e-02,  9.48184356e-03,  2.35845149e-02,\n",
       "         -2.30230740e-03,  6.39959425e-02, -3.06559145e-03, -1.27062518e-02,\n",
       "         -3.41080055e-02,  2.89476905e-02,  3.28945778e-02,  2.75653396e-02,\n",
       "          7.77764898e-03, -2.24319790e-02,  3.05959005e-02, -3.54382619e-02,\n",
       "          1.14818793e-02, -4.51975502e-02,  6.44024462e-02, -9.88722499e-03,\n",
       "          2.27953028e-03, -1.30694481e-02,  1.98692027e-02,  1.06504587e-02,\n",
       "          2.28916816e-02, -7.03840889e-03, -3.34091559e-02, -2.45975461e-02,\n",
       "         -3.38840224e-02,  1.33409649e-02, -3.04791853e-02,  4.84550744e-02,\n",
       "          9.22864769e-04, -1.46353813e-02, -5.46116801e-03, -1.45620322e-02,\n",
       "         -1.31225167e-02, -2.42818743e-02,  1.07970059e-01,  2.51768976e-02,\n",
       "         -7.17389435e-02,  1.90473739e-02,  1.32678906e-02, -7.17046186e-02,\n",
       "         -1.62337814e-02,  4.57721539e-02,  2.58853529e-02, -8.39149952e-02,\n",
       "         -2.50576530e-02, -1.72371361e-02, -3.78759541e-02,  3.10091004e-02,\n",
       "          5.51595055e-02,  2.01881677e-03, -2.26349127e-03, -2.76416391e-02,\n",
       "         -4.97977696e-02, -3.46796289e-02,  1.30762272e-02,  1.02395588e-03,\n",
       "         -5.25184460e-02, -4.00272943e-02, -3.08065377e-02,  6.15006592e-03,\n",
       "          6.17428357e-03, -4.30766195e-02, -1.36801442e-02,  2.67169047e-02,\n",
       "         -6.03981456e-03,  7.25068152e-02,  4.82506910e-03, -2.48657372e-02,\n",
       "         -1.38723906e-02,  7.70281721e-03,  4.16447408e-02,  1.20840585e-02,\n",
       "         -4.28943634e-02,  3.63960341e-02, -2.13425085e-02,  4.08948660e-02,\n",
       "         -8.99411086e-03, -1.07698524e-02, -1.60742756e-02,  1.78514291e-02,\n",
       "          1.88078987e-03, -6.53988123e-02,  4.36420925e-02,  5.93246222e-02,\n",
       "         -1.71621889e-02,  3.72655503e-02, -4.55705710e-02,  2.92229615e-02,\n",
       "          8.83428473e-03,  1.40270963e-02, -3.08281835e-02, -2.21579075e-02,\n",
       "          2.36412343e-02, -1.93060338e-02,  7.23411515e-03,  1.41571788e-02,\n",
       "          2.14524604e-02,  4.59389351e-02,  5.39728180e-02, -2.16762386e-02,\n",
       "          3.74192633e-02, -4.13936935e-03, -7.60568678e-02,  3.05979773e-02,\n",
       "          1.71484519e-02, -5.02097234e-02, -5.26039535e-03,  2.83620413e-02,\n",
       "         -2.46704426e-02, -4.20708433e-02,  3.58249173e-02, -2.43928302e-02,\n",
       "         -8.71623214e-03,  2.70140693e-02, -1.04623027e-02,  4.31815498e-02,\n",
       "          1.19692478e-02,  3.93058993e-02,  9.83762366e-05,  3.30338851e-02,\n",
       "         -1.16971023e-02,  2.63954364e-02,  4.75809537e-03,  2.84430236e-02,\n",
       "         -2.24052928e-02,  3.98420729e-03, -1.18503394e-02,  2.49532480e-02,\n",
       "          3.99476336e-03,  4.73461783e-04,  5.09462170e-02, -2.56586373e-02,\n",
       "          8.52382276e-03, -3.31587940e-02,  7.07583651e-02, -5.43009229e-02,\n",
       "          7.56643619e-03, -2.39990409e-02,  3.09544373e-02, -3.31449993e-02,\n",
       "          5.88027909e-02, -2.14312449e-02, -2.84079351e-02,  6.92074886e-03,\n",
       "         -3.89350392e-02, -7.45838182e-03, -6.68926956e-03,  3.39234546e-02,\n",
       "          1.08880691e-01, -1.87631771e-02,  8.34289193e-03, -3.69834830e-03,\n",
       "         -1.60472877e-02, -6.30174130e-02,  1.10543504e-01,  4.10817973e-02,\n",
       "         -4.81299050e-02, -1.04391314e-02, -3.56017372e-05,  3.18729207e-02,\n",
       "          4.12357450e-02,  4.84278277e-02,  4.33478951e-02,  4.32377197e-02,\n",
       "          3.70727619e-03,  6.50051832e-02, -2.43674759e-02, -7.49836490e-02,\n",
       "          1.47485435e-02, -2.76494473e-02,  7.26631284e-02, -1.25408228e-02,\n",
       "          1.91163439e-02,  9.08577442e-03,  4.18728478e-02, -4.88887588e-03,\n",
       "         -2.59573571e-02, -2.36858148e-02, -4.86237146e-02,  8.89180750e-02,\n",
       "         -5.57559282e-02,  4.27132612e-03, -1.21857384e-02, -4.05284464e-02,\n",
       "         -2.04458684e-02,  1.28502864e-02, -1.23461485e-02,  1.72064025e-02,\n",
       "         -1.43455220e-02, -4.72548679e-02, -7.06443470e-03, -9.16925073e-03,\n",
       "          4.00518849e-02, -2.02096608e-02, -5.74991778e-02, -1.44475652e-02]),\n",
       "  'score': tensor(0.0796)},\n",
       " {'page_number': 30,\n",
       "  'sentence_chunk': '“Dudley — go — just go.”   Harry walked round and round his new room. Someone knew he had moved out of his cupboard and they seemed to know he hadn’t received his first letter. Surely that meant they’d try again?And this time he’d make sure they didn’t fail. He had a plan. The repaired alarm clock rang at six o’clock the next morning. Harry turned it off quickly and dressed silently. He mustn’t wake the Dursleys. He stole downstairs without turning on any of the lights.   He was going to wait for the postman on the corner of Privet Drive and get the letters for number four first. His heart hammered as he crept across the dark hall toward the front door —    “AAAAARRRGH!”   Harry leapt into the air; he’d trodden on something big and squashy on the doormat — something alive!   Lights clicked on upstairs and to his horror Harry realized that the big, squashy something had been his uncle’s face. Uncle Vernon had been lying at the foot of the front door in a sleeping bag, clearly making sure that Harry didn’t do exactly what he’d been trying to do. He shouted at Harry for about half an hour and then told him to go and make a cup of tea. Harry shuffled miserably off into the kitchen and by the time he got back, the mail had arrived, right into Uncle Vernon’s lap. Harry could see three letters addressed in green ink.   “I want —” he began, but Uncle Vernon was tearing the letters into pieces before his eyes.   Uncle Vernon didn’t go to work that day.',\n",
       "  'chunk_char_count': 1467,\n",
       "  'chunk_word_count': 285,\n",
       "  'chunk_token_count': 366.75,\n",
       "  'embedding': array([-1.98950507e-02, -6.86561614e-02,  5.78382192e-03, -8.08113534e-03,\n",
       "          5.68708517e-02,  4.66487259e-02, -1.02603594e-02,  2.40449747e-03,\n",
       "         -7.53567368e-03,  2.38865669e-05,  4.72724773e-02, -2.57714074e-02,\n",
       "         -4.89323912e-03, -1.97959784e-02, -4.25455384e-02, -2.57027354e-02,\n",
       "          4.69228849e-02, -3.22547965e-02, -5.26396148e-02, -1.95526145e-02,\n",
       "          3.73984426e-02, -3.32618393e-02,  7.13450648e-03, -9.36350878e-03,\n",
       "         -1.89408697e-02, -1.65222306e-02,  6.66653737e-02,  1.82754584e-02,\n",
       "         -5.99126704e-02,  2.44846717e-02, -3.12736109e-02, -3.48089971e-02,\n",
       "          3.69363502e-02,  4.33106124e-02, -6.15783827e-03,  2.40520425e-02,\n",
       "          3.62468585e-02, -2.87446473e-03,  2.30497401e-02, -2.54483502e-02,\n",
       "         -4.74306494e-02, -9.93829221e-03,  2.56914436e-03, -3.16255423e-03,\n",
       "          1.86636765e-02, -8.07802100e-03, -5.16270548e-02,  3.61356474e-02,\n",
       "          3.37028541e-02,  1.28300171e-02,  3.94333526e-02,  8.30693915e-03,\n",
       "         -2.72222348e-02,  6.27989545e-02,  3.89038748e-03,  2.21881308e-02,\n",
       "          2.18376014e-02,  6.96246326e-02, -4.82933130e-03, -3.22650857e-02,\n",
       "          7.07562193e-02, -1.43245957e-03, -2.38792170e-02,  6.52177026e-03,\n",
       "          1.87147763e-02, -1.84849165e-02, -5.02648503e-02, -2.09730230e-02,\n",
       "         -3.48781459e-02, -1.72979347e-02,  8.18528794e-03, -2.78448295e-02,\n",
       "         -1.06299538e-02,  9.80557129e-03, -2.00577490e-02,  2.62623653e-02,\n",
       "         -2.41964348e-02,  2.12203525e-02, -1.20450696e-02, -4.68901545e-03,\n",
       "         -5.88340685e-03,  5.44745885e-02,  1.32588912e-02,  1.37001295e-02,\n",
       "          2.24733576e-02,  2.15813033e-02,  2.37631090e-02, -8.07714760e-02,\n",
       "          1.44562116e-02,  1.13603137e-02,  2.30046269e-02,  7.48247951e-02,\n",
       "          5.12539735e-03,  1.00407615e-01, -1.03873089e-02, -1.06181446e-02,\n",
       "         -4.56640944e-02,  1.04390949e-01, -4.36385395e-03, -2.04236452e-02,\n",
       "          2.15643458e-02, -2.69562360e-02, -6.78917095e-02, -1.46893822e-02,\n",
       "          2.69873459e-02, -2.53741797e-02,  1.16527723e-02, -3.21380571e-02,\n",
       "         -2.99854539e-02, -2.34159417e-02, -3.48750092e-02, -6.93149492e-03,\n",
       "         -8.72107409e-03,  2.93768793e-02,  1.58801340e-02, -4.52336948e-03,\n",
       "          2.44662613e-02,  5.00964560e-03,  2.83359252e-02, -6.95146527e-03,\n",
       "          2.33443603e-02, -2.31597908e-02,  3.55256498e-02,  5.04617393e-02,\n",
       "          1.95630379e-02,  2.05724966e-02,  4.75219116e-02, -3.99288684e-02,\n",
       "         -2.13111397e-02, -1.11025674e-02, -9.40277278e-02, -1.34836268e-02,\n",
       "          2.03626417e-02, -6.63478300e-02, -2.71658078e-02,  2.30527036e-02,\n",
       "          3.51623110e-02,  4.15009335e-02,  1.68046710e-04,  4.75578420e-02,\n",
       "         -2.26371046e-02,  5.00151850e-02, -2.06787847e-02,  2.33121356e-03,\n",
       "         -2.90447026e-02,  2.47393437e-02, -2.59333337e-03,  3.91439684e-02,\n",
       "         -4.50909249e-02,  1.30556062e-01,  4.58211675e-02, -2.08146982e-02,\n",
       "         -2.25814097e-02, -3.42911594e-02, -1.99798532e-02, -1.89453512e-02,\n",
       "          6.29299274e-03,  5.91110513e-02,  1.10399686e-02,  8.22751131e-03,\n",
       "         -1.17645133e-02,  3.81567739e-02,  3.38782780e-02, -1.66535210e-02,\n",
       "         -3.41080539e-02,  1.44281453e-02,  2.49040518e-02, -9.59668308e-03,\n",
       "          3.71777490e-02, -4.51426730e-02,  7.72850309e-03,  9.47975088e-03,\n",
       "          2.38605533e-02,  4.67202626e-02, -3.33361477e-02,  2.25905497e-02,\n",
       "         -2.86323465e-02, -4.20111865e-02,  1.30170127e-02,  3.61073278e-02,\n",
       "         -3.50765362e-02,  4.80792858e-02, -4.83091688e-03, -1.58341862e-02,\n",
       "          8.92031100e-03,  2.25234032e-02, -2.60801660e-03, -2.01992467e-02,\n",
       "         -5.82072847e-02,  3.66065726e-02, -3.95867638e-02,  2.50286460e-02,\n",
       "          2.87074037e-02,  2.28794441e-02,  4.69776019e-02,  4.29761074e-02,\n",
       "         -1.63911004e-02,  5.73283667e-03,  2.78918073e-02, -7.03733712e-02,\n",
       "         -1.30067766e-02, -5.95036633e-02, -3.88158415e-03, -4.53041233e-02,\n",
       "         -6.51132166e-02,  1.12068504e-02,  3.14235091e-02,  1.64243914e-02,\n",
       "          2.31052446e-03,  2.04730742e-02, -3.29739377e-02, -1.00455778e-02,\n",
       "         -7.26506347e-03,  5.26737161e-02,  5.81770949e-02, -6.86314655e-03,\n",
       "          3.19235176e-02, -7.38538615e-03, -2.83331401e-03,  7.68221775e-03,\n",
       "         -1.55109176e-02, -7.57522788e-03, -4.24450561e-02,  2.74282694e-03,\n",
       "          2.12296422e-04, -8.60038176e-02, -1.57907400e-02, -4.89304075e-03,\n",
       "         -3.70510668e-02,  6.01666309e-02, -2.64690574e-02,  1.50428470e-02,\n",
       "         -4.67852280e-02, -7.46556148e-02, -3.18076089e-02, -4.33182344e-02,\n",
       "         -1.10261757e-02, -3.03949676e-02,  2.65905894e-02,  3.03387102e-02,\n",
       "         -1.21515002e-02,  3.60085210e-03, -5.33439871e-03, -1.79665606e-03,\n",
       "          2.91697885e-04, -2.98233591e-02,  2.69405730e-02, -1.17250392e-02,\n",
       "          7.50368312e-02, -5.23258522e-02,  2.34272461e-02, -2.09615752e-02,\n",
       "         -2.08757352e-02, -1.18893478e-02,  4.05755378e-02, -4.31546103e-03,\n",
       "          8.57125130e-03,  1.94365755e-02,  5.96440472e-02,  4.65205312e-02,\n",
       "          1.84622277e-02, -2.02900879e-02, -5.53153940e-02,  2.20616870e-02,\n",
       "         -1.26877483e-02,  3.15156206e-03, -4.44627255e-02,  3.61413732e-02,\n",
       "         -7.26093128e-02, -2.78712567e-02,  2.41352618e-03,  3.65624786e-03,\n",
       "         -1.36385476e-02,  4.76975329e-02,  2.49716546e-02,  3.38032767e-02,\n",
       "         -2.97273453e-02,  4.51169387e-02, -5.60362777e-03, -4.18826006e-02,\n",
       "         -5.67164784e-03, -4.35518697e-02, -1.97914802e-02, -2.89223995e-02,\n",
       "         -2.06689723e-02,  4.98052128e-02, -2.71762740e-02,  2.72806305e-02,\n",
       "          8.68112687e-03, -1.49820652e-02, -4.16303165e-02,  3.46588949e-03,\n",
       "          4.71696220e-02,  3.44492644e-02,  1.74717009e-02,  5.69412559e-02,\n",
       "         -1.81129314e-02, -6.47503883e-02, -9.74709075e-03, -3.98434065e-02,\n",
       "         -3.98037508e-02, -4.12676595e-02, -9.04297605e-02,  4.36882339e-02,\n",
       "         -2.05218680e-02,  8.42942204e-03,  1.42316315e-02,  1.84838418e-02,\n",
       "          1.97838973e-02,  5.12218028e-02, -3.09683615e-03,  1.55691663e-02,\n",
       "         -1.56740136e-02,  2.09370293e-02, -5.43494560e-02,  1.35742854e-02,\n",
       "          4.08071006e-04, -4.48260009e-02,  4.06289361e-02,  1.50229444e-03,\n",
       "          5.63824177e-03, -4.06502932e-02, -5.56543767e-02, -5.57908975e-03,\n",
       "          1.24200033e-02, -5.57911769e-02,  1.27365382e-03, -1.98913575e-03,\n",
       "         -2.80817635e-02,  6.21365430e-03, -7.35675618e-02,  3.73532623e-03,\n",
       "         -2.93134991e-02, -7.22434232e-03,  3.00796926e-02, -1.11169796e-02,\n",
       "          3.87239642e-02, -1.35590518e-02,  8.34591873e-03,  5.22664487e-02,\n",
       "         -3.70987542e-02, -7.87485205e-03, -8.40400383e-02,  1.55249937e-02,\n",
       "         -1.53460691e-03,  1.36225270e-02, -9.79106650e-02,  4.48448472e-02,\n",
       "         -2.44784784e-02, -2.60954443e-02,  1.74026042e-02,  2.38423813e-02,\n",
       "          2.97979154e-02,  4.25122678e-02, -4.07462344e-02,  1.67356394e-02,\n",
       "         -1.31862825e-02, -4.59317304e-02, -1.89784449e-03, -4.32136320e-02,\n",
       "         -6.26446158e-02,  5.03151529e-02,  2.94832401e-02, -1.35427434e-02,\n",
       "          2.43867207e-02, -1.04036383e-01, -7.10478351e-02,  8.24321061e-02,\n",
       "          2.05434654e-02, -1.72125660e-02, -1.10536655e-02,  8.03316012e-02,\n",
       "          1.76437199e-02, -3.15658227e-02, -1.47896940e-02, -1.49713922e-03,\n",
       "         -7.49131367e-02, -6.07184619e-02,  1.02721840e-01,  4.14675521e-03,\n",
       "          8.30758188e-04, -2.85737943e-02, -4.05158959e-02, -3.04960962e-02,\n",
       "          3.35248150e-02,  8.35321657e-03, -1.37288021e-02,  1.21344309e-02,\n",
       "          3.95213179e-02, -6.87898742e-03,  8.51746555e-03, -2.30854154e-02,\n",
       "         -3.74480523e-02,  2.05025375e-02,  2.71081869e-02,  5.86776179e-04,\n",
       "         -4.45270725e-02, -4.87755798e-03,  1.68924965e-02,  4.00863178e-02,\n",
       "          6.10517571e-03,  7.15782046e-02,  4.14354056e-02, -3.16504985e-02,\n",
       "         -3.49551328e-02,  1.97959431e-02, -3.01354788e-02,  2.23680935e-03,\n",
       "         -3.27982232e-02, -7.22042173e-02, -2.79779709e-03,  7.64917061e-02,\n",
       "         -2.50179321e-02, -8.52003321e-02,  4.82313782e-02, -3.42722572e-02,\n",
       "         -1.87707786e-02,  3.27873193e-02,  4.21285443e-02,  3.75363976e-02,\n",
       "          4.82572392e-02,  2.50283442e-02, -5.66600971e-02,  1.66895967e-02,\n",
       "         -3.52355391e-02, -1.61421497e-03,  1.60456076e-02, -5.46087101e-02,\n",
       "          1.47145139e-02, -6.61457479e-02, -1.00347839e-01, -1.23726893e-02,\n",
       "          2.28916127e-02, -1.80210378e-02, -7.77600473e-03,  1.80434026e-02,\n",
       "          4.78357077e-02,  1.32113248e-02,  1.29517624e-02, -7.17984419e-03,\n",
       "          5.46667445e-03,  2.49056630e-02,  3.74576859e-02, -6.65133223e-02,\n",
       "         -2.38676947e-02, -7.25845399e-04,  1.32807577e-02,  7.84426276e-03,\n",
       "         -2.37865094e-02,  1.22233732e-02, -5.06528504e-02, -2.56966539e-02,\n",
       "          2.56542172e-02, -1.26834035e-01, -8.90091807e-03, -1.61471718e-03,\n",
       "          2.08372087e-03, -2.97192261e-02, -1.75228044e-02,  2.39110161e-02,\n",
       "          6.57156669e-03,  1.77834108e-02, -3.00451159e-03,  1.77348275e-02,\n",
       "          3.60771455e-02, -5.05024893e-03, -4.02186401e-02, -4.24470231e-02,\n",
       "         -8.07700492e-03,  4.45541143e-02,  1.66470464e-02,  3.78879122e-02,\n",
       "         -2.25781724e-02, -3.23505029e-02,  3.72595191e-02,  4.08589393e-02,\n",
       "         -1.41446909e-03,  4.88331504e-02, -6.98490366e-02,  2.36648247e-02,\n",
       "         -6.28447020e-03,  1.38792470e-02,  1.04825450e-02, -1.28115173e-02,\n",
       "         -4.71450724e-02,  1.28511051e-02, -4.40494269e-02,  7.82667249e-02,\n",
       "         -3.85884307e-02, -1.92907825e-02,  2.77972016e-02,  2.29178835e-02,\n",
       "          2.06654537e-02,  2.49950420e-02,  4.03669352e-33, -1.91244371e-02,\n",
       "          1.42613682e-03,  1.58527214e-02,  1.22322133e-02, -3.06744538e-02,\n",
       "          8.20708089e-03, -2.78394036e-02,  5.78810386e-02,  3.30851264e-02,\n",
       "         -3.82471224e-03, -3.30012441e-02,  2.09714510e-02, -1.23978537e-02,\n",
       "          1.91218371e-03,  5.62422862e-03, -2.62545701e-03,  7.26175532e-02,\n",
       "         -5.69435535e-03,  1.13382924e-03, -9.75390244e-03, -5.18087372e-02,\n",
       "          4.73342091e-02, -2.49991771e-02,  1.03419043e-01, -1.09781334e-02,\n",
       "         -1.29047651e-02,  9.42287687e-03, -1.68418940e-02, -3.61021459e-02,\n",
       "          3.42104845e-02,  2.11271495e-02, -1.70300603e-02,  2.74063684e-02,\n",
       "         -2.48668753e-02, -1.05158955e-01, -5.04934415e-02,  2.57116351e-02,\n",
       "          1.17347371e-02, -4.63658161e-02, -2.24602483e-02, -9.37239453e-03,\n",
       "          2.59059034e-02, -2.26289574e-02, -2.28946786e-02, -8.10606703e-02,\n",
       "          2.77309436e-02,  2.16985587e-02, -2.15464793e-02, -4.65280609e-03,\n",
       "         -1.61494818e-02,  6.25762492e-02,  5.04050776e-02, -1.76887419e-02,\n",
       "         -4.12871055e-02,  6.70639500e-02,  4.48565222e-02,  1.71673253e-01,\n",
       "         -3.59017923e-02,  4.86310460e-02, -6.65658293e-03,  7.93687534e-03,\n",
       "          4.52048481e-02,  1.63549334e-02,  6.87117642e-03, -5.63673861e-02,\n",
       "          4.10065576e-02, -1.55567201e-02, -2.70084348e-02,  4.44131205e-03,\n",
       "         -1.11381626e-02,  4.05422933e-02, -5.04641868e-02,  3.33977044e-02,\n",
       "          3.28427441e-02, -2.54973732e-02, -6.32365281e-03, -1.49994455e-02,\n",
       "         -1.68440379e-02,  5.45551963e-02, -1.78493224e-02, -4.15142160e-03,\n",
       "         -1.83324870e-02,  5.08360863e-02,  1.46790976e-02, -2.49983128e-02,\n",
       "          1.02184657e-02,  6.71388907e-03,  1.12524470e-02, -4.11708131e-02,\n",
       "          7.19412137e-03, -5.23686036e-02,  2.65588611e-02, -1.32560823e-02,\n",
       "          1.59378850e-03, -1.22296214e-02,  1.42666893e-02,  3.99853289e-03,\n",
       "          1.79872587e-02, -2.94158719e-02, -4.13622297e-02,  2.03281385e-03,\n",
       "         -3.49554792e-02,  1.74515769e-02, -5.28970780e-03,  3.59968729e-02,\n",
       "          1.10466350e-02, -1.49105042e-02, -6.37126807e-03, -1.57796987e-03,\n",
       "         -5.44427820e-02, -4.54458222e-03,  7.73191303e-02,  1.37957167e-02,\n",
       "         -9.09710526e-02,  1.17141465e-02,  3.27792279e-02, -3.07665188e-02,\n",
       "          1.58789344e-02,  3.59490365e-02,  2.66215280e-02, -4.37177084e-02,\n",
       "         -1.89996902e-02, -2.17266548e-02, -8.04822296e-02,  2.11511161e-02,\n",
       "          3.66295949e-02, -1.30184712e-02, -4.57916781e-02, -1.59953851e-02,\n",
       "         -7.60902017e-02, -4.00880724e-02,  1.73355974e-02, -3.66452895e-02,\n",
       "         -2.45765094e-02, -1.56280380e-02, -2.52463244e-04,  5.95930554e-02,\n",
       "         -1.06192250e-02, -2.95892376e-02, -4.93323356e-02,  2.86767688e-02,\n",
       "         -1.11582186e-02,  5.13778329e-02,  1.70713980e-02,  3.45215923e-03,\n",
       "         -3.27817239e-02, -3.20797563e-02,  2.04213639e-03, -1.87060293e-02,\n",
       "         -3.64281386e-02,  6.54129386e-02, -3.63600217e-02,  3.27875726e-02,\n",
       "         -1.48602435e-02, -1.89695619e-02,  1.52493024e-03,  1.14610679e-02,\n",
       "          1.57613661e-02, -3.80379185e-02,  3.20709385e-02, -1.41023109e-02,\n",
       "          2.09537596e-02,  3.57488878e-02, -4.32596467e-02,  1.30480528e-02,\n",
       "          1.04387309e-02,  1.92898400e-02, -3.74048166e-02, -2.88344529e-02,\n",
       "          1.97581146e-02, -7.65801892e-02, -3.39821284e-03, -2.34362315e-02,\n",
       "          2.51190159e-02,  5.86557575e-02,  5.68514504e-02, -3.65888118e-03,\n",
       "          3.56600769e-02, -1.36747235e-03, -2.62164902e-02,  1.48471566e-02,\n",
       "          5.33428304e-02, -1.95979252e-02, -1.60312690e-02,  3.93774770e-02,\n",
       "          9.68534593e-03, -2.99094245e-02,  4.44877520e-02, -6.29002303e-02,\n",
       "          1.68430880e-02,  1.44336214e-02, -1.60067454e-02,  1.26356604e-02,\n",
       "         -3.02589033e-02,  3.09738405e-02, -1.10736676e-03, -6.99319504e-03,\n",
       "          8.85179080e-03,  3.29983272e-02,  1.68417569e-03,  2.96987742e-02,\n",
       "          1.67682059e-02, -2.37614959e-02, -4.03119549e-02,  4.76339459e-02,\n",
       "          3.13352281e-03, -1.45251183e-02,  3.62176597e-02, -2.23984830e-02,\n",
       "          1.60399321e-02, -3.90279032e-02,  8.72683674e-02, -4.71316874e-02,\n",
       "          1.86572745e-02, -3.29413153e-02,  3.42227295e-02, -1.83839239e-02,\n",
       "          3.80204879e-02, -2.20008548e-02, -4.75773327e-02, -2.47929431e-02,\n",
       "         -5.87277375e-02,  4.44272114e-03, -4.24048677e-02,  2.05720700e-02,\n",
       "          5.66756539e-02, -4.22335938e-02, -3.61834504e-02, -6.27410505e-03,\n",
       "         -2.07653106e-03, -5.81650101e-02,  5.30822761e-02, -1.08183352e-02,\n",
       "         -5.05796969e-02, -3.56926136e-02,  2.29178499e-02,  1.04900561e-02,\n",
       "          5.84041374e-03,  5.78328259e-02,  4.59223315e-02,  6.27799542e-04,\n",
       "          2.33728569e-02,  6.83701262e-02, -2.68041566e-02, -2.59634126e-02,\n",
       "          4.35896441e-02, -4.45600860e-02,  9.36272591e-02, -4.62878309e-02,\n",
       "          5.78307845e-02,  4.58302945e-02,  3.77632231e-02,  2.43191198e-02,\n",
       "         -1.99126881e-02,  2.14270651e-02, -5.15770353e-02,  6.15242533e-02,\n",
       "         -1.50076644e-02,  3.87011506e-02, -3.16223800e-02, -6.02132864e-02,\n",
       "         -2.62027811e-02,  1.38911428e-02,  1.67342182e-02,  2.15060525e-02,\n",
       "         -2.70016193e-02, -4.38030809e-02,  8.36000894e-04, -6.54828548e-03,\n",
       "         -9.64822900e-03, -4.87217493e-02, -3.84900183e-03,  1.84892081e-02]),\n",
       "  'score': tensor(0.0760)},\n",
       " {'page_number': 84,\n",
       "  'sentence_chunk': '“Now, form a line,” Professor McGonagall told the first years, “and follow me.”   Feeling oddly as though his legs had turned to lead, Harry got into line behind a boy with sandy hair, with Ron behind him, and they walked out of the chamber, back across the hall, and through a pair of double doors into the Great Hall.   Harry had never even imagined such a strange and splendid place. It was lit by thousands and thousands of candles that were floating in midair over four long tables, where the rest of the students were sitting. These tables were laid with glittering golden plates and goblets. At the top of the hall was another long table where the teachers were sitting. Professor McGonagall led the first years up here, so that they came to a halt in a line facing the other students, with the teachers behind them. The hundreds of faces staring at them looked like pale lanterns in the flickering candlelight. Dotted here and there among the students, the ghosts shone misty silver. Mainly to avoid all the staring eyes, Harry looked upward and saw a velvety black ceiling dotted with stars. He heard Hermione whisper, “Its bewitched to look like the sky outside. I read about it in Hogwarts, A History.”   It was hard to believe there was a ceiling there at all, and that the Great Hall didn’t simply open on to the heavens.   Harry quickly looked down again as Professor McGonagall silently placed a four-legged stool in front of the first years. On top of the stool she put a pointed wizard’s hat. This hat was patched and frayed and extremely dirty. Aunt Petunia wouldn’t have let it in the house.    Maybe they had to try and get a rabbit out of it, Harry thought wildly, that seemed the sort of thing — noticing that everyone in the hall was now staring at the hat, he stared at it, too. For a few seconds, there was complete silence. Then the hat twitched.',\n",
       "  'chunk_char_count': 1872,\n",
       "  'chunk_word_count': 348,\n",
       "  'chunk_token_count': 468.0,\n",
       "  'embedding': array([-1.47205619e-02, -7.53704086e-02,  2.55700182e-02, -5.55467680e-02,\n",
       "          1.08529925e-01,  2.44092550e-02,  1.56287104e-02,  3.17750759e-02,\n",
       "         -1.72939524e-02, -1.65520627e-02,  3.18759494e-02,  4.53891158e-02,\n",
       "         -1.52637409e-02, -1.36077814e-02, -6.11184537e-03, -4.53448109e-02,\n",
       "         -6.61773682e-02,  6.66107750e-03, -3.79590765e-02, -3.24621722e-02,\n",
       "          1.70332547e-02, -5.48684895e-02, -4.76256013e-02, -4.90779690e-02,\n",
       "         -4.17220481e-02,  1.62467733e-02,  3.09468247e-02,  3.45846899e-02,\n",
       "         -3.25666927e-02, -4.47428878e-03, -1.65242385e-02,  3.94440861e-03,\n",
       "          6.61787204e-03,  2.80530620e-02,  5.97325945e-03, -7.61188334e-03,\n",
       "          6.93159699e-02,  2.00202735e-03,  5.95193394e-02, -1.04857720e-02,\n",
       "         -5.82915582e-02, -3.36549692e-02, -1.93778845e-03, -9.33787692e-03,\n",
       "          2.50545912e-03, -2.22303648e-03, -5.75232059e-02,  1.06331021e-01,\n",
       "          4.04535085e-02, -2.97604571e-03,  1.49839688e-02,  1.92161240e-02,\n",
       "          5.26957592e-05,  5.64105213e-02, -1.77061409e-02,  4.86631617e-02,\n",
       "          3.97938788e-02, -3.74318399e-02,  1.61606204e-02, -3.88930216e-02,\n",
       "          5.44317476e-02, -4.69524562e-02,  1.63633772e-03,  1.28801018e-02,\n",
       "         -3.02216578e-02, -6.11170707e-03, -3.54804881e-02,  8.17085244e-03,\n",
       "         -3.92428897e-02,  2.53566215e-03,  2.12988816e-02, -5.49612418e-02,\n",
       "          4.82438318e-03,  4.77447286e-02, -4.60927524e-02,  2.88938582e-02,\n",
       "          3.12726945e-02,  4.39172760e-02,  2.98007987e-02, -3.98028549e-03,\n",
       "         -1.41008692e-02,  1.44420620e-02, -4.75963810e-03, -3.38594206e-02,\n",
       "          4.17771861e-02, -3.47679518e-02,  8.02663062e-03,  2.58571710e-02,\n",
       "         -1.27194049e-02,  1.20189087e-02,  1.30345337e-02,  9.69588012e-02,\n",
       "          1.73099842e-02,  7.97879696e-02,  1.05768128e-03,  3.99078429e-02,\n",
       "         -3.13243195e-02,  8.52161795e-02, -2.14680843e-02, -6.03363290e-03,\n",
       "          1.61827896e-02, -2.56966501e-02, -4.10271026e-02, -1.33231487e-02,\n",
       "          3.58193368e-02,  2.15152577e-02, -1.49086025e-02, -7.16711162e-03,\n",
       "         -7.55183306e-03, -9.41566471e-03, -4.31111865e-02,  2.69367285e-02,\n",
       "         -4.30840515e-02, -1.37324131e-03,  2.77035665e-02, -2.09293179e-02,\n",
       "          7.68356724e-03, -5.81253972e-03,  6.48909155e-03, -4.06890698e-02,\n",
       "          2.01240070e-02, -5.33836707e-02,  5.11440374e-02,  1.50894690e-02,\n",
       "         -1.97206195e-02, -2.13716924e-02,  4.52718735e-02, -1.56969130e-02,\n",
       "         -9.07524116e-03,  2.76340637e-02, -4.06008214e-02,  2.15478148e-03,\n",
       "          2.99062282e-02, -7.20368698e-02, -5.15501760e-02,  1.11595523e-02,\n",
       "          6.44879118e-02,  6.80314153e-02,  2.91342661e-03,  2.83776056e-02,\n",
       "          1.36337252e-02,  8.01582858e-02, -2.45905388e-02,  4.64061159e-04,\n",
       "         -2.70710867e-02, -1.30770672e-02,  2.62792711e-03,  1.37075000e-02,\n",
       "          1.19327791e-02,  5.85376471e-02,  3.36558335e-02, -4.59066480e-02,\n",
       "          4.03259322e-03, -1.64767820e-02,  2.37049931e-03,  6.21718448e-03,\n",
       "          8.77723855e-04, -2.25837231e-02, -2.18882654e-02,  1.24266036e-02,\n",
       "          1.40859317e-02,  1.91713795e-02,  1.40163079e-02,  1.10339494e-02,\n",
       "         -5.47026619e-02,  1.37253664e-02,  5.82832564e-03,  9.80594382e-03,\n",
       "         -3.69719323e-03, -6.69719949e-02, -1.94554962e-02,  1.56990550e-02,\n",
       "          4.81577478e-02,  3.90832983e-02, -1.48700085e-02,  2.90396530e-02,\n",
       "         -4.93712500e-02, -1.72743201e-02,  1.52362604e-02,  1.99916828e-02,\n",
       "         -2.93487478e-02,  7.01023359e-03,  6.36246754e-03, -7.73980794e-03,\n",
       "          5.16349263e-02,  1.03165284e-02, -1.31938104e-02, -1.62255950e-02,\n",
       "         -5.49024865e-02, -8.80227983e-03,  6.65175216e-03,  1.30015356e-03,\n",
       "          4.35451791e-02,  3.68036292e-02,  5.03338128e-03,  5.78417536e-03,\n",
       "         -2.96310429e-02,  2.86010373e-02,  3.58061939e-02, -3.06345001e-02,\n",
       "          2.44791433e-03, -4.77948452e-05, -5.54753328e-03, -1.02569342e-01,\n",
       "          2.90102996e-02, -5.68077080e-02,  9.49399080e-03, -6.16822811e-03,\n",
       "         -5.10627357e-03,  8.95079896e-02, -2.26279646e-02,  3.40786204e-02,\n",
       "         -2.30867751e-02,  1.35517195e-02,  4.38168049e-02, -7.27870092e-02,\n",
       "         -7.94628228e-04, -5.01015410e-02,  5.54830469e-02,  2.62440369e-03,\n",
       "          1.67203825e-02,  1.95797579e-03, -1.93851441e-02,  2.08076984e-02,\n",
       "         -3.20800915e-02, -8.26791152e-02,  1.67944748e-02, -5.08702621e-02,\n",
       "         -3.21021266e-02,  5.51820770e-02, -1.21022100e-02,  4.75513414e-02,\n",
       "         -5.79800531e-02, -6.06814260e-03, -2.18313280e-02, -3.35325114e-02,\n",
       "         -3.69681567e-02,  1.49313109e-02, -7.49623775e-03,  3.51505242e-02,\n",
       "          2.13362207e-03,  1.32955462e-02,  2.64970083e-02,  3.33780721e-02,\n",
       "         -3.02479174e-02, -7.80876502e-02,  7.14052748e-03, -3.27346064e-02,\n",
       "          6.91373553e-03, -6.36563525e-02,  4.19508442e-02, -7.08393101e-03,\n",
       "          1.83122786e-04, -6.51116148e-02, -2.24846937e-02,  6.29176572e-03,\n",
       "          1.42294075e-02, -1.64717156e-02, -3.16984244e-02,  6.72356486e-02,\n",
       "          1.00913327e-02, -3.55503000e-02, -2.09646509e-03, -2.34670043e-02,\n",
       "         -4.31664959e-02, -1.93485674e-02, -2.15393528e-02, -2.00541746e-02,\n",
       "         -3.84980664e-02,  3.23078111e-02, -1.21678691e-02, -2.93535851e-02,\n",
       "          5.06378561e-02,  2.52528731e-02, -5.13160182e-03,  7.55435135e-03,\n",
       "         -4.08511385e-02,  3.73049267e-02,  1.90707259e-02, -2.40505878e-02,\n",
       "          1.04444912e-02, -7.22064152e-02, -6.40533194e-02, -4.99157719e-02,\n",
       "         -1.15994858e-02, -2.69183423e-02,  1.86811406e-02,  2.24980600e-02,\n",
       "         -2.65949517e-02,  3.80917527e-02, -2.93937568e-02,  4.09887508e-02,\n",
       "          8.43025818e-02,  1.19441313e-04, -2.37643346e-02, -2.85575036e-02,\n",
       "         -3.46881733e-03, -5.99980652e-02,  2.43462939e-02, -1.15729468e-02,\n",
       "         -3.47250327e-02, -2.68098265e-02, -5.53991459e-02,  8.37946013e-02,\n",
       "          1.96653139e-02,  9.98408347e-03, -2.29854155e-02,  1.41120376e-02,\n",
       "          5.64343855e-03,  4.69264016e-02, -1.97062045e-02, -7.16128619e-03,\n",
       "          2.26879176e-02, -4.49695289e-02, -6.17145002e-02,  2.86280829e-02,\n",
       "         -1.35802040e-02, -3.97077873e-02, -2.19067791e-03,  7.89687559e-02,\n",
       "         -5.03933392e-02, -5.59447557e-02, -1.37811787e-02,  1.30573695e-03,\n",
       "          8.77862703e-03, -2.83094514e-02, -1.48679689e-02, -3.55036487e-03,\n",
       "         -1.20782312e-02, -8.18280783e-03, -8.90400335e-02,  3.52470167e-02,\n",
       "          1.92783345e-02, -3.36546302e-02, -8.83201323e-03, -1.01011116e-02,\n",
       "          2.66710129e-02, -2.75612902e-02, -1.04738316e-02,  6.36732057e-02,\n",
       "         -7.50155076e-02,  2.77107004e-02, -7.70283043e-02,  1.12128397e-02,\n",
       "          2.25821659e-02, -3.83065343e-02, -1.20940479e-02,  8.40173010e-03,\n",
       "          7.76484469e-03,  1.51491687e-02, -1.90971680e-02,  1.45464521e-02,\n",
       "         -1.29765123e-02,  3.21372114e-02, -5.74368052e-02,  2.77845841e-02,\n",
       "         -7.35755544e-03, -1.94074661e-02, -1.29457405e-02, -3.29076033e-03,\n",
       "         -2.20972616e-02,  7.02288970e-02,  4.76603620e-02, -3.81048094e-03,\n",
       "         -2.72410046e-02, -7.41198435e-02, -1.41649535e-02,  7.26802498e-02,\n",
       "          2.69983709e-02,  1.50122950e-02, -1.37710245e-02,  1.53881116e-02,\n",
       "          9.20163561e-03,  1.93960089e-02, -4.79491800e-03, -2.80706980e-03,\n",
       "         -3.42636183e-02, -7.89243057e-02,  2.90483553e-02, -1.13367038e-02,\n",
       "          6.73680659e-03, -2.81638484e-02, -3.86948250e-02, -2.03270447e-02,\n",
       "         -7.52440235e-03,  5.05520962e-02, -5.01211472e-02, -5.39930686e-02,\n",
       "          1.66255794e-02, -1.92714913e-03, -7.26606743e-03, -6.16676584e-02,\n",
       "         -1.03164941e-01,  1.14219952e-02,  2.18941458e-03,  5.58675081e-02,\n",
       "          7.66765559e-03, -3.81878354e-02, -9.48574394e-03,  3.78051698e-02,\n",
       "          3.35062370e-02, -1.62839014e-02, -1.79346427e-02, -9.45309084e-03,\n",
       "         -4.03686687e-02, -1.76163539e-02,  1.98062174e-02, -2.65896302e-02,\n",
       "         -1.02471770e-03, -6.34919330e-02, -3.60981151e-02,  9.00166854e-02,\n",
       "         -3.16346996e-02,  8.28830525e-03,  2.97799241e-02,  1.46266995e-02,\n",
       "         -3.02714743e-02,  1.71749406e-02,  7.38380570e-03, -1.38063217e-02,\n",
       "          5.94079262e-03,  1.77037977e-02, -3.09920348e-02,  2.30013225e-02,\n",
       "          1.15268268e-02,  3.75130912e-03,  1.51787112e-02, -7.47048631e-02,\n",
       "          1.82620343e-02, -6.57539889e-02, -5.59127033e-02, -2.28219423e-02,\n",
       "          5.65941148e-02, -2.13084146e-02, -1.38762891e-02, -3.62566370e-03,\n",
       "          5.72506227e-02, -1.08726472e-02,  2.34402921e-02, -2.12383159e-02,\n",
       "         -9.18868780e-02,  3.57728601e-02,  3.97855453e-02, -5.23764491e-02,\n",
       "         -8.23888555e-03,  4.55032885e-02, -3.35416347e-02,  2.09880359e-02,\n",
       "         -2.69649327e-02, -9.58488556e-04, -1.16431698e-01, -2.53495779e-02,\n",
       "         -4.35218327e-02, -8.50215331e-02, -6.46909922e-02,  9.38229263e-03,\n",
       "          5.04266750e-03, -4.66925278e-02,  3.55384052e-02,  1.07074287e-02,\n",
       "         -4.23506424e-02,  5.44529548e-03,  9.83238127e-03,  6.78742751e-02,\n",
       "          4.20533232e-02, -1.68024302e-02, -1.95214991e-02, -2.07029637e-02,\n",
       "          2.27327645e-02,  4.46047336e-02,  1.19288452e-02,  2.74819899e-02,\n",
       "         -3.40156518e-02,  2.18965067e-03,  2.03793291e-02,  3.57003957e-02,\n",
       "         -1.11996909e-04,  1.84978768e-02, -1.35588925e-02,  1.67360585e-02,\n",
       "         -3.22925393e-03, -1.03647041e-03,  9.70229972e-03, -3.01250722e-02,\n",
       "         -6.27110228e-02, -1.81119097e-03, -5.90719134e-02,  7.58715943e-02,\n",
       "         -5.50022647e-02,  1.95839219e-02,  4.58109602e-02,  4.03495366e-03,\n",
       "         -2.42907461e-02,  2.71967836e-02,  3.94773027e-33, -6.52730279e-03,\n",
       "         -3.86314206e-02,  1.77542660e-02, -5.87586546e-03, -3.11557278e-02,\n",
       "         -1.62800476e-02, -1.47845158e-02,  2.13567447e-02, -4.53619112e-04,\n",
       "         -2.42523756e-02, -9.29915160e-03,  4.28468082e-03, -2.25933865e-02,\n",
       "          2.24796068e-02, -1.58601198e-02,  2.61240918e-03,  1.22055598e-01,\n",
       "          7.75612369e-02, -1.98403560e-02, -1.33926123e-02,  3.17067169e-02,\n",
       "          7.56031135e-03, -3.19645368e-02,  2.40659937e-02,  6.01863936e-02,\n",
       "          2.59163161e-03,  9.25161690e-03, -3.84000055e-02,  9.04128607e-03,\n",
       "          1.78925712e-02,  3.02944053e-02, -1.97266489e-02,  4.07134034e-02,\n",
       "         -4.42066565e-02, -8.12368467e-02, -2.03229059e-02,  1.45282904e-02,\n",
       "          1.59238540e-02, -2.91468576e-02,  4.15843502e-02, -2.19297409e-03,\n",
       "          3.83754261e-02,  4.98957280e-03,  8.38954672e-02, -2.18147468e-02,\n",
       "         -8.93613789e-03, -2.94350199e-02, -5.83273172e-03,  2.05957089e-02,\n",
       "         -1.46819642e-02,  1.89720746e-02,  7.53335059e-02,  7.86350388e-03,\n",
       "         -5.59128597e-02, -2.87334565e-02,  3.05122063e-02,  1.00819759e-01,\n",
       "         -7.21206218e-02,  5.72681539e-02,  4.89898026e-03,  9.76786856e-03,\n",
       "          2.12337431e-02, -2.16973051e-02,  3.09204664e-02, -3.11727244e-02,\n",
       "          8.56510736e-03, -8.99216160e-03,  1.67636257e-02,  5.26443543e-03,\n",
       "          4.36770730e-02, -2.45741699e-02,  1.92594025e-02,  5.01145683e-02,\n",
       "         -7.78890215e-03,  4.51789331e-03,  4.41831276e-02,  1.57858413e-02,\n",
       "         -1.19648911e-02, -8.37780349e-03,  1.10897580e-02, -9.90871433e-03,\n",
       "          1.44584232e-03,  1.21314973e-01,  9.68114566e-03,  6.10175636e-03,\n",
       "         -1.96760092e-02,  4.88476921e-03,  5.75305615e-03, -3.29899192e-02,\n",
       "         -6.74204668e-03, -2.56004203e-02,  1.47410110e-02,  2.76037324e-02,\n",
       "          2.56537297e-03, -1.37348399e-02, -1.20805930e-02,  2.20026746e-02,\n",
       "         -2.42690425e-02, -3.22192274e-02,  3.97069044e-02,  3.70842554e-02,\n",
       "         -2.62235850e-02,  5.03152190e-03, -2.78131124e-02,  2.93679186e-03,\n",
       "          3.60623784e-02, -4.80429530e-02,  9.57561098e-03, -1.35093294e-02,\n",
       "          8.94883182e-03,  1.55811990e-02,  4.64091375e-02,  2.01444719e-02,\n",
       "         -1.23540863e-01,  2.08665915e-02,  2.04037176e-03,  1.42604867e-02,\n",
       "         -3.51074189e-02,  4.34796475e-02, -4.25366545e-03, -1.49250207e-02,\n",
       "         -3.07779107e-02, -5.79530597e-02, -1.43095907e-02,  1.19526736e-01,\n",
       "          5.00038937e-02,  6.44414593e-03,  2.20223963e-02,  5.50380116e-03,\n",
       "         -9.15595144e-02,  4.57806662e-02,  7.80060962e-02,  1.86409745e-02,\n",
       "         -7.79800396e-03, -4.52017821e-02,  2.83206366e-02,  3.10900453e-02,\n",
       "          1.16998237e-02, -6.05846494e-02,  8.50736909e-03,  2.20422782e-02,\n",
       "          9.30209737e-03,  4.44871373e-02, -3.31420830e-04, -3.89374234e-02,\n",
       "         -1.01962071e-02, -2.07956061e-02,  2.80885235e-03, -1.58673301e-02,\n",
       "         -1.07851345e-03, -1.38627309e-02, -1.77137032e-02,  8.15734174e-03,\n",
       "         -3.45983170e-02, -9.44257621e-03,  2.11112667e-02,  1.39717674e-02,\n",
       "         -2.96689384e-02, -7.68258870e-02,  4.10916321e-02, -7.21882377e-03,\n",
       "          1.39840990e-02,  1.89481154e-02, -1.90447196e-02,  3.12060378e-02,\n",
       "          1.71786435e-02,  2.91981851e-03,  1.65404174e-02, -1.75428204e-02,\n",
       "          2.02039927e-02, -1.14545813e-02, -7.23480619e-03,  1.75058190e-03,\n",
       "          2.23795883e-02,  2.74931379e-02,  1.66084021e-02, -1.23066185e-02,\n",
       "         -1.45210448e-04,  1.75362900e-02, -7.39787668e-02,  2.74325982e-02,\n",
       "          2.20293086e-02,  3.71106900e-02,  1.85609758e-02,  5.21144457e-02,\n",
       "          9.08903871e-03, -5.99096855e-03,  3.68904844e-02, -8.74640867e-02,\n",
       "          1.07076690e-02,  2.51885168e-02,  2.93225851e-02,  4.00630608e-02,\n",
       "         -2.53077392e-02,  2.69101616e-02, -6.68316474e-03, -7.19373254e-03,\n",
       "         -2.24154703e-02, -1.25476038e-02, -2.33762190e-02,  8.42497721e-02,\n",
       "          5.78551833e-03, -1.86252799e-02,  1.08460858e-02,  2.51995381e-02,\n",
       "         -5.27309394e-03, -7.03451969e-03, -3.18039507e-02, -1.26004107e-02,\n",
       "          8.78584944e-03,  1.58332719e-03,  4.40661572e-02, -7.92572275e-02,\n",
       "          1.23640057e-02, -6.84440583e-02, -5.18655330e-02,  5.25979698e-03,\n",
       "          2.43716035e-02, -2.66908668e-02, -7.43119791e-03, -1.98214520e-02,\n",
       "          1.88722033e-02,  2.03857804e-03, -1.06520311e-03,  5.13304286e-02,\n",
       "          7.39802942e-02,  3.41456966e-03,  5.95664093e-03, -6.28377777e-04,\n",
       "          6.11584485e-02, -6.03000447e-02,  7.97626972e-02,  2.90935431e-02,\n",
       "         -6.78102858e-03, -6.26912937e-02,  4.14333902e-02,  5.08396290e-02,\n",
       "         -1.40372664e-02,  6.55466095e-02, -1.14745158e-03,  2.34877653e-02,\n",
       "          2.60416195e-02,  3.88831720e-02, -9.15896706e-03, -6.25186563e-02,\n",
       "          1.28362486e-02,  1.55484881e-02,  5.60791492e-02,  1.47184376e-02,\n",
       "          1.32710524e-02,  3.44276056e-03, -1.08754588e-02,  1.98081648e-03,\n",
       "          7.94811267e-03,  1.22806616e-02, -6.02197014e-02,  7.14112371e-02,\n",
       "         -5.05885631e-02,  7.78919179e-03,  3.17381136e-02, -4.84060869e-02,\n",
       "         -2.35366612e-03, -2.89996993e-03, -3.33022363e-02,  1.11134164e-03,\n",
       "         -5.42590246e-02, -3.32814306e-02, -9.65250060e-02,  2.69264020e-02,\n",
       "         -3.60827707e-02,  9.33739822e-04,  1.69948339e-02, -4.01723981e-02]),\n",
       "  'score': tensor(0.0666)}]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.3,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what made harry frightened?\n",
      "[INFO] Time taken to get scores on 232 embeddings: 0.00009 seconds.\n",
      "Answer:\n",
      "\n",
      "According to the passage, Harry was frightened because he saw his uncle lying at\n",
      "the foot of the front door in a sleeping bag, clearly making sure that Harry\n",
      "didn't do exactly what he'd been trying to do.\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 33,\n",
       "  'sentence_chunk': 'After what seemed like hours they reached the rock, where Uncle Vernon, slipping and sliding, led the way to the broken-down house.   The inside was horrible; it smelled strongly of seaweed, the wind whistled through the gaps in the wooden walls, and the fireplace was damp and empty. There were only two rooms.   Uncle Vernon’s rations turned out to be a bag of chips each and four bananas. He tried to start a fire but the empty chip bags just smoked and shriveled up.   “Could do with some of those letters now, eh?”he said cheerfully.   He was in a very good mood. Obviously he thought nobody stood a chance of reaching them here in a storm to deliver mail. Harry privately agreed, though the thought didn’t cheer him up at all.   As night fell, the promised storm blew up around them. Spray from the high waves splattered the walls of the hut and a fierce wind rattled the filthy windows. Aunt Petunia found a few moldy blankets in the second room and made up a bed for Dudley on the moth-eaten sofa. She and Uncle Vernon went off to the lumpy bed next door, and Harry was left to find the softest bit of floor he could and to curl up under the thinnest, most ragged blanket.   The storm raged more and more ferociously as the night went on. Harry couldn’t sleep. He shivered and turned over, trying to get comfortable, his stomach rumbling with hunger. Dudley’s snores were drowned by the low rolls',\n",
       "  'chunk_char_count': 1404,\n",
       "  'chunk_word_count': 266,\n",
       "  'chunk_token_count': 351.0,\n",
       "  'embedding': array([-4.13232902e-03, -5.26856072e-02,  3.08595020e-02,  1.64366011e-02,\n",
       "          4.69253771e-02,  4.00580429e-02, -1.42691075e-03,  2.07580011e-02,\n",
       "          1.65706705e-02,  1.75059400e-02, -1.90336257e-02, -3.98144266e-03,\n",
       "         -1.97892003e-02,  2.22806330e-03, -1.56133883e-02, -2.25855522e-02,\n",
       "         -7.62222754e-03, -4.55011614e-02, -5.35014719e-02, -2.25445572e-02,\n",
       "          4.18849625e-02, -3.18862647e-02,  4.09064963e-02, -1.80285443e-02,\n",
       "         -7.28200236e-03, -2.59032752e-02,  3.30002233e-02,  1.43521326e-02,\n",
       "         -5.48705123e-02,  1.04089202e-02, -3.07093728e-02, -3.42610478e-02,\n",
       "          3.23290154e-02,  1.96294431e-02,  9.23311058e-03,  1.14925206e-02,\n",
       "          2.44957414e-02,  1.41649079e-02,  4.21821140e-02, -2.91605033e-02,\n",
       "         -3.52991484e-02, -9.21322219e-03,  1.22016212e-02,  1.21213142e-02,\n",
       "          3.64410505e-02, -1.58046056e-02, -3.44485790e-02,  4.51214723e-02,\n",
       "          1.52290370e-02,  1.01029482e-02,  4.28195596e-02,  3.52459215e-02,\n",
       "         -3.18172872e-02,  6.40698671e-02,  1.21984193e-02,  2.80647981e-03,\n",
       "          2.12796647e-02,  4.32549641e-02, -3.64730060e-02, -4.12349701e-02,\n",
       "          1.29502684e-01, -8.80212616e-03, -5.40167501e-04,  6.27772436e-02,\n",
       "          4.05644216e-02, -1.65321566e-02, -1.06105655e-02,  1.10433279e-02,\n",
       "         -5.15163876e-02,  5.85855963e-03, -8.38539936e-03, -2.84273569e-02,\n",
       "         -9.78676602e-03, -1.28651280e-02, -2.30502728e-02, -1.71403214e-02,\n",
       "          1.28776869e-02,  5.55895502e-03,  1.06433388e-02,  5.62370429e-03,\n",
       "          9.59136058e-03,  9.73465592e-02,  6.10576803e-03,  2.27931025e-03,\n",
       "          3.92921977e-02,  3.38322110e-02,  8.93121399e-03, -4.18332517e-02,\n",
       "          1.15054911e-02, -8.31817510e-04,  4.89084013e-02,  8.72860700e-02,\n",
       "          6.79972582e-03,  1.04110517e-01, -1.84928197e-02, -1.19922135e-03,\n",
       "         -5.04258983e-02,  6.86587989e-02, -1.07101994e-04,  9.37242433e-03,\n",
       "          1.78503282e-02, -3.34886573e-02, -6.95898682e-02,  7.07149832e-03,\n",
       "          1.27498526e-02, -4.63571539e-03, -1.40349043e-03, -2.54415814e-02,\n",
       "         -4.35630754e-02, -1.98781379e-02, -1.65692661e-02,  1.88542146e-03,\n",
       "          1.15123792e-02,  2.84372140e-02,  2.36486308e-02, -2.40736101e-02,\n",
       "          2.63923593e-02, -2.03054640e-02,  1.31133823e-02,  1.88903641e-02,\n",
       "          4.72184680e-02, -2.65197363e-02,  3.53375860e-02,  6.50978684e-02,\n",
       "          1.73090193e-02,  9.70031619e-02,  4.22190651e-02, -4.88221385e-02,\n",
       "         -1.56086739e-02,  3.15591432e-02, -8.76481310e-02,  2.16906667e-02,\n",
       "          4.86550704e-02, -6.32523969e-02, -5.46199381e-02,  2.15717070e-02,\n",
       "          1.88572351e-02,  4.24362049e-02,  2.54888227e-03,  2.21068915e-02,\n",
       "          1.14145596e-03,  4.37048115e-02, -9.44632478e-03,  5.00879297e-03,\n",
       "         -3.75611074e-02, -6.72223791e-03, -4.08119522e-03,  1.18900910e-02,\n",
       "         -5.49739599e-02,  8.32586288e-02,  5.55424243e-02,  2.25027446e-02,\n",
       "         -4.67431694e-02, -5.05152233e-02, -1.07524311e-02, -5.16084861e-03,\n",
       "         -1.18591646e-02, -9.34015494e-03, -1.80123351e-03,  2.63767876e-02,\n",
       "          2.81064189e-03,  5.07131144e-02,  2.87819728e-02, -1.96568538e-02,\n",
       "         -3.02546583e-02,  1.44532407e-02,  4.00284007e-02,  1.94739867e-02,\n",
       "          4.66844402e-02, -4.61033992e-02,  2.89772102e-03, -2.49803364e-02,\n",
       "          1.10741723e-02,  3.59533913e-02, -5.12460060e-02,  1.82862754e-03,\n",
       "          1.61280539e-02, -3.59541401e-02,  2.77506355e-02,  3.28682363e-02,\n",
       "         -3.64269316e-02,  2.88984533e-02,  1.37088671e-02,  6.38443325e-03,\n",
       "          2.58131027e-02,  5.93338273e-02, -9.19057708e-03, -1.21378470e-02,\n",
       "         -7.48341084e-02,  2.36565843e-02, -2.55238861e-02,  2.72069462e-02,\n",
       "          4.47153784e-02,  3.31539214e-02,  8.85687675e-03,  5.73319420e-02,\n",
       "          2.59128511e-02,  1.73939317e-02,  2.95132529e-02, -4.39530388e-02,\n",
       "          3.57362349e-03, -5.23593388e-02, -6.36876468e-03, -5.84955476e-02,\n",
       "         -9.26610976e-02, -1.85058992e-02,  4.40677803e-04,  2.40006130e-02,\n",
       "         -3.28655988e-02,  4.56981957e-02, -2.42811479e-02, -4.99721020e-02,\n",
       "         -2.75023077e-02,  2.12856736e-02,  1.59793012e-02, -1.68840829e-02,\n",
       "          5.44704236e-02, -1.00093260e-02,  7.86297396e-03,  8.07394797e-04,\n",
       "         -9.21528228e-03, -3.02070635e-03,  6.18719030e-03, -5.02287084e-03,\n",
       "         -5.73766492e-02, -8.40242878e-02, -9.02783405e-03,  1.47709502e-02,\n",
       "         -3.31966206e-02,  4.65306640e-02, -4.75549474e-02, -4.40012012e-03,\n",
       "         -7.78414235e-02, -7.15027004e-02, -2.70386580e-02, -7.65879080e-02,\n",
       "          2.19148264e-04, -3.00865993e-02,  2.18251720e-02,  3.45955379e-02,\n",
       "         -9.44687333e-03, -9.48018767e-03,  2.43652165e-02,  1.09271631e-02,\n",
       "         -1.35977063e-02, -9.88548342e-03,  2.10529212e-02,  4.16429155e-02,\n",
       "          4.80194949e-02, -4.16600751e-03,  2.19301996e-03, -1.26677267e-02,\n",
       "         -9.55574494e-03, -1.81069970e-02,  3.58372703e-02, -2.07007825e-02,\n",
       "          4.16671522e-02,  1.67044979e-02,  3.51407714e-02,  5.34605086e-02,\n",
       "         -8.26076197e-04, -1.67446341e-02, -7.68106477e-03,  2.09943540e-02,\n",
       "         -2.06818022e-02,  4.66903392e-03, -4.12127301e-02,  2.99510639e-02,\n",
       "         -7.12755769e-02, -1.19716981e-02, -1.71795078e-02, -1.62094124e-02,\n",
       "          2.42372584e-02,  5.80835119e-02,  9.23483819e-03,  3.35045084e-02,\n",
       "         -3.40523161e-02,  3.90216634e-02, -2.17330866e-02, -6.06778488e-02,\n",
       "         -4.71653091e-03, -1.41099263e-02, -2.89286282e-02, -2.01291889e-02,\n",
       "         -1.62274949e-02,  1.66339315e-02,  4.04197583e-03,  6.70379698e-02,\n",
       "          7.64509384e-03, -2.33321059e-02, -3.92365083e-02,  2.46702787e-03,\n",
       "          6.80445731e-02,  2.64289100e-02,  3.05761467e-03,  6.19441755e-02,\n",
       "         -2.33564060e-02, -8.34793970e-02,  7.62719754e-03, -1.80495530e-02,\n",
       "          4.16462868e-03, -3.65644842e-02, -1.01278462e-01,  3.33744772e-02,\n",
       "          8.95697856e-04,  3.06639113e-02,  5.57590984e-02, -2.37922706e-02,\n",
       "         -8.29893351e-03,  3.91698405e-02, -2.98852697e-02, -2.21965276e-03,\n",
       "          3.22134723e-03,  1.35602830e-02, -3.23242173e-02,  7.43692508e-03,\n",
       "         -3.24170478e-02, -1.62717365e-02,  3.61665115e-02,  1.00345649e-02,\n",
       "         -3.73716541e-02, -6.34140149e-02, -6.33560866e-02, -4.18453058e-03,\n",
       "          2.70241443e-02, -3.67497578e-02,  5.52247241e-02,  8.80274270e-03,\n",
       "         -1.59123577e-02,  2.65289824e-02, -7.40130544e-02, -1.01686514e-03,\n",
       "         -2.03232076e-02,  3.39320884e-03,  2.59169247e-02, -5.75434463e-03,\n",
       "          1.35233570e-02, -2.17638910e-02,  2.76642367e-02,  3.95986922e-02,\n",
       "         -3.61395776e-02, -1.74842756e-02, -1.03035010e-02,  2.27079503e-02,\n",
       "         -1.00416401e-02,  9.76401439e-04, -6.25086576e-02,  5.32042496e-02,\n",
       "         -1.00482432e-02,  9.09902342e-03,  5.35038020e-03,  6.24772208e-03,\n",
       "         -7.48293148e-03,  2.09527537e-02, -2.50338092e-02, -2.21812818e-02,\n",
       "         -3.71459350e-02, -2.47383267e-02, -1.23243853e-02, -2.88655590e-02,\n",
       "         -3.32756490e-02,  6.07883893e-02,  2.27758270e-02, -7.85134919e-03,\n",
       "         -1.04389125e-02, -8.16177130e-02, -4.89750020e-02,  8.28198120e-02,\n",
       "          4.20069136e-03,  2.11662101e-03, -4.56997240e-03,  5.97465113e-02,\n",
       "          5.16540697e-03, -4.05930541e-02, -2.48842053e-02, -6.10351888e-03,\n",
       "         -6.67031035e-02, -2.14019157e-02,  8.22379217e-02, -2.13603918e-02,\n",
       "          1.31642222e-02, -1.93366986e-02, -2.23265328e-02, -4.05974779e-03,\n",
       "          5.78874489e-03,  1.28751677e-02,  9.67279077e-03,  5.20805654e-04,\n",
       "          9.19653177e-02,  3.62963453e-02,  3.93429808e-02, -3.37251909e-02,\n",
       "         -6.43570647e-02, -1.80708990e-02,  2.44556442e-02,  2.81514116e-02,\n",
       "         -5.16536906e-02, -3.26384627e-03,  2.38348339e-02,  2.51506176e-02,\n",
       "          4.58444692e-02,  4.74778600e-02, -1.25654414e-02, -3.26537639e-02,\n",
       "         -6.84103742e-02,  2.40002051e-02,  1.36722010e-02, -1.23716937e-02,\n",
       "         -2.90730596e-02, -1.48775410e-02, -1.30625591e-02,  6.68310001e-02,\n",
       "         -2.27925163e-02, -1.09713271e-01,  2.15358045e-02, -2.83291079e-02,\n",
       "         -2.40833480e-02,  3.26590147e-03,  4.02968414e-02,  2.60824319e-02,\n",
       "          2.79595088e-02,  2.09710430e-02, -7.10211545e-02,  2.00423356e-02,\n",
       "         -2.25847997e-02, -2.30575725e-02, -7.72745162e-03, -7.35339597e-02,\n",
       "         -1.80892963e-02, -7.55347461e-02, -5.59126884e-02, -6.78689312e-03,\n",
       "         -2.01247074e-02, -3.58015783e-02, -6.96045533e-02,  3.51299299e-03,\n",
       "          2.72261575e-02, -4.23333906e-02, -1.57527756e-02,  2.30952874e-02,\n",
       "          3.94509334e-05, -6.41961815e-03,  6.23021238e-02, -8.66640508e-02,\n",
       "         -4.25517820e-02, -6.21574465e-03, -3.12700197e-02,  1.75387319e-02,\n",
       "          2.05427818e-02,  2.73063332e-02, -6.02877326e-02, -2.32865922e-02,\n",
       "          1.75115000e-02, -1.09271362e-01,  1.61868650e-02, -3.21214437e-03,\n",
       "          2.11811475e-02, -3.99593376e-02, -4.41893609e-03,  3.43913436e-02,\n",
       "          1.59835368e-02,  9.70222894e-03, -1.74921788e-02,  4.14313711e-02,\n",
       "          2.42259260e-02, -2.83267349e-02, -6.40875101e-02, -5.13336621e-02,\n",
       "         -4.22702245e-02,  2.33611409e-02,  3.10823042e-02,  5.26460856e-02,\n",
       "         -1.39511134e-02,  1.96365751e-02,  2.47653928e-02,  4.33338284e-02,\n",
       "         -7.60390889e-03,  7.53017841e-03, -2.45164186e-02,  1.75480936e-02,\n",
       "         -3.10309380e-02,  1.92360058e-02, -1.95413381e-02, -1.19868070e-02,\n",
       "         -5.74583821e-02, -1.80527940e-03, -5.48746474e-02,  9.69111696e-02,\n",
       "         -5.48216626e-02, -1.13159446e-02,  1.44350417e-02, -9.88896500e-05,\n",
       "          7.85460416e-03,  2.41722595e-02,  3.81894274e-33, -1.31374048e-02,\n",
       "         -4.46870830e-03,  3.13924514e-02, -1.17548332e-02, -2.61865016e-02,\n",
       "          9.28382576e-03, -1.78383160e-02,  3.03653292e-02,  8.32541659e-03,\n",
       "          1.31684048e-02, -1.76488608e-02,  1.21897459e-02,  1.44067584e-02,\n",
       "          2.32747253e-02,  5.00016985e-03,  5.47549105e-04,  6.95171654e-02,\n",
       "         -1.94729939e-02,  4.36198805e-03, -1.27257081e-02, -5.93469627e-02,\n",
       "          9.64225549e-03, -4.00050059e-02,  8.14703479e-02,  3.18956785e-02,\n",
       "         -1.36183482e-02, -1.58550441e-02, -9.72369313e-03, -2.05036420e-02,\n",
       "          4.87810113e-02,  2.54518073e-02, -2.37572733e-02,  1.76398829e-02,\n",
       "         -4.16887850e-02, -9.73708481e-02, -6.62371367e-02,  5.37960865e-02,\n",
       "         -4.82743577e-04, -9.76548344e-03, -3.72805446e-02,  9.37573239e-03,\n",
       "          5.72180972e-02,  7.67957140e-03, -4.35051415e-03, -7.11693913e-02,\n",
       "          3.00813206e-02,  3.92831489e-02, -8.12897051e-05,  1.87927820e-02,\n",
       "         -1.03722736e-02,  1.85166132e-02,  7.54986852e-02, -2.57111969e-03,\n",
       "         -6.29061311e-02,  9.56183821e-02,  4.96169701e-02,  1.76121339e-01,\n",
       "         -4.92689461e-02,  7.74482116e-02, -5.51275583e-03,  4.06010170e-03,\n",
       "          4.59501483e-02,  1.90605130e-02, -2.20198017e-02, -2.42606178e-02,\n",
       "          3.14037353e-02, -4.21233533e-04, -4.60753031e-03,  5.16473455e-03,\n",
       "          2.26263776e-02,  2.29310468e-02, -3.30548175e-02,  2.24221833e-02,\n",
       "          2.34820060e-02, -2.15405580e-02, -1.27873421e-02, -2.58958470e-02,\n",
       "         -2.11473387e-02,  8.38843957e-02, -4.51535769e-02, -1.64184608e-02,\n",
       "          1.22063691e-02,  1.18978303e-02, -1.76665336e-02, -4.65630218e-02,\n",
       "         -1.39664859e-02,  1.15507385e-02, -1.45973582e-02, -3.50819565e-02,\n",
       "         -6.52750582e-03, -4.84163947e-02, -2.19098423e-02,  2.54555233e-03,\n",
       "          1.94569281e-03, -1.25592621e-02,  4.20156568e-02,  4.11367603e-03,\n",
       "         -1.40038887e-02, -1.60931945e-02, -1.53467208e-02, -4.67786789e-02,\n",
       "         -5.67555986e-02,  6.62292307e-03, -1.26617122e-03,  2.54207128e-03,\n",
       "         -1.97888948e-02, -1.83169544e-02, -6.94308896e-03, -1.24508590e-02,\n",
       "         -3.49422060e-02,  2.35359836e-02,  6.21473975e-02,  3.68818553e-04,\n",
       "         -4.80904952e-02,  6.43368717e-03,  3.98644358e-02, -1.54948244e-02,\n",
       "          3.24325077e-02,  5.17352596e-02,  2.42324453e-02, -4.01078016e-02,\n",
       "         -2.40022950e-02, -2.18624435e-02, -6.42178208e-02,  4.89437804e-02,\n",
       "          5.37148723e-03, -2.84383837e-02,  2.11754534e-02, -1.13188298e-02,\n",
       "         -9.52092409e-02, -4.18434367e-02,  7.65726808e-03, -2.09636912e-02,\n",
       "         -2.30567399e-02, -2.19014343e-02, -1.88980829e-02,  3.12372893e-02,\n",
       "         -5.36467740e-03, -2.41124481e-02, -1.37953153e-02,  1.79037824e-02,\n",
       "         -1.66496495e-03,  3.64992544e-02,  2.29523946e-02,  1.22095840e-02,\n",
       "         -6.27180887e-03, -7.99145103e-02, -3.62917148e-02, -9.25348233e-03,\n",
       "         -2.20017955e-02,  3.61355655e-02, -2.31320076e-02,  3.33286934e-02,\n",
       "         -2.26074588e-02, -3.08445841e-02,  1.19000012e-02, -1.92881674e-02,\n",
       "          1.92754157e-02, -2.69436650e-02, -4.72724554e-04, -3.93819883e-02,\n",
       "         -1.40467985e-02,  4.31378372e-02, -3.56680900e-02, -1.39543377e-02,\n",
       "          3.34798023e-02,  6.81263208e-02, -1.20219430e-02, -2.24518124e-02,\n",
       "          2.84763221e-02, -4.77517061e-02, -8.10497813e-03,  1.02610001e-02,\n",
       "          2.49094293e-02,  5.07580787e-02,  4.14117016e-02, -2.77680326e-02,\n",
       "          5.41230962e-02, -6.01336500e-03, -3.61111984e-02,  5.94212599e-02,\n",
       "          6.54832274e-03, -8.74340162e-02, -2.01666728e-02, -5.66617679e-03,\n",
       "         -1.19416276e-02, -4.35749963e-02,  4.29545678e-02, -2.38251388e-02,\n",
       "         -4.25016275e-03,  1.13558620e-02, -2.00725114e-03,  1.60075184e-02,\n",
       "         -1.79355424e-02,  3.82244550e-02,  9.22842138e-03,  2.01907121e-02,\n",
       "         -7.30796577e-03, -1.74092490e-03, -6.40940433e-03,  3.05501148e-02,\n",
       "         -5.35267405e-04,  7.29934266e-03, -5.28031364e-02,  3.55484374e-02,\n",
       "          2.89497562e-02, -4.95155603e-02,  2.07952391e-02, -1.97610129e-02,\n",
       "          1.35045676e-02, -2.31142156e-02,  7.41740167e-02, -6.90774769e-02,\n",
       "         -7.49514299e-03, -1.79254636e-02, -1.29238535e-02,  1.30428514e-03,\n",
       "          2.91884243e-02, -2.35395469e-02, -1.82140768e-02, -1.92803517e-02,\n",
       "         -4.38394211e-02,  3.84029709e-02, -1.42937377e-02,  3.48812267e-02,\n",
       "          5.83597980e-02, -2.88505200e-02, -2.18356438e-02, -3.35261151e-02,\n",
       "          1.45150917e-02, -4.74299602e-02,  9.73985419e-02,  4.84744582e-04,\n",
       "         -1.42717047e-03, -1.54534215e-02,  3.57374214e-02,  7.70541048e-03,\n",
       "          4.45932560e-02,  3.51992832e-03,  6.51532263e-02,  6.25385670e-03,\n",
       "          3.21504995e-02,  7.43565783e-02, -2.82729585e-02, -8.56425166e-02,\n",
       "          2.31088679e-02, -2.89521497e-02,  7.95214027e-02, -4.49918285e-02,\n",
       "          4.05370183e-02,  1.31632518e-02,  2.14468781e-02,  4.64808708e-03,\n",
       "         -1.11293439e-02,  3.02781295e-02, -2.10012868e-02,  7.55757838e-02,\n",
       "         -2.47067045e-02,  5.39939813e-02, -4.64941002e-02, -7.15036392e-02,\n",
       "         -3.92472744e-02, -1.09468978e-02,  9.68280062e-03,  1.37582945e-03,\n",
       "         -1.24948733e-02, -9.85357072e-03, -5.00321062e-03, -7.69597152e-03,\n",
       "          1.95768178e-02, -2.67775059e-02, -1.16487136e-02,  7.82018993e-03]),\n",
       "  'score': tensor(0.1005)},\n",
       " {'page_number': 149,\n",
       "  'sentence_chunk': 'he whispered. “Dad?”   They just looked at him, smiling. And slowly, Harry looked into the faces of the other people in the mirror, and saw other pairs of green eyes like his, other noses like his, even a little old man who looked as though he had Harry’s knobbly knees — Harry was looking at his family, for the first time in his life.   The Potters smiled and waved at Harry and he stared hungrily back at them, his hands pressed flat against the glass as though he was hoping to fall right through it and reach them. He had a powerful kind of ache inside him, half joy, half terrible sadness.   How long he stood there, he didn’t know. The reflections did not fade and he looked and looked until a distant noise brought him back to his senses. He couldn’t stay here, he had to find his way back to bed. He tore his eyes away from his mother’s face, whispered, “I’ll come back,” and hurried from the room. “You could have woken me up,” said Ron, crossly.   “You can come tonight, I’m going back, I want to show you the mirror.   “I’d like to see your mom and dad,” Ron said eagerly.',\n",
       "  'chunk_char_count': 1084,\n",
       "  'chunk_word_count': 215,\n",
       "  'chunk_token_count': 271.0,\n",
       "  'embedding': array([-3.22584286e-02, -3.66241112e-02,  2.88079735e-02, -4.26184908e-02,\n",
       "          5.16755953e-02,  4.86250445e-02, -5.86316083e-03, -5.76427672e-04,\n",
       "         -4.37465347e-02,  2.79261768e-02,  2.82089952e-02, -1.82233583e-02,\n",
       "         -4.95810322e-02, -1.24207120e-02, -1.54328477e-02,  4.32885019e-03,\n",
       "          1.14680296e-02, -1.08991273e-01, -4.89247739e-02, -2.70757712e-02,\n",
       "          4.66211848e-02, -4.49828524e-03,  1.99254453e-02, -7.69307790e-03,\n",
       "         -3.05551589e-02, -8.68796371e-03,  6.26692772e-02,  1.17901750e-02,\n",
       "         -5.00802249e-02,  2.34830342e-02, -2.43528653e-02, -6.70645684e-02,\n",
       "          4.96593490e-03,  6.57508383e-03, -1.63573143e-03,  2.39265785e-02,\n",
       "          2.33792085e-02,  7.18638115e-03,  6.35080710e-02, -9.62411985e-03,\n",
       "         -6.11599088e-02,  1.91477556e-02,  6.64450368e-03, -1.37261050e-02,\n",
       "          3.48125063e-02, -7.48865632e-03, -7.76223913e-02,  2.16455888e-02,\n",
       "         -2.46132305e-03, -4.49298844e-02,  3.69355902e-02,  3.03156339e-02,\n",
       "         -3.14220600e-02,  3.44242081e-02,  4.83674482e-02,  1.03410728e-01,\n",
       "          5.96085526e-02,  1.60696190e-02,  1.42969778e-02, -6.81322441e-02,\n",
       "          1.34708226e-01,  1.39202382e-02, -3.74102145e-02, -2.78503429e-02,\n",
       "         -3.03355185e-03,  1.71334464e-02, -2.61533521e-02, -4.06973399e-02,\n",
       "         -3.91800255e-02,  6.10231841e-03,  3.84902991e-02, -8.48232731e-02,\n",
       "          1.62263755e-02,  3.91158238e-02, -3.51764821e-02,  6.87598512e-02,\n",
       "          5.03435247e-02,  2.44265962e-02, -1.49178123e-02,  2.73597380e-03,\n",
       "         -4.52140011e-02,  6.91310391e-02, -4.12514200e-03, -1.36444513e-02,\n",
       "          3.83129269e-02, -1.68410689e-02,  8.45033396e-03, -8.83285180e-02,\n",
       "         -4.94570285e-03,  3.39361653e-02,  4.20828350e-02,  8.04389119e-02,\n",
       "         -1.41931688e-02,  7.52906427e-02, -1.19036399e-02,  8.26639030e-03,\n",
       "         -2.45696455e-02,  8.41099694e-02, -1.30706029e-02,  7.85389636e-03,\n",
       "          3.20368931e-02, -4.46522422e-03, -4.23646495e-02,  1.02293286e-02,\n",
       "          1.74748451e-02, -1.33827934e-02, -8.10248870e-03, -4.58516292e-02,\n",
       "         -4.22071330e-02, -6.51137456e-02, -1.46586746e-02, -5.20243160e-02,\n",
       "          3.10687162e-02,  3.60983796e-02,  4.55656648e-03, -4.38001752e-03,\n",
       "          3.20950076e-02, -4.19840449e-03, -1.12152146e-02,  2.31464449e-02,\n",
       "          5.33229336e-02, -3.58488820e-02,  1.08335242e-01, -6.47978671e-03,\n",
       "          1.34696234e-02, -7.03649316e-03,  3.60518172e-02, -5.07808104e-02,\n",
       "          3.43441172e-03,  2.10896395e-02, -5.73124550e-02,  7.42092868e-03,\n",
       "          3.67926732e-02, -6.59222752e-02, -5.72744682e-02,  1.10036368e-02,\n",
       "          2.86523122e-02,  8.11575577e-02, -2.42613275e-02,  2.49401666e-02,\n",
       "         -7.32980110e-03,  2.60274354e-02, -1.37190279e-02,  8.68713204e-03,\n",
       "         -9.38874763e-03,  1.01188850e-03, -9.31836758e-03,  4.25207615e-02,\n",
       "         -5.16052507e-02,  9.25234854e-02,  3.52897048e-02, -2.76523866e-02,\n",
       "          9.86683927e-03, -3.11925542e-02,  3.49116065e-02,  2.33205091e-02,\n",
       "         -2.75078584e-02,  1.34690041e-02, -4.89997528e-02,  2.79748961e-02,\n",
       "          2.62249936e-03,  4.32126410e-02,  3.42873670e-02, -3.49123478e-02,\n",
       "         -1.19581223e-02,  3.46881635e-02,  3.79196517e-02,  1.11516463e-02,\n",
       "          4.24642041e-02, -4.61634360e-02,  8.78123660e-03,  2.47962531e-02,\n",
       "          4.59339023e-02,  7.55688176e-02,  8.12297501e-03,  1.84866413e-02,\n",
       "         -3.92823555e-02, -3.10285073e-02,  2.46521812e-02,  1.38615370e-02,\n",
       "         -6.76830858e-02,  3.37738432e-02, -3.71843250e-03, -1.27506182e-02,\n",
       "         -3.33380932e-03,  2.44267844e-02, -3.77067141e-02, -5.65264076e-02,\n",
       "         -2.70121191e-02,  4.03001793e-02, -5.65715693e-03, -1.35101182e-02,\n",
       "          4.65187617e-02,  7.76188076e-02,  7.71656865e-03,  3.89905125e-02,\n",
       "         -9.15651303e-03, -8.91902018e-03,  5.74277863e-02, -4.60073017e-02,\n",
       "         -7.60781020e-03,  3.76295596e-02, -2.96084769e-02, -4.82235774e-02,\n",
       "         -3.28981318e-02, -1.28675054e-03,  2.52776463e-02,  4.74329200e-03,\n",
       "         -8.37016385e-03,  4.50735092e-02, -2.71012969e-02,  4.75635491e-02,\n",
       "         -1.86685659e-02,  1.08625088e-02,  5.48518784e-02, -1.83563214e-02,\n",
       "          3.73457223e-02, -2.29240209e-02,  1.97325693e-03,  1.19311800e-02,\n",
       "          8.12468177e-04,  6.29939651e-03, -2.35824827e-02, -5.43530285e-02,\n",
       "         -4.26315293e-02, -3.41973975e-02,  1.38237681e-02, -8.35997984e-03,\n",
       "          1.89145107e-03,  1.99854281e-02, -3.65135968e-02,  2.12223288e-02,\n",
       "         -3.20006423e-02, -5.19189835e-02,  6.44249236e-03,  4.07622829e-02,\n",
       "         -5.79209812e-03,  1.71899702e-02, -1.20331824e-03,  4.25344557e-02,\n",
       "         -3.62357907e-02, -1.14559494e-02, -8.55121203e-03,  4.23112661e-02,\n",
       "          6.17031520e-03, -3.11334319e-02,  5.96949682e-02,  3.19458395e-02,\n",
       "          5.67174405e-02,  3.85963777e-03,  3.12728100e-02,  3.67546342e-02,\n",
       "         -1.11719044e-02, -2.10022964e-02, -1.61061180e-03, -2.05087494e-02,\n",
       "         -1.41033093e-02,  1.90631356e-02, -8.22151545e-03,  9.24932733e-02,\n",
       "         -1.99863184e-02, -2.88016032e-02, -3.50325629e-02,  3.21230851e-02,\n",
       "          1.70524009e-02,  1.27862894e-03, -4.91352603e-02,  2.24756859e-02,\n",
       "         -5.80011867e-02,  1.26593234e-02, -3.11533734e-02, -2.31108274e-02,\n",
       "         -4.90281684e-03, -9.61825904e-03,  1.93544924e-02,  1.04796374e-02,\n",
       "          2.83596432e-03,  3.48129384e-02, -1.90570597e-02, -3.85854952e-02,\n",
       "          3.69909108e-02, -2.89993063e-02,  4.01235446e-02, -2.76056807e-02,\n",
       "          1.56469774e-02,  1.32822981e-02, -3.48929539e-02,  4.57307771e-02,\n",
       "         -2.15056408e-02, -1.89892109e-02, -4.09755930e-02,  9.99576505e-03,\n",
       "          5.33565693e-02,  1.82472579e-02,  2.79957941e-03,  7.71333203e-02,\n",
       "         -3.01794764e-02, -1.01726025e-01, -5.74656669e-03, -4.79397476e-02,\n",
       "         -1.53430691e-02,  7.32539129e-03, -3.30865122e-02, -1.41140679e-02,\n",
       "         -2.73444597e-02,  2.37634517e-02, -7.55271455e-03, -4.77014482e-03,\n",
       "         -1.46511896e-02,  6.53361753e-02, -3.76268364e-02,  1.18727824e-02,\n",
       "         -2.84194294e-02, -6.63625216e-03, -6.97180554e-02,  3.44440900e-03,\n",
       "         -2.36155782e-02, -9.05533694e-03, -4.05994419e-04,  4.75953296e-02,\n",
       "         -2.50651948e-02, -1.73877776e-02, -3.92353116e-03, -2.00065114e-02,\n",
       "          3.31392586e-02, -7.13540837e-02,  3.49094346e-02,  1.72496750e-03,\n",
       "          1.33425947e-02,  2.23732530e-03, -6.61424324e-02, -9.63488966e-03,\n",
       "         -8.87346268e-03,  6.21837284e-03,  3.31774987e-02,  1.11707365e-02,\n",
       "          4.22008373e-02, -4.20772377e-03,  1.98219772e-02,  5.50167412e-02,\n",
       "         -6.06978796e-02,  5.42379450e-03, -7.58278668e-02,  1.58464648e-02,\n",
       "          4.05806974e-02, -4.89476882e-03, -4.60180752e-02,  2.72653736e-02,\n",
       "         -1.05531532e-02,  7.83574581e-03, -2.59097759e-02,  3.02518290e-02,\n",
       "         -3.73279303e-02,  2.60569733e-02, -1.75333768e-02, -3.53628732e-02,\n",
       "         -4.81901318e-02,  2.52390536e-03,  8.82299733e-04,  2.41141524e-02,\n",
       "         -4.15451676e-02,  3.25926766e-02,  7.25909993e-02, -3.02545652e-02,\n",
       "         -2.85110697e-02, -2.94224843e-02, -5.39544970e-02,  7.96280354e-02,\n",
       "          5.66025004e-02,  3.00809625e-03,  1.98075622e-02,  9.09820721e-02,\n",
       "          3.66159491e-02, -1.95278227e-02,  1.88126042e-02, -4.70815366e-03,\n",
       "         -5.29820621e-02, -2.97161732e-02,  2.75337361e-02, -4.23184037e-02,\n",
       "          2.86714989e-03,  1.20660791e-03,  9.08052456e-03, -1.93442572e-02,\n",
       "         -1.58100054e-02,  1.88585147e-02, -3.97987524e-03,  4.13983203e-02,\n",
       "          1.04752854e-02, -4.28002775e-02, -3.69218108e-03, -2.78778765e-02,\n",
       "         -6.06700517e-02, -1.36645762e-02,  4.66801748e-02, -1.79539481e-03,\n",
       "          2.91053634e-02, -5.67290979e-03,  1.44263161e-02,  6.65890723e-02,\n",
       "          2.66853180e-02,  5.83614148e-02,  2.19989922e-02, -6.80010999e-04,\n",
       "         -3.65597978e-02,  3.12058795e-02,  2.99353655e-02,  3.87041271e-03,\n",
       "         -8.53139535e-03, -2.43898612e-02, -2.10737111e-03,  6.51449561e-02,\n",
       "         -2.79396102e-02, -4.07004952e-02,  4.47944440e-02, -6.52785152e-02,\n",
       "         -1.04995416e-02, -3.20788361e-02,  4.98463139e-02,  5.02040889e-03,\n",
       "         -5.12957852e-03,  3.10920086e-02, -3.61117050e-02, -6.57875137e-03,\n",
       "         -3.13425437e-02, -2.55867448e-02,  3.95704359e-02, -8.24726298e-02,\n",
       "         -1.56430388e-03, -7.67328143e-02, -5.73468767e-02,  1.06720245e-02,\n",
       "          5.36201941e-03, -3.85349654e-02, -2.44995411e-02,  1.17171612e-02,\n",
       "          4.78509776e-02, -1.47453006e-02, -3.05481609e-02, -4.03535329e-02,\n",
       "         -5.17690852e-02,  1.90623049e-02,  7.17606349e-03, -7.58890584e-02,\n",
       "         -2.20295973e-02,  1.82682592e-02,  2.19584610e-02,  3.07279583e-02,\n",
       "         -5.82345529e-03, -4.00915891e-02, -7.74421617e-02, -5.50036356e-02,\n",
       "          1.59260426e-02, -7.72353262e-02, -2.15532724e-02,  2.32889429e-02,\n",
       "          2.09740028e-02, -2.43603345e-02,  4.61277813e-02,  3.01438347e-02,\n",
       "          4.35592085e-02, -1.65378302e-02, -2.62820292e-02,  1.73397418e-02,\n",
       "          1.95558853e-02, -2.61670724e-02, -1.92224607e-02, -2.47497223e-02,\n",
       "         -1.07817808e-02,  3.20738479e-02,  3.27755436e-02,  2.38221195e-02,\n",
       "         -4.28267419e-02, -2.96078064e-03,  1.93314180e-02,  5.05552031e-02,\n",
       "         -1.01596201e-02,  1.33868707e-02, -4.74298000e-02,  3.14531252e-02,\n",
       "         -2.42181495e-02,  2.57089920e-02,  1.35563193e-02, -3.02714724e-02,\n",
       "         -3.15769576e-02,  8.77986103e-03, -3.98014225e-02,  7.21760169e-02,\n",
       "         -3.07232309e-02,  5.95495012e-03, -1.70270000e-02,  1.38779534e-02,\n",
       "         -2.17637308e-02,  2.20604669e-02,  3.57056372e-33,  5.12829004e-03,\n",
       "          4.06572111e-02,  1.36372382e-02,  5.67519851e-03, -2.04749219e-02,\n",
       "          1.02639310e-02,  1.30185848e-02,  4.45821928e-03,  2.69735768e-03,\n",
       "         -4.29636892e-03, -8.78350530e-03, -3.87545340e-02, -2.38869358e-02,\n",
       "          8.47705454e-03, -3.00944224e-02,  2.38378718e-02,  9.68542174e-02,\n",
       "         -2.22702064e-02, -1.47872856e-02,  2.17368477e-03, -5.79064935e-02,\n",
       "          6.05352521e-02, -8.82513542e-03,  3.92908268e-02,  5.06040566e-02,\n",
       "          1.57476850e-02,  8.67034867e-03, -3.11053135e-02,  1.21146766e-02,\n",
       "          1.81712043e-02,  3.16978283e-02, -3.33146490e-02,  1.92808155e-02,\n",
       "         -4.14960422e-02, -5.89702614e-02, -3.31932157e-02,  3.53762619e-02,\n",
       "         -2.50638891e-02, -3.88921872e-02, -4.78562117e-02, -3.01261395e-02,\n",
       "          6.29166290e-02,  9.43451561e-03, -4.13636714e-02, -4.10129055e-02,\n",
       "          1.98532064e-02, -1.30399186e-02, -2.31978428e-02,  3.40843685e-02,\n",
       "         -2.02041324e-02,  5.24771027e-02,  6.55312240e-02,  1.70717258e-02,\n",
       "         -4.51203138e-02,  1.94936972e-02,  6.57610223e-02,  1.18994460e-01,\n",
       "         -6.75271153e-02,  2.05586515e-02,  1.11366352e-02, -5.80784306e-03,\n",
       "          1.82915833e-02,  1.78534456e-03,  3.68826017e-02, -8.03083330e-02,\n",
       "          3.89627628e-02,  3.68001685e-02, -2.20761225e-02,  2.60658450e-02,\n",
       "          5.72680831e-02,  9.30438656e-03, -7.17766434e-02,  8.75720871e-04,\n",
       "          3.23886834e-02, -4.99251261e-02, -3.28756608e-02, -3.94411348e-02,\n",
       "         -1.35226613e-02,  3.93656604e-02, -2.57253498e-02, -2.86147837e-02,\n",
       "         -2.30912934e-03,  6.89221472e-02,  1.14414021e-02, -1.40267359e-02,\n",
       "         -1.80744678e-02,  4.03560922e-02, -5.36269583e-02, -8.02566111e-03,\n",
       "         -3.73660587e-02, -4.92530242e-02, -1.39532574e-02,  8.93569458e-03,\n",
       "          1.33014668e-03, -7.07326829e-02,  3.29512805e-02,  4.75829802e-02,\n",
       "         -5.17557049e-03, -2.95898505e-02, -2.72100996e-02, -3.51786874e-02,\n",
       "         -8.03991407e-02,  4.44709808e-02, -2.08333810e-03, -3.56857553e-02,\n",
       "          2.86979675e-02, -2.43413523e-02, -7.80881336e-03, -2.27877144e-02,\n",
       "         -1.74753517e-02,  1.04279267e-02,  3.69462669e-02,  2.60643773e-02,\n",
       "         -5.23418002e-02,  4.76545980e-03,  2.87839547e-02, -4.75279912e-02,\n",
       "         -1.79515760e-02,  7.00757951e-02,  1.99612603e-02, -2.13876609e-02,\n",
       "         -2.02577505e-02, -9.71598644e-03, -4.40832824e-02,  4.68152016e-02,\n",
       "          3.20730060e-02, -5.61496466e-02,  6.64860988e-03, -7.46113881e-02,\n",
       "         -8.68858024e-02,  1.29425349e-02,  5.32251559e-02,  4.71875584e-03,\n",
       "         -2.43082494e-02, -1.80711932e-02,  5.05749742e-03,  4.44506891e-02,\n",
       "         -7.18948839e-04, -1.77916065e-02, -2.03444008e-02,  3.21686491e-02,\n",
       "         -5.50011508e-02,  2.64004972e-02, -2.24353634e-02,  2.06759013e-02,\n",
       "         -1.18108373e-02, -5.86366095e-03,  6.94922311e-03, -2.04854403e-02,\n",
       "         -7.02050421e-03,  1.35213267e-02, -4.09159027e-02,  4.13843282e-02,\n",
       "         -1.81448255e-02, -3.59337265e-03, -1.46495728e-02, -3.53013389e-02,\n",
       "         -2.69302949e-02, -5.40439561e-02,  2.67308857e-02, -4.33877520e-02,\n",
       "          2.51098238e-02,  7.26787820e-02, -2.98476033e-02, -6.18187524e-03,\n",
       "         -2.45666271e-03,  2.14312263e-02, -5.88648133e-02,  1.39374826e-02,\n",
       "          4.15522978e-02, -2.51847990e-02,  5.61187370e-03, -3.85814765e-03,\n",
       "          4.14773189e-02,  6.20890297e-02,  1.48050161e-02,  4.94999252e-03,\n",
       "         -8.90688505e-03, -1.23766651e-02, -3.53921875e-02,  4.35820296e-02,\n",
       "         -8.40592291e-03, -4.89400439e-02, -2.38500331e-02, -7.16293463e-03,\n",
       "         -1.95143037e-02, -2.81477235e-02,  1.44215431e-02, -4.39965241e-02,\n",
       "          1.89685647e-03,  1.88502204e-02, -4.01327349e-02,  5.24612777e-02,\n",
       "         -1.96866598e-02, -1.50719250e-03,  6.45702844e-03,  1.99474413e-02,\n",
       "         -2.83070523e-02, -2.54853014e-02, -6.38249377e-03,  8.66825506e-02,\n",
       "         -1.96058806e-02, -2.47719847e-02, -3.27679664e-02,  6.42063320e-02,\n",
       "          1.55030368e-02, -4.24607992e-02, -2.78141834e-02, -1.83888543e-02,\n",
       "         -4.79420368e-03, -2.30170209e-02,  5.24066463e-02, -8.52138028e-02,\n",
       "          1.85563546e-02,  1.24579854e-02, -4.51267362e-02, -2.41898801e-02,\n",
       "          2.70562433e-02, -8.66463035e-03, -1.55514767e-02, -4.21209447e-02,\n",
       "         -2.92207021e-02,  9.32958256e-03, -1.72827691e-02,  4.82889190e-02,\n",
       "          5.14916740e-02, -1.78611502e-02,  4.84544337e-02, -6.04247954e-03,\n",
       "          4.70842943e-02, -3.98054533e-02,  1.02307245e-01,  5.14120655e-03,\n",
       "         -7.13306759e-03, -2.03915183e-02,  2.68602837e-02, -2.60677934e-02,\n",
       "         -1.64915696e-02, -2.73229118e-04,  4.70230244e-02,  3.93801332e-02,\n",
       "         -2.28860008e-04,  1.86886061e-02, -2.29774211e-02, -3.10018286e-02,\n",
       "          3.50255296e-02, -6.27474533e-03,  6.12203851e-02, -1.49993952e-02,\n",
       "          2.07214020e-02, -1.18316533e-02,  4.93258238e-03,  1.18836183e-02,\n",
       "          3.17075029e-02, -3.24920728e-03, -5.65926060e-02,  6.33685514e-02,\n",
       "         -1.25923427e-02,  4.15719859e-02,  3.38173695e-02, -6.04519732e-02,\n",
       "          5.91601944e-03,  1.66700240e-02, -8.09071772e-03,  1.32747721e-02,\n",
       "         -3.42815071e-02, -2.93078329e-02, -4.72656116e-02, -2.72971652e-02,\n",
       "         -1.85071006e-02, -1.50633147e-02, -3.21723633e-02,  2.07202192e-02]),\n",
       "  'score': tensor(0.0857)},\n",
       " {'page_number': 30,\n",
       "  'sentence_chunk': '“Dudley — go — just go.”   Harry walked round and round his new room. Someone knew he had moved out of his cupboard and they seemed to know he hadn’t received his first letter. Surely that meant they’d try again?And this time he’d make sure they didn’t fail. He had a plan. The repaired alarm clock rang at six o’clock the next morning. Harry turned it off quickly and dressed silently. He mustn’t wake the Dursleys. He stole downstairs without turning on any of the lights.   He was going to wait for the postman on the corner of Privet Drive and get the letters for number four first. His heart hammered as he crept across the dark hall toward the front door —    “AAAAARRRGH!”   Harry leapt into the air; he’d trodden on something big and squashy on the doormat — something alive!   Lights clicked on upstairs and to his horror Harry realized that the big, squashy something had been his uncle’s face. Uncle Vernon had been lying at the foot of the front door in a sleeping bag, clearly making sure that Harry didn’t do exactly what he’d been trying to do. He shouted at Harry for about half an hour and then told him to go and make a cup of tea. Harry shuffled miserably off into the kitchen and by the time he got back, the mail had arrived, right into Uncle Vernon’s lap. Harry could see three letters addressed in green ink.   “I want —” he began, but Uncle Vernon was tearing the letters into pieces before his eyes.   Uncle Vernon didn’t go to work that day.',\n",
       "  'chunk_char_count': 1467,\n",
       "  'chunk_word_count': 285,\n",
       "  'chunk_token_count': 366.75,\n",
       "  'embedding': array([-1.98950507e-02, -6.86561614e-02,  5.78382192e-03, -8.08113534e-03,\n",
       "          5.68708517e-02,  4.66487259e-02, -1.02603594e-02,  2.40449747e-03,\n",
       "         -7.53567368e-03,  2.38865669e-05,  4.72724773e-02, -2.57714074e-02,\n",
       "         -4.89323912e-03, -1.97959784e-02, -4.25455384e-02, -2.57027354e-02,\n",
       "          4.69228849e-02, -3.22547965e-02, -5.26396148e-02, -1.95526145e-02,\n",
       "          3.73984426e-02, -3.32618393e-02,  7.13450648e-03, -9.36350878e-03,\n",
       "         -1.89408697e-02, -1.65222306e-02,  6.66653737e-02,  1.82754584e-02,\n",
       "         -5.99126704e-02,  2.44846717e-02, -3.12736109e-02, -3.48089971e-02,\n",
       "          3.69363502e-02,  4.33106124e-02, -6.15783827e-03,  2.40520425e-02,\n",
       "          3.62468585e-02, -2.87446473e-03,  2.30497401e-02, -2.54483502e-02,\n",
       "         -4.74306494e-02, -9.93829221e-03,  2.56914436e-03, -3.16255423e-03,\n",
       "          1.86636765e-02, -8.07802100e-03, -5.16270548e-02,  3.61356474e-02,\n",
       "          3.37028541e-02,  1.28300171e-02,  3.94333526e-02,  8.30693915e-03,\n",
       "         -2.72222348e-02,  6.27989545e-02,  3.89038748e-03,  2.21881308e-02,\n",
       "          2.18376014e-02,  6.96246326e-02, -4.82933130e-03, -3.22650857e-02,\n",
       "          7.07562193e-02, -1.43245957e-03, -2.38792170e-02,  6.52177026e-03,\n",
       "          1.87147763e-02, -1.84849165e-02, -5.02648503e-02, -2.09730230e-02,\n",
       "         -3.48781459e-02, -1.72979347e-02,  8.18528794e-03, -2.78448295e-02,\n",
       "         -1.06299538e-02,  9.80557129e-03, -2.00577490e-02,  2.62623653e-02,\n",
       "         -2.41964348e-02,  2.12203525e-02, -1.20450696e-02, -4.68901545e-03,\n",
       "         -5.88340685e-03,  5.44745885e-02,  1.32588912e-02,  1.37001295e-02,\n",
       "          2.24733576e-02,  2.15813033e-02,  2.37631090e-02, -8.07714760e-02,\n",
       "          1.44562116e-02,  1.13603137e-02,  2.30046269e-02,  7.48247951e-02,\n",
       "          5.12539735e-03,  1.00407615e-01, -1.03873089e-02, -1.06181446e-02,\n",
       "         -4.56640944e-02,  1.04390949e-01, -4.36385395e-03, -2.04236452e-02,\n",
       "          2.15643458e-02, -2.69562360e-02, -6.78917095e-02, -1.46893822e-02,\n",
       "          2.69873459e-02, -2.53741797e-02,  1.16527723e-02, -3.21380571e-02,\n",
       "         -2.99854539e-02, -2.34159417e-02, -3.48750092e-02, -6.93149492e-03,\n",
       "         -8.72107409e-03,  2.93768793e-02,  1.58801340e-02, -4.52336948e-03,\n",
       "          2.44662613e-02,  5.00964560e-03,  2.83359252e-02, -6.95146527e-03,\n",
       "          2.33443603e-02, -2.31597908e-02,  3.55256498e-02,  5.04617393e-02,\n",
       "          1.95630379e-02,  2.05724966e-02,  4.75219116e-02, -3.99288684e-02,\n",
       "         -2.13111397e-02, -1.11025674e-02, -9.40277278e-02, -1.34836268e-02,\n",
       "          2.03626417e-02, -6.63478300e-02, -2.71658078e-02,  2.30527036e-02,\n",
       "          3.51623110e-02,  4.15009335e-02,  1.68046710e-04,  4.75578420e-02,\n",
       "         -2.26371046e-02,  5.00151850e-02, -2.06787847e-02,  2.33121356e-03,\n",
       "         -2.90447026e-02,  2.47393437e-02, -2.59333337e-03,  3.91439684e-02,\n",
       "         -4.50909249e-02,  1.30556062e-01,  4.58211675e-02, -2.08146982e-02,\n",
       "         -2.25814097e-02, -3.42911594e-02, -1.99798532e-02, -1.89453512e-02,\n",
       "          6.29299274e-03,  5.91110513e-02,  1.10399686e-02,  8.22751131e-03,\n",
       "         -1.17645133e-02,  3.81567739e-02,  3.38782780e-02, -1.66535210e-02,\n",
       "         -3.41080539e-02,  1.44281453e-02,  2.49040518e-02, -9.59668308e-03,\n",
       "          3.71777490e-02, -4.51426730e-02,  7.72850309e-03,  9.47975088e-03,\n",
       "          2.38605533e-02,  4.67202626e-02, -3.33361477e-02,  2.25905497e-02,\n",
       "         -2.86323465e-02, -4.20111865e-02,  1.30170127e-02,  3.61073278e-02,\n",
       "         -3.50765362e-02,  4.80792858e-02, -4.83091688e-03, -1.58341862e-02,\n",
       "          8.92031100e-03,  2.25234032e-02, -2.60801660e-03, -2.01992467e-02,\n",
       "         -5.82072847e-02,  3.66065726e-02, -3.95867638e-02,  2.50286460e-02,\n",
       "          2.87074037e-02,  2.28794441e-02,  4.69776019e-02,  4.29761074e-02,\n",
       "         -1.63911004e-02,  5.73283667e-03,  2.78918073e-02, -7.03733712e-02,\n",
       "         -1.30067766e-02, -5.95036633e-02, -3.88158415e-03, -4.53041233e-02,\n",
       "         -6.51132166e-02,  1.12068504e-02,  3.14235091e-02,  1.64243914e-02,\n",
       "          2.31052446e-03,  2.04730742e-02, -3.29739377e-02, -1.00455778e-02,\n",
       "         -7.26506347e-03,  5.26737161e-02,  5.81770949e-02, -6.86314655e-03,\n",
       "          3.19235176e-02, -7.38538615e-03, -2.83331401e-03,  7.68221775e-03,\n",
       "         -1.55109176e-02, -7.57522788e-03, -4.24450561e-02,  2.74282694e-03,\n",
       "          2.12296422e-04, -8.60038176e-02, -1.57907400e-02, -4.89304075e-03,\n",
       "         -3.70510668e-02,  6.01666309e-02, -2.64690574e-02,  1.50428470e-02,\n",
       "         -4.67852280e-02, -7.46556148e-02, -3.18076089e-02, -4.33182344e-02,\n",
       "         -1.10261757e-02, -3.03949676e-02,  2.65905894e-02,  3.03387102e-02,\n",
       "         -1.21515002e-02,  3.60085210e-03, -5.33439871e-03, -1.79665606e-03,\n",
       "          2.91697885e-04, -2.98233591e-02,  2.69405730e-02, -1.17250392e-02,\n",
       "          7.50368312e-02, -5.23258522e-02,  2.34272461e-02, -2.09615752e-02,\n",
       "         -2.08757352e-02, -1.18893478e-02,  4.05755378e-02, -4.31546103e-03,\n",
       "          8.57125130e-03,  1.94365755e-02,  5.96440472e-02,  4.65205312e-02,\n",
       "          1.84622277e-02, -2.02900879e-02, -5.53153940e-02,  2.20616870e-02,\n",
       "         -1.26877483e-02,  3.15156206e-03, -4.44627255e-02,  3.61413732e-02,\n",
       "         -7.26093128e-02, -2.78712567e-02,  2.41352618e-03,  3.65624786e-03,\n",
       "         -1.36385476e-02,  4.76975329e-02,  2.49716546e-02,  3.38032767e-02,\n",
       "         -2.97273453e-02,  4.51169387e-02, -5.60362777e-03, -4.18826006e-02,\n",
       "         -5.67164784e-03, -4.35518697e-02, -1.97914802e-02, -2.89223995e-02,\n",
       "         -2.06689723e-02,  4.98052128e-02, -2.71762740e-02,  2.72806305e-02,\n",
       "          8.68112687e-03, -1.49820652e-02, -4.16303165e-02,  3.46588949e-03,\n",
       "          4.71696220e-02,  3.44492644e-02,  1.74717009e-02,  5.69412559e-02,\n",
       "         -1.81129314e-02, -6.47503883e-02, -9.74709075e-03, -3.98434065e-02,\n",
       "         -3.98037508e-02, -4.12676595e-02, -9.04297605e-02,  4.36882339e-02,\n",
       "         -2.05218680e-02,  8.42942204e-03,  1.42316315e-02,  1.84838418e-02,\n",
       "          1.97838973e-02,  5.12218028e-02, -3.09683615e-03,  1.55691663e-02,\n",
       "         -1.56740136e-02,  2.09370293e-02, -5.43494560e-02,  1.35742854e-02,\n",
       "          4.08071006e-04, -4.48260009e-02,  4.06289361e-02,  1.50229444e-03,\n",
       "          5.63824177e-03, -4.06502932e-02, -5.56543767e-02, -5.57908975e-03,\n",
       "          1.24200033e-02, -5.57911769e-02,  1.27365382e-03, -1.98913575e-03,\n",
       "         -2.80817635e-02,  6.21365430e-03, -7.35675618e-02,  3.73532623e-03,\n",
       "         -2.93134991e-02, -7.22434232e-03,  3.00796926e-02, -1.11169796e-02,\n",
       "          3.87239642e-02, -1.35590518e-02,  8.34591873e-03,  5.22664487e-02,\n",
       "         -3.70987542e-02, -7.87485205e-03, -8.40400383e-02,  1.55249937e-02,\n",
       "         -1.53460691e-03,  1.36225270e-02, -9.79106650e-02,  4.48448472e-02,\n",
       "         -2.44784784e-02, -2.60954443e-02,  1.74026042e-02,  2.38423813e-02,\n",
       "          2.97979154e-02,  4.25122678e-02, -4.07462344e-02,  1.67356394e-02,\n",
       "         -1.31862825e-02, -4.59317304e-02, -1.89784449e-03, -4.32136320e-02,\n",
       "         -6.26446158e-02,  5.03151529e-02,  2.94832401e-02, -1.35427434e-02,\n",
       "          2.43867207e-02, -1.04036383e-01, -7.10478351e-02,  8.24321061e-02,\n",
       "          2.05434654e-02, -1.72125660e-02, -1.10536655e-02,  8.03316012e-02,\n",
       "          1.76437199e-02, -3.15658227e-02, -1.47896940e-02, -1.49713922e-03,\n",
       "         -7.49131367e-02, -6.07184619e-02,  1.02721840e-01,  4.14675521e-03,\n",
       "          8.30758188e-04, -2.85737943e-02, -4.05158959e-02, -3.04960962e-02,\n",
       "          3.35248150e-02,  8.35321657e-03, -1.37288021e-02,  1.21344309e-02,\n",
       "          3.95213179e-02, -6.87898742e-03,  8.51746555e-03, -2.30854154e-02,\n",
       "         -3.74480523e-02,  2.05025375e-02,  2.71081869e-02,  5.86776179e-04,\n",
       "         -4.45270725e-02, -4.87755798e-03,  1.68924965e-02,  4.00863178e-02,\n",
       "          6.10517571e-03,  7.15782046e-02,  4.14354056e-02, -3.16504985e-02,\n",
       "         -3.49551328e-02,  1.97959431e-02, -3.01354788e-02,  2.23680935e-03,\n",
       "         -3.27982232e-02, -7.22042173e-02, -2.79779709e-03,  7.64917061e-02,\n",
       "         -2.50179321e-02, -8.52003321e-02,  4.82313782e-02, -3.42722572e-02,\n",
       "         -1.87707786e-02,  3.27873193e-02,  4.21285443e-02,  3.75363976e-02,\n",
       "          4.82572392e-02,  2.50283442e-02, -5.66600971e-02,  1.66895967e-02,\n",
       "         -3.52355391e-02, -1.61421497e-03,  1.60456076e-02, -5.46087101e-02,\n",
       "          1.47145139e-02, -6.61457479e-02, -1.00347839e-01, -1.23726893e-02,\n",
       "          2.28916127e-02, -1.80210378e-02, -7.77600473e-03,  1.80434026e-02,\n",
       "          4.78357077e-02,  1.32113248e-02,  1.29517624e-02, -7.17984419e-03,\n",
       "          5.46667445e-03,  2.49056630e-02,  3.74576859e-02, -6.65133223e-02,\n",
       "         -2.38676947e-02, -7.25845399e-04,  1.32807577e-02,  7.84426276e-03,\n",
       "         -2.37865094e-02,  1.22233732e-02, -5.06528504e-02, -2.56966539e-02,\n",
       "          2.56542172e-02, -1.26834035e-01, -8.90091807e-03, -1.61471718e-03,\n",
       "          2.08372087e-03, -2.97192261e-02, -1.75228044e-02,  2.39110161e-02,\n",
       "          6.57156669e-03,  1.77834108e-02, -3.00451159e-03,  1.77348275e-02,\n",
       "          3.60771455e-02, -5.05024893e-03, -4.02186401e-02, -4.24470231e-02,\n",
       "         -8.07700492e-03,  4.45541143e-02,  1.66470464e-02,  3.78879122e-02,\n",
       "         -2.25781724e-02, -3.23505029e-02,  3.72595191e-02,  4.08589393e-02,\n",
       "         -1.41446909e-03,  4.88331504e-02, -6.98490366e-02,  2.36648247e-02,\n",
       "         -6.28447020e-03,  1.38792470e-02,  1.04825450e-02, -1.28115173e-02,\n",
       "         -4.71450724e-02,  1.28511051e-02, -4.40494269e-02,  7.82667249e-02,\n",
       "         -3.85884307e-02, -1.92907825e-02,  2.77972016e-02,  2.29178835e-02,\n",
       "          2.06654537e-02,  2.49950420e-02,  4.03669352e-33, -1.91244371e-02,\n",
       "          1.42613682e-03,  1.58527214e-02,  1.22322133e-02, -3.06744538e-02,\n",
       "          8.20708089e-03, -2.78394036e-02,  5.78810386e-02,  3.30851264e-02,\n",
       "         -3.82471224e-03, -3.30012441e-02,  2.09714510e-02, -1.23978537e-02,\n",
       "          1.91218371e-03,  5.62422862e-03, -2.62545701e-03,  7.26175532e-02,\n",
       "         -5.69435535e-03,  1.13382924e-03, -9.75390244e-03, -5.18087372e-02,\n",
       "          4.73342091e-02, -2.49991771e-02,  1.03419043e-01, -1.09781334e-02,\n",
       "         -1.29047651e-02,  9.42287687e-03, -1.68418940e-02, -3.61021459e-02,\n",
       "          3.42104845e-02,  2.11271495e-02, -1.70300603e-02,  2.74063684e-02,\n",
       "         -2.48668753e-02, -1.05158955e-01, -5.04934415e-02,  2.57116351e-02,\n",
       "          1.17347371e-02, -4.63658161e-02, -2.24602483e-02, -9.37239453e-03,\n",
       "          2.59059034e-02, -2.26289574e-02, -2.28946786e-02, -8.10606703e-02,\n",
       "          2.77309436e-02,  2.16985587e-02, -2.15464793e-02, -4.65280609e-03,\n",
       "         -1.61494818e-02,  6.25762492e-02,  5.04050776e-02, -1.76887419e-02,\n",
       "         -4.12871055e-02,  6.70639500e-02,  4.48565222e-02,  1.71673253e-01,\n",
       "         -3.59017923e-02,  4.86310460e-02, -6.65658293e-03,  7.93687534e-03,\n",
       "          4.52048481e-02,  1.63549334e-02,  6.87117642e-03, -5.63673861e-02,\n",
       "          4.10065576e-02, -1.55567201e-02, -2.70084348e-02,  4.44131205e-03,\n",
       "         -1.11381626e-02,  4.05422933e-02, -5.04641868e-02,  3.33977044e-02,\n",
       "          3.28427441e-02, -2.54973732e-02, -6.32365281e-03, -1.49994455e-02,\n",
       "         -1.68440379e-02,  5.45551963e-02, -1.78493224e-02, -4.15142160e-03,\n",
       "         -1.83324870e-02,  5.08360863e-02,  1.46790976e-02, -2.49983128e-02,\n",
       "          1.02184657e-02,  6.71388907e-03,  1.12524470e-02, -4.11708131e-02,\n",
       "          7.19412137e-03, -5.23686036e-02,  2.65588611e-02, -1.32560823e-02,\n",
       "          1.59378850e-03, -1.22296214e-02,  1.42666893e-02,  3.99853289e-03,\n",
       "          1.79872587e-02, -2.94158719e-02, -4.13622297e-02,  2.03281385e-03,\n",
       "         -3.49554792e-02,  1.74515769e-02, -5.28970780e-03,  3.59968729e-02,\n",
       "          1.10466350e-02, -1.49105042e-02, -6.37126807e-03, -1.57796987e-03,\n",
       "         -5.44427820e-02, -4.54458222e-03,  7.73191303e-02,  1.37957167e-02,\n",
       "         -9.09710526e-02,  1.17141465e-02,  3.27792279e-02, -3.07665188e-02,\n",
       "          1.58789344e-02,  3.59490365e-02,  2.66215280e-02, -4.37177084e-02,\n",
       "         -1.89996902e-02, -2.17266548e-02, -8.04822296e-02,  2.11511161e-02,\n",
       "          3.66295949e-02, -1.30184712e-02, -4.57916781e-02, -1.59953851e-02,\n",
       "         -7.60902017e-02, -4.00880724e-02,  1.73355974e-02, -3.66452895e-02,\n",
       "         -2.45765094e-02, -1.56280380e-02, -2.52463244e-04,  5.95930554e-02,\n",
       "         -1.06192250e-02, -2.95892376e-02, -4.93323356e-02,  2.86767688e-02,\n",
       "         -1.11582186e-02,  5.13778329e-02,  1.70713980e-02,  3.45215923e-03,\n",
       "         -3.27817239e-02, -3.20797563e-02,  2.04213639e-03, -1.87060293e-02,\n",
       "         -3.64281386e-02,  6.54129386e-02, -3.63600217e-02,  3.27875726e-02,\n",
       "         -1.48602435e-02, -1.89695619e-02,  1.52493024e-03,  1.14610679e-02,\n",
       "          1.57613661e-02, -3.80379185e-02,  3.20709385e-02, -1.41023109e-02,\n",
       "          2.09537596e-02,  3.57488878e-02, -4.32596467e-02,  1.30480528e-02,\n",
       "          1.04387309e-02,  1.92898400e-02, -3.74048166e-02, -2.88344529e-02,\n",
       "          1.97581146e-02, -7.65801892e-02, -3.39821284e-03, -2.34362315e-02,\n",
       "          2.51190159e-02,  5.86557575e-02,  5.68514504e-02, -3.65888118e-03,\n",
       "          3.56600769e-02, -1.36747235e-03, -2.62164902e-02,  1.48471566e-02,\n",
       "          5.33428304e-02, -1.95979252e-02, -1.60312690e-02,  3.93774770e-02,\n",
       "          9.68534593e-03, -2.99094245e-02,  4.44877520e-02, -6.29002303e-02,\n",
       "          1.68430880e-02,  1.44336214e-02, -1.60067454e-02,  1.26356604e-02,\n",
       "         -3.02589033e-02,  3.09738405e-02, -1.10736676e-03, -6.99319504e-03,\n",
       "          8.85179080e-03,  3.29983272e-02,  1.68417569e-03,  2.96987742e-02,\n",
       "          1.67682059e-02, -2.37614959e-02, -4.03119549e-02,  4.76339459e-02,\n",
       "          3.13352281e-03, -1.45251183e-02,  3.62176597e-02, -2.23984830e-02,\n",
       "          1.60399321e-02, -3.90279032e-02,  8.72683674e-02, -4.71316874e-02,\n",
       "          1.86572745e-02, -3.29413153e-02,  3.42227295e-02, -1.83839239e-02,\n",
       "          3.80204879e-02, -2.20008548e-02, -4.75773327e-02, -2.47929431e-02,\n",
       "         -5.87277375e-02,  4.44272114e-03, -4.24048677e-02,  2.05720700e-02,\n",
       "          5.66756539e-02, -4.22335938e-02, -3.61834504e-02, -6.27410505e-03,\n",
       "         -2.07653106e-03, -5.81650101e-02,  5.30822761e-02, -1.08183352e-02,\n",
       "         -5.05796969e-02, -3.56926136e-02,  2.29178499e-02,  1.04900561e-02,\n",
       "          5.84041374e-03,  5.78328259e-02,  4.59223315e-02,  6.27799542e-04,\n",
       "          2.33728569e-02,  6.83701262e-02, -2.68041566e-02, -2.59634126e-02,\n",
       "          4.35896441e-02, -4.45600860e-02,  9.36272591e-02, -4.62878309e-02,\n",
       "          5.78307845e-02,  4.58302945e-02,  3.77632231e-02,  2.43191198e-02,\n",
       "         -1.99126881e-02,  2.14270651e-02, -5.15770353e-02,  6.15242533e-02,\n",
       "         -1.50076644e-02,  3.87011506e-02, -3.16223800e-02, -6.02132864e-02,\n",
       "         -2.62027811e-02,  1.38911428e-02,  1.67342182e-02,  2.15060525e-02,\n",
       "         -2.70016193e-02, -4.38030809e-02,  8.36000894e-04, -6.54828548e-03,\n",
       "         -9.64822900e-03, -4.87217493e-02, -3.84900183e-03,  1.84892081e-02]),\n",
       "  'score': tensor(0.0804)},\n",
       " {'page_number': 195,\n",
       "  'sentence_chunk': 'After dinner the three of them sat nervously apart in the common room. Nobody bothered them; none of the Gryffindors had anything to say to Harry any more, after all. This was the first night he hadn’t been upset by it. Hermione was skimming through all her notes, hoping to come across one of the enchantments they were about to try to break. Harry and Ron didn’t talk much. Both of them were thinking about what they were about to do.   Slowly, the room emptied as people drifted off to bed.   “Better get the cloak,” Ron muttered, as Lee Jordan finally left, stretching and yawning. Harry ran upstairs to their dark dormitory. He pulled out the cloak and then his eyes fell on the flute Hagrid had given him for Christmas. He pocketed it to use on Fluffy — he didn’t feel much like singing.   He ran back down to the common room.   “We’d better put the cloak on here, and make sure it covers all three of us – if Filch spots one of our feet wandering along on its own —”    “What are you doing?”said a voice from the corner of the room. Neville appeared from behind an armchair, clutching Trevor the toad, who looked as though he’d been making another bid for freedom.   “Nothing, Neville, nothing,” said Harry, hurriedly putting the cloak behind his back.   Neville stared at their guilty faces.   “You’re going out again,” he said.   “No, no, no,” said Hermione. “No, we’re not.',\n",
       "  'chunk_char_count': 1383,\n",
       "  'chunk_word_count': 267,\n",
       "  'chunk_token_count': 345.75,\n",
       "  'embedding': array([-7.08629470e-03, -7.44938776e-02,  2.20745280e-02, -3.16365100e-02,\n",
       "          7.19486102e-02,  5.31424768e-02, -1.00178458e-03,  3.97081114e-02,\n",
       "          1.69324875e-02,  2.17399225e-02, -5.95137058e-03,  9.28963441e-03,\n",
       "         -2.78275181e-02, -6.01628348e-02, -8.87446292e-03,  8.75352044e-03,\n",
       "          9.07300878e-03, -7.68467262e-02, -5.28948344e-02, -4.84360345e-02,\n",
       "          3.36956307e-02, -6.54695481e-02, -2.57712454e-02, -1.56021565e-02,\n",
       "          3.62956002e-02, -2.26765443e-02,  1.93703528e-02,  6.33184286e-03,\n",
       "          8.53415579e-03, -2.55548880e-02, -1.74455531e-02,  1.01421718e-02,\n",
       "          3.82663473e-03,  2.45282948e-02, -2.58522704e-02,  9.37764999e-03,\n",
       "          4.88054901e-02,  9.20585915e-03,  7.20130056e-02, -1.51328109e-02,\n",
       "         -6.42922968e-02,  3.15152970e-03, -3.29344044e-03,  7.99742248e-03,\n",
       "          1.65420249e-02,  8.60930234e-03, -6.90679550e-02,  3.06542255e-02,\n",
       "         -1.09830191e-02, -4.55713719e-02,  4.50545289e-02,  2.65473537e-02,\n",
       "         -2.16995701e-02,  9.76329437e-04,  5.59461117e-02,  8.76260772e-02,\n",
       "          2.86563486e-02,  8.60458333e-03, -3.65983993e-02, -7.07286224e-02,\n",
       "          9.50869694e-02, -1.64432582e-02, -2.20662002e-02,  2.12807711e-02,\n",
       "         -2.61639506e-02, -4.92113456e-02, -2.22957823e-02, -3.08922492e-02,\n",
       "         -3.63601893e-02, -1.32255731e-02,  2.13629622e-02, -7.03687146e-02,\n",
       "         -1.38499103e-02,  5.05669862e-02, -1.08772758e-02,  4.73000705e-02,\n",
       "          2.25223321e-02,  7.31297806e-02, -4.84742504e-03,  1.79244112e-02,\n",
       "         -1.83246918e-02,  8.61868709e-02, -1.05759287e-02,  3.41556296e-02,\n",
       "          6.81228749e-03, -1.51223410e-02,  1.93165280e-02, -3.51050422e-02,\n",
       "         -9.02408559e-04, -2.86014695e-02, -1.74723957e-02,  8.42628181e-02,\n",
       "         -8.61792546e-03,  1.26132354e-01, -2.48669628e-02, -1.03039825e-02,\n",
       "         -3.75157520e-02,  9.67599601e-02, -4.22738716e-02,  3.39312013e-03,\n",
       "          1.80819761e-02, -3.47567275e-02, -5.67115545e-02, -4.98607457e-02,\n",
       "         -6.52768463e-03, -2.37651449e-02, -2.87433472e-02, -1.25750732e-02,\n",
       "         -5.92600591e-02, -3.33794020e-02,  4.14891988e-02,  2.54624598e-02,\n",
       "          5.84684720e-04,  3.06551494e-02, -1.28805954e-02, -7.17470609e-03,\n",
       "          1.54488282e-02,  1.07259699e-03, -1.63254401e-04,  3.75260264e-02,\n",
       "          4.03042845e-02, -2.57310141e-02,  3.04387119e-02,  3.65712047e-02,\n",
       "         -1.52925188e-02, -5.70038147e-03,  3.64104770e-02, -5.99736013e-02,\n",
       "          1.88073218e-02, -3.89984739e-03, -8.79838020e-02,  2.88852677e-02,\n",
       "          5.46848439e-02, -4.23923656e-02, -4.06031087e-02,  9.12149530e-03,\n",
       "          1.64040625e-02,  2.62802839e-02,  2.56591290e-02,  3.52171957e-02,\n",
       "         -5.99330897e-03,  1.20267253e-02, -5.40384948e-02, -9.32282768e-03,\n",
       "         -2.62540840e-02,  1.34265600e-02, -2.37477683e-02,  5.15315086e-02,\n",
       "         -6.74195364e-02,  4.76757511e-02,  4.94555682e-02, -3.55559923e-02,\n",
       "         -7.56838266e-03, -2.06299126e-02,  3.16557661e-02,  3.29165347e-02,\n",
       "         -2.07322687e-02,  2.91761151e-03, -4.36522439e-02,  4.13621403e-02,\n",
       "          6.62523182e-03,  4.61394973e-02,  3.09894197e-02, -9.22305626e-04,\n",
       "          2.31081364e-03,  1.24770431e-02,  7.54863117e-03,  2.66821105e-02,\n",
       "          3.35111395e-02, -1.55704822e-02, -2.34487820e-02,  1.18592363e-02,\n",
       "          1.16604185e-02,  4.05406579e-02, -1.87484883e-02, -7.44731072e-03,\n",
       "         -2.01519150e-02, -3.89202237e-02,  2.76343785e-02,  6.65793428e-03,\n",
       "         -1.38420807e-02,  8.95730481e-02, -2.46101841e-02, -1.84878502e-02,\n",
       "          2.27161702e-02, -4.54107039e-02, -2.79051233e-02, -6.90966398e-02,\n",
       "         -5.44542298e-02,  2.24238355e-02, -3.46345380e-02,  8.50105379e-03,\n",
       "          4.11168896e-02,  3.33400331e-02,  4.95496579e-02,  4.18302976e-02,\n",
       "         -3.62837762e-02, -2.58076680e-03,  3.06957010e-02, -5.67134731e-02,\n",
       "         -2.28146650e-02, -4.99603078e-02, -2.75860000e-02, -7.75433406e-02,\n",
       "         -6.12386204e-02, -2.30559781e-02, -5.27937114e-02, -1.54434405e-02,\n",
       "         -4.89431210e-02,  4.53820676e-02,  7.89164752e-03, -2.02458929e-02,\n",
       "         -3.06603778e-02,  1.87956577e-03,  5.92162460e-02,  9.72613972e-03,\n",
       "          1.30147273e-02, -3.57541479e-02, -3.68749984e-02, -6.35320740e-03,\n",
       "         -1.90826263e-02,  8.56928714e-03,  1.52199287e-02, -4.25081290e-02,\n",
       "          1.56417186e-03, -3.83274145e-02, -9.41477343e-03,  1.05247519e-03,\n",
       "         -2.77367774e-02,  4.19290252e-02, -3.29809636e-02,  4.81322333e-02,\n",
       "         -5.38356490e-02, -4.51545343e-02, -1.65542439e-02, -8.16121697e-03,\n",
       "         -4.52103605e-03, -1.91201065e-02,  9.53415502e-03,  2.02122312e-02,\n",
       "          1.57655906e-02,  2.31808946e-02,  1.18077584e-02,  2.96620298e-02,\n",
       "         -2.25535426e-02, -5.83786108e-02,  1.94426700e-02,  3.67894955e-02,\n",
       "          7.67622292e-02, -4.62425537e-02,  1.41269630e-02, -2.08913572e-02,\n",
       "         -9.90541279e-03, -1.21823233e-02, -5.63547900e-03,  1.78706404e-02,\n",
       "         -7.06773670e-03,  1.45648494e-02,  2.29327679e-02,  3.56944427e-02,\n",
       "          2.79498156e-02, -5.26414923e-02, -2.72008535e-02, -4.27666644e-04,\n",
       "         -5.35400538e-03,  1.63208414e-02, -5.87331764e-02,  1.43498359e-02,\n",
       "         -3.31406891e-02, -1.22276554e-03,  8.76325089e-03, -2.71458607e-02,\n",
       "          1.97381303e-02,  2.24472620e-02,  1.50472969e-02,  3.71705741e-02,\n",
       "         -3.07678375e-02,  3.79051343e-02,  1.29951276e-02, -4.47028950e-02,\n",
       "         -6.36002282e-03, -4.24019769e-02, -3.25588253e-03, -5.09028770e-02,\n",
       "         -3.52380052e-03,  2.63652261e-02, -1.07208164e-02,  3.28606293e-02,\n",
       "          9.84646194e-03, -5.61320819e-02, -2.01535188e-02,  2.73525007e-02,\n",
       "          4.30144705e-02,  2.70123351e-02,  1.11051183e-02,  5.41928560e-02,\n",
       "         -5.41454414e-03, -1.00527838e-01, -3.16992681e-03, -2.63656452e-02,\n",
       "         -2.49634590e-03, -2.03037225e-02, -1.30162284e-01,  4.37135547e-02,\n",
       "          1.33783109e-02, -2.36845519e-02,  2.15480253e-02,  1.77107565e-02,\n",
       "          2.43755337e-02,  9.06958729e-02, -3.35601643e-02,  2.91374624e-02,\n",
       "         -1.68983545e-02,  4.17776518e-02, -5.23291677e-02,  1.62684079e-02,\n",
       "         -3.84396687e-02, -4.16958407e-02,  1.57663766e-02,  5.58817701e-04,\n",
       "         -2.72508785e-02, -1.69662070e-02, -4.78579663e-02, -5.05250832e-03,\n",
       "          3.21359113e-02, -5.45575544e-02,  4.02327925e-02,  5.53490594e-04,\n",
       "          3.56700830e-02,  1.87647585e-02, -5.42856082e-02,  3.38808894e-02,\n",
       "         -3.66515182e-02, -3.34847509e-03, -5.53161278e-03, -1.42183034e-02,\n",
       "          9.55430139e-03, -1.91266220e-02,  1.00305164e-02,  4.23212908e-02,\n",
       "         -5.31970449e-02, -4.89737652e-03,  3.96017637e-03,  2.44116504e-02,\n",
       "          6.07803315e-02,  1.44960433e-02, -4.14362773e-02,  4.84709404e-02,\n",
       "          2.44262777e-02, -1.95928533e-02, -6.26192242e-03,  4.90026455e-03,\n",
       "         -2.70042587e-02,  3.42756137e-02, -3.15132774e-02,  2.63322666e-02,\n",
       "         -3.62955779e-02, -3.09043308e-03, -3.26176584e-02, -1.50276255e-02,\n",
       "         -5.58135249e-02,  5.60539700e-02,  2.53478885e-02, -6.17963029e-03,\n",
       "         -1.21332966e-02, -4.74995002e-02, -2.59609129e-02,  1.10308997e-01,\n",
       "          1.79345347e-02,  6.36437023e-03, -2.96283159e-02,  6.10669740e-02,\n",
       "          3.20692323e-02,  5.87245356e-03,  1.27171725e-02, -1.89951155e-03,\n",
       "         -5.99441826e-02, -7.20293680e-03,  7.97716677e-02,  3.09191365e-02,\n",
       "         -1.47508038e-02, -2.92501878e-02, -1.79201923e-02, -3.03134471e-02,\n",
       "          1.23719312e-02, -1.34797255e-02, -5.40858693e-02,  4.72731926e-02,\n",
       "          5.11978641e-02, -3.54969054e-02,  1.68613996e-02,  1.82827155e-03,\n",
       "         -8.26896913e-03, -1.03118708e-02,  2.24239789e-02,  1.05614625e-02,\n",
       "          4.11573891e-03,  1.26325188e-03, -7.50332884e-03,  4.26309705e-02,\n",
       "          2.81609967e-02,  3.08737606e-02,  1.78347882e-02, -3.08689419e-02,\n",
       "         -5.80989681e-02, -1.92593597e-02,  6.03272654e-02, -2.27754544e-02,\n",
       "         -2.86705736e-02, -5.24598435e-02,  5.79293957e-03,  1.13270305e-01,\n",
       "         -2.70320456e-02, -5.03987111e-02,  3.27892303e-02, -2.32860241e-02,\n",
       "         -4.29078862e-02, -2.76266262e-02,  5.48377335e-02,  1.16684334e-02,\n",
       "         -1.61988977e-02,  4.35901061e-02, -2.40589697e-02,  4.34388481e-02,\n",
       "         -1.14679057e-02, -5.40662045e-03,  2.73526870e-02, -6.97151870e-02,\n",
       "          4.15201895e-02, -5.46101071e-02, -7.97946379e-02,  2.00231187e-03,\n",
       "         -5.84807433e-03, -8.32136348e-03, -1.03678657e-02,  1.12412293e-02,\n",
       "          3.89021374e-02,  1.76647045e-02,  1.28571491e-03, -3.52818556e-02,\n",
       "         -3.71606573e-02,  1.29764238e-02,  1.05864359e-02, -2.06624623e-02,\n",
       "         -1.20990016e-02,  2.45460123e-02, -2.63000447e-02,  2.68134344e-02,\n",
       "         -4.71455604e-03, -1.02290921e-02, -9.15221050e-02, -2.27687377e-02,\n",
       "         -2.23881137e-02, -1.18851408e-01, -5.93456207e-03, -8.50365032e-03,\n",
       "         -2.83095893e-02, -2.67576277e-02, -1.91116445e-02,  1.47616630e-02,\n",
       "          3.52067198e-03, -7.21891085e-03,  8.55921581e-03,  6.92953914e-03,\n",
       "          2.86143236e-02, -4.76288423e-02, -2.64847130e-02, -4.64510098e-02,\n",
       "         -1.52160758e-02,  2.86037177e-02,  3.16535272e-02,  1.45137655e-02,\n",
       "         -3.21002603e-02,  1.82518642e-02,  2.13942751e-02,  2.41732057e-02,\n",
       "         -2.62816437e-02,  4.15940061e-02, -1.41114332e-02,  3.36457975e-02,\n",
       "          5.64195449e-03,  2.93699987e-02, -1.11116879e-02, -2.96231639e-02,\n",
       "         -3.42655405e-02,  2.30252091e-02, -1.11736050e-02,  9.26342160e-02,\n",
       "         -7.17407987e-02, -1.61883589e-02, -1.88520532e-02,  3.06486040e-02,\n",
       "         -1.90695040e-02, -9.69732821e-04,  4.05264387e-33,  1.78361200e-02,\n",
       "          2.95207687e-02,  2.79021282e-02,  7.45876040e-03, -1.60720684e-02,\n",
       "         -1.08553031e-02, -2.14491487e-02,  3.04941777e-02,  3.86947533e-03,\n",
       "          1.11044943e-02, -2.25471742e-02, -7.74593875e-02, -2.69802082e-02,\n",
       "          2.19804402e-02,  2.16813125e-02,  2.41208021e-02,  6.28693923e-02,\n",
       "          4.88644280e-02, -2.97650937e-02,  1.21428194e-02, -2.67116483e-02,\n",
       "          4.09170017e-02, -5.00031933e-02,  3.70614454e-02,  1.48027819e-02,\n",
       "          2.54845545e-02,  4.25446257e-02, -2.21073069e-02, -2.05105152e-02,\n",
       "          1.50983492e-02,  3.00364401e-02, -5.35936132e-02,  3.75009812e-02,\n",
       "         -2.50552017e-02, -1.09811343e-01, -6.17452292e-03,  1.36383753e-02,\n",
       "          1.54789281e-03, -2.67577358e-02,  9.22884513e-03, -5.01360558e-03,\n",
       "          2.73256768e-02, -4.35897410e-02,  3.51306908e-02, -1.94342397e-02,\n",
       "         -3.35187726e-02,  1.52299153e-02, -2.30543613e-02,  2.56362110e-02,\n",
       "         -4.15378176e-02,  3.62413935e-02,  5.62564060e-02,  8.56396183e-03,\n",
       "         -3.18022631e-02,  1.89529005e-02,  8.43464397e-03,  1.55829087e-01,\n",
       "         -4.80835363e-02,  7.71743953e-02,  2.86444876e-04,  1.52085893e-04,\n",
       "          4.22076285e-02, -5.46598155e-03,  1.45862838e-02, -4.02816348e-02,\n",
       "          1.99319646e-02,  2.66252570e-02,  1.76062640e-02, -1.74457300e-02,\n",
       "          4.45189588e-02,  2.52929348e-02, -4.46611159e-02, -1.15166986e-02,\n",
       "          4.33784276e-02,  3.85035737e-03, -3.94102149e-02, -1.08549614e-02,\n",
       "         -8.38941522e-03,  3.95160206e-02, -6.54953122e-02,  1.90564233e-03,\n",
       "          2.09126193e-02,  7.95200765e-02,  3.02503891e-02,  1.50343915e-02,\n",
       "         -4.35195975e-02,  9.01916437e-03,  1.66350100e-02, -3.99064980e-02,\n",
       "          1.33100133e-02, -3.76776271e-02,  9.71368793e-03, -1.47771407e-02,\n",
       "          2.49028043e-03, -1.55214658e-02,  1.90285817e-02,  5.77546060e-02,\n",
       "          2.61725802e-02,  8.26312229e-03, -1.75185055e-02, -2.77677849e-02,\n",
       "         -7.25742504e-02,  2.80684214e-02, -4.52517197e-02,  1.14431074e-02,\n",
       "         -1.64971818e-04, -5.49435019e-02,  1.79989561e-02,  5.18789422e-03,\n",
       "         -2.94274837e-02, -5.78682544e-03,  7.11888224e-02,  2.40827538e-02,\n",
       "         -4.74053062e-02,  5.33002503e-02,  4.35564518e-02, -3.47361341e-02,\n",
       "          9.41527449e-03,  9.08227116e-02, -1.93809737e-02, -7.46625215e-02,\n",
       "         -3.57691199e-02, -1.52239241e-02, -7.21030589e-03,  9.52959713e-03,\n",
       "          1.89085826e-02, -1.31996525e-02,  2.26911232e-02, -3.61182466e-02,\n",
       "         -1.03289381e-01, -1.40888654e-02,  3.07127330e-02, -3.77596542e-02,\n",
       "         -2.76665781e-02, -1.93495285e-02, -1.51626393e-02,  3.39536853e-02,\n",
       "          1.25354119e-02, -2.54313219e-02,  1.28657017e-02,  3.40893678e-02,\n",
       "         -5.07011972e-02,  4.27511632e-02,  6.50608819e-03, -1.81000605e-02,\n",
       "         -2.12803520e-02, -2.34973487e-02,  1.92305613e-02,  3.84970498e-03,\n",
       "         -2.55429447e-02,  4.22887057e-02, -4.25956398e-02,  4.51226309e-02,\n",
       "         -7.59907113e-03, -3.04186307e-02,  3.29418224e-03, -2.91008316e-03,\n",
       "         -2.88832337e-02, -3.59751470e-02,  2.01489776e-02, -2.76588975e-03,\n",
       "          4.05418761e-02,  3.79116498e-02, -1.73382927e-02,  2.83543579e-02,\n",
       "         -5.86630916e-03,  4.70230263e-03, -2.77654640e-03, -4.03438359e-02,\n",
       "          4.19169590e-02, -3.58712338e-02, -4.21173451e-03,  2.20147222e-02,\n",
       "         -1.31896115e-04,  3.04815825e-02,  3.64072546e-02, -1.91784203e-02,\n",
       "          3.26346159e-02,  3.23791690e-02, -8.08942840e-02,  5.11912070e-02,\n",
       "         -2.92143691e-02, -3.62902991e-02,  4.18180637e-02,  3.50342989e-02,\n",
       "         -4.11086529e-02, -4.80125807e-02,  4.50038351e-02, -5.98221384e-02,\n",
       "          1.15895914e-02, -1.43407024e-02, -2.50790864e-02,  2.67161094e-02,\n",
       "          7.80998496e-03,  8.77071470e-02,  2.92784628e-02,  4.07847315e-02,\n",
       "         -2.60506198e-02,  2.21535880e-02,  7.93088973e-03,  6.51885495e-02,\n",
       "          1.15191983e-02, -1.06437374e-02, -1.03169158e-02,  6.71008751e-02,\n",
       "         -2.74880119e-02, -1.35773430e-02,  1.32217025e-02, -1.73453111e-02,\n",
       "          2.45242435e-02, -1.29570806e-04,  6.72443211e-02, -7.18048215e-02,\n",
       "         -1.97527464e-03, -1.23100467e-02,  1.99143309e-02,  5.16201183e-03,\n",
       "          5.86064793e-02, -1.30363798e-04, -4.24043089e-02,  2.47584633e-03,\n",
       "          8.18925072e-03, -2.81266719e-02,  1.00252843e-02,  3.26608717e-02,\n",
       "          4.72940691e-02, -1.67179778e-02,  2.67999601e-02,  7.81488605e-03,\n",
       "         -4.62561054e-03, -1.59483906e-02,  5.24178781e-02,  1.77557860e-02,\n",
       "         -2.06609704e-02, -6.10230211e-03,  2.55610310e-02, -2.98713353e-02,\n",
       "         -1.01340259e-03,  2.19287015e-02,  4.50439528e-02,  1.94276627e-02,\n",
       "         -6.76816935e-03,  8.10186490e-02, -2.66175997e-02, -3.46401744e-02,\n",
       "          2.90799364e-02, -1.74919758e-02,  6.58837929e-02, -5.86273596e-02,\n",
       "          3.99295315e-02, -2.83789113e-02,  2.61486955e-02,  6.00944087e-03,\n",
       "         -4.15012352e-02,  2.49973107e-02, -6.32924885e-02,  1.39696440e-02,\n",
       "         -6.21763468e-02, -8.26508552e-03,  1.48010878e-02, -4.95979898e-02,\n",
       "         -4.45115119e-02,  2.03000885e-02,  1.74150765e-02,  9.99345444e-03,\n",
       "         -7.71448901e-03, -2.95911711e-02, -1.32644298e-02, -4.96730842e-02,\n",
       "          5.52318292e-03,  1.18668666e-02, -5.71057685e-02, -1.45681333e-02]),\n",
       "  'score': tensor(0.0799)},\n",
       " {'page_number': 37,\n",
       "  'sentence_chunk': 'the back of the sofa, jerked the gun out of Uncle Vernon’s hands, bent it into a knot as easily as if it had been made of rubber, and threw it into a corner of the room.   Uncle Vernon made another funny noise, like a mouse being trodden on.   “Anyway — Harry,” said the giant, turning his back on the Dursleys, “a very happy birthday to yeh. Got summat fer yeh here — I mighta sat on it at some point, but it’ll taste all right.”   From an inside pocket of his black overcoat he pulled a slightly squashed box. Harry opened it with trembling fingers. Inside was a large, sticky chocolate cake with Happy Birthday Harry written on it in green icing.   Harry looked up at the giant. He meant to say thank you, but the words got lost on the way to his mouth, and what he said instead was, “Who are you?”   The giant chuckled.   “True, I haven’t introduced meself. Rubeus Hagrid, Keeper of Keys and Grounds at Hogwarts.”   He held out an enormous hand and shook Harry’s whole arm.   “What about that tea then, eh?”he said, rubbing his hands together. “I’d not say no ter summat stronger if yeh’ve got it, mind.”   His eyes fell on the empty grate with the shriveled chip bags in it and he snorted. He bent down over the fireplace; they couldn’t see what he was doing but when he drew back a second later, there was a roaring fire there. It filled the whole damp hut with flickering light and Harry felt the warmth wash over him as though he’d sunk into a hot bath.   The giant sat back down on the sofa, which sagged under his weight, and began taking all sorts of things out of the pockets of his coat: a copper kettle, a squashy package of sausages, a poker, a teapot, several chipped mugs, and a bottle of some amber liquid that he took a swig from before starting to make tea.',\n",
       "  'chunk_char_count': 1777,\n",
       "  'chunk_word_count': 355,\n",
       "  'chunk_token_count': 444.25,\n",
       "  'embedding': array([-1.16029819e-02, -7.92929605e-02,  3.08981165e-03, -1.30167208e-03,\n",
       "          8.58196169e-02,  5.07268645e-02,  2.66230106e-02, -9.07168444e-03,\n",
       "          2.89322436e-02,  1.89274140e-02, -3.75045612e-02, -4.19283239e-03,\n",
       "         -8.46771244e-03, -5.25668897e-02, -1.79998223e-02, -5.46161383e-02,\n",
       "          1.45493692e-03, -2.29812209e-02, -7.62851685e-02,  1.00613362e-03,\n",
       "          4.88178357e-02, -8.46984796e-03,  3.99893858e-02, -3.83815616e-02,\n",
       "          2.64759436e-02, -2.83913128e-02,  3.17332149e-02,  2.50203926e-02,\n",
       "         -3.85564985e-04,  9.43003036e-03, -5.28195500e-02, -3.02969143e-02,\n",
       "          2.25925613e-02,  6.28962144e-02,  2.74600871e-02,  1.10662859e-02,\n",
       "          3.80548276e-02, -1.98029876e-02,  8.89967158e-02, -4.45686001e-03,\n",
       "         -6.79605380e-02,  1.23806223e-02,  2.65479870e-02,  5.16716111e-03,\n",
       "         -6.43932074e-03, -2.19918285e-02, -1.58849396e-02,  5.05406931e-02,\n",
       "          3.37787122e-02, -1.39020961e-02,  4.71026711e-02,  3.39798070e-02,\n",
       "         -4.36503813e-02,  3.93333100e-02,  1.32375443e-02,  3.01339235e-02,\n",
       "          6.96425512e-03,  4.26900275e-02,  1.12353060e-02, -8.14659745e-02,\n",
       "          1.21306174e-01,  7.73223769e-03, -5.94100431e-02,  7.82168657e-03,\n",
       "         -5.33161045e-04, -6.31993636e-04, -2.73481589e-02, -2.58351751e-02,\n",
       "         -3.57487872e-02, -1.71762034e-02,  1.44340862e-02, -2.38296017e-02,\n",
       "         -4.29871306e-03,  5.06746694e-02, -2.69140359e-02, -6.22449117e-03,\n",
       "          2.86339503e-02,  4.49463874e-02,  1.29390787e-02,  1.14073846e-02,\n",
       "         -2.87498161e-02,  5.36587015e-02,  3.66953597e-03,  9.70622059e-03,\n",
       "          4.92335223e-02, -1.34160946e-04,  2.51120534e-02, -2.63345633e-02,\n",
       "          1.68450139e-02, -2.27270145e-02, -1.57307405e-02,  1.15376405e-01,\n",
       "          8.72506015e-03,  9.43892822e-02, -4.15204912e-02,  4.69947569e-02,\n",
       "         -3.83429378e-02,  1.01890936e-01, -3.65066640e-02, -2.90748999e-02,\n",
       "          2.99202930e-02,  1.23662427e-02, -8.36159736e-02, -5.00873104e-02,\n",
       "          2.56158542e-02, -4.45895530e-02, -2.80001163e-02, -1.11950068e-02,\n",
       "         -2.78804284e-02,  1.19145969e-02, -3.47391702e-03, -5.11705838e-02,\n",
       "         -1.93812661e-02,  4.57560532e-02,  1.05471332e-02, -3.24719064e-02,\n",
       "          1.40781580e-02,  2.35257894e-02, -6.11554645e-03,  1.40764378e-02,\n",
       "          1.11216484e-02, -1.19593861e-02, -1.68615263e-02,  5.47250658e-02,\n",
       "         -2.51169465e-02,  4.16357592e-02,  4.78890650e-02, -5.06506115e-02,\n",
       "         -8.60426389e-03,  2.12721340e-02, -8.07041675e-02,  1.53794130e-02,\n",
       "          1.83338504e-02, -1.82190761e-02, -3.72726582e-02,  1.32050319e-02,\n",
       "          6.72761798e-02,  4.91987206e-02, -1.99023932e-02,  2.32248828e-02,\n",
       "          6.18408376e-05,  5.14890552e-02,  1.01812370e-02,  9.45591833e-03,\n",
       "         -3.99676301e-02,  1.24134822e-02,  7.86692742e-03,  2.36726925e-02,\n",
       "         -2.85296999e-02,  4.81934957e-02,  2.17283387e-02,  1.18762469e-02,\n",
       "         -5.80026582e-03, -5.38017489e-02, -7.53100403e-03, -1.76504534e-03,\n",
       "         -1.05692372e-02,  7.38150487e-03, -7.74411112e-03, -1.71177723e-02,\n",
       "          2.74888985e-02,  4.42716293e-02,  2.75831707e-02, -5.95767573e-02,\n",
       "         -1.42271379e-02, -2.15490609e-02,  4.03581075e-02,  1.01616718e-02,\n",
       "          1.75209455e-02, -2.60670632e-02, -2.39874385e-02,  1.49278557e-02,\n",
       "          5.04862256e-02,  3.50827090e-02, -3.75147676e-03, -2.37125400e-02,\n",
       "         -1.05010979e-02, -4.44522128e-03,  1.80218723e-02,  2.96598431e-02,\n",
       "         -2.36881413e-02,  4.29982767e-02,  2.75390781e-03, -1.63700636e-02,\n",
       "          2.21380237e-02, -1.76593326e-02, -2.53348239e-02, -1.53975450e-02,\n",
       "         -6.54881522e-02, -1.47769134e-03, -3.32285538e-02,  6.60288706e-03,\n",
       "          5.52329980e-02, -1.28547503e-02,  5.91476448e-02,  2.08627172e-02,\n",
       "         -5.73085435e-02,  9.61830933e-03,  6.81013763e-02, -4.64419276e-02,\n",
       "         -2.76299696e-02, -6.74187988e-02, -6.69395030e-02, -4.40785736e-02,\n",
       "         -3.36055122e-02, -2.49683037e-02,  2.60876622e-02,  8.76130722e-03,\n",
       "         -1.97437759e-02,  8.06915294e-03,  1.62241012e-02, -7.11892843e-02,\n",
       "         -2.03990564e-02,  9.51779075e-03,  5.55477552e-02, -3.55402976e-02,\n",
       "          2.95565128e-02, -2.42105704e-02, -1.68793760e-02, -2.19998471e-02,\n",
       "         -1.39136203e-02, -1.53286587e-02, -2.81673353e-02, -8.12261179e-03,\n",
       "         -3.71598341e-02, -5.90045452e-02, -1.51696494e-02, -1.99641027e-02,\n",
       "         -8.80160369e-03,  6.10708296e-02, -3.26783992e-02,  2.71825884e-02,\n",
       "         -3.22700813e-02, -5.70894368e-02, -3.99981029e-02, -5.11185676e-02,\n",
       "         -3.36581431e-02, -1.98920798e-02,  2.36484092e-02,  3.04799117e-02,\n",
       "         -2.91175228e-02,  2.56335568e-02,  4.35015038e-02, -5.28743444e-03,\n",
       "         -5.38803264e-03, -7.09606260e-02,  2.19628531e-02,  6.82608560e-02,\n",
       "          2.93607432e-02, -3.59955467e-02,  2.92214658e-02,  1.82609800e-02,\n",
       "         -1.92596503e-02, -4.07350846e-02,  1.12082958e-02, -9.52723995e-03,\n",
       "         -9.36601963e-03, -3.80809675e-03,  1.34018939e-02,  2.22725086e-02,\n",
       "          8.76420923e-03, -1.98361501e-02, -1.82856545e-02, -4.01125522e-03,\n",
       "         -9.88209341e-03,  2.76787784e-02, -2.76047401e-02,  4.18044161e-03,\n",
       "         -2.84840111e-02, -1.22299162e-03, -5.56403259e-03, -2.24117171e-02,\n",
       "          1.54214399e-02,  3.37727442e-02,  1.20810466e-02,  2.67907958e-02,\n",
       "         -3.87593396e-02,  6.76228404e-02,  1.02502769e-02, -5.93090467e-02,\n",
       "          6.47857506e-03, -6.67328387e-02,  2.09159553e-02, -5.17406054e-02,\n",
       "         -1.17234075e-02,  3.63247767e-02, -2.47818381e-02,  4.21576686e-02,\n",
       "          3.49422055e-03, -2.75398195e-02, -3.34884152e-02,  4.90389345e-03,\n",
       "          4.36514914e-02,  5.83232865e-02,  6.77396683e-03,  5.38397096e-02,\n",
       "         -1.88647956e-02, -5.99400438e-02,  2.92717759e-02, -4.87317331e-02,\n",
       "         -3.78434509e-02, -3.51558588e-02, -3.62330303e-02,  4.04195152e-02,\n",
       "          2.43123360e-02, -1.68696861e-03,  1.91430263e-02, -2.06549186e-02,\n",
       "         -7.64574669e-03,  2.70174313e-02, -7.58814206e-03,  4.09724005e-03,\n",
       "          3.43358330e-03, -6.88661821e-04, -3.97932492e-02, -2.27204314e-03,\n",
       "          2.89102644e-02, -3.73600870e-02,  1.49219371e-02, -5.84130641e-03,\n",
       "          3.83524247e-03, -3.38670723e-02, -4.13727015e-02,  7.57882884e-03,\n",
       "          1.02234498e-01, -5.40321991e-02,  4.87732515e-02,  2.98933052e-02,\n",
       "          3.19113652e-03,  1.35642299e-02, -8.10607523e-02,  1.36148054e-02,\n",
       "         -2.68930346e-02, -1.79307144e-02, -2.69664377e-02,  6.97642658e-03,\n",
       "          2.85160914e-02, -1.60322264e-02,  1.55934915e-02,  2.17859745e-02,\n",
       "         -4.53738756e-02,  3.56197059e-02, -5.53353578e-02,  2.83172801e-02,\n",
       "          5.83452061e-02, -5.98941045e-03, -5.80073185e-02,  2.62279678e-02,\n",
       "         -2.83695795e-02, -1.67369507e-02, -8.94134771e-03,  4.31313999e-02,\n",
       "          1.95011590e-02,  4.45848517e-02, -6.27780706e-02,  4.79874760e-02,\n",
       "          1.52277788e-02, -4.06703167e-02, -1.05170598e-02, -3.53893153e-02,\n",
       "         -1.84803028e-02,  1.15314148e-01,  4.04934138e-02, -1.96650904e-02,\n",
       "         -2.24494692e-02, -7.23153427e-02, -3.31377164e-02,  6.79415017e-02,\n",
       "         -2.01606229e-02,  1.24476757e-02, -1.85466669e-02,  4.94804196e-02,\n",
       "          2.23236289e-02, -1.74757279e-02, -1.47471987e-02, -1.42792678e-02,\n",
       "         -4.25483519e-03, -4.36974280e-02,  9.89340916e-02, -2.30004564e-02,\n",
       "          2.96854018e-03, -5.44051826e-03, -4.54269946e-02, -1.78927034e-02,\n",
       "         -8.02096259e-03, -1.28211798e-02, -4.06441763e-02, -1.31377680e-02,\n",
       "          3.12396400e-02,  6.05188170e-03, -1.86211243e-02, -4.57459539e-02,\n",
       "         -2.63393279e-02, -1.11561147e-02,  1.61223151e-02,  1.30910324e-02,\n",
       "         -3.96092646e-02,  1.31444316e-02,  2.01033540e-02,  2.75062080e-02,\n",
       "         -1.96647290e-02,  4.82396856e-02,  8.95186514e-03, -2.91784350e-02,\n",
       "         -7.05047771e-02, -2.51096790e-03,  1.85895935e-02, -2.87466124e-02,\n",
       "         -3.57453618e-03, -7.06783384e-02,  5.93991950e-03,  5.93774095e-02,\n",
       "         -1.92536041e-02, -7.45835379e-02,  2.39262730e-02, -3.31607851e-04,\n",
       "         -2.20317859e-02, -1.96666364e-02, -1.37820486e-02,  5.97575940e-02,\n",
       "         -8.81353451e-04,  5.77237317e-03, -4.87498827e-02,  2.13594511e-02,\n",
       "          4.39123670e-03, -7.84141943e-03,  2.09983462e-03, -7.99788013e-02,\n",
       "          5.41075021e-02, -5.81456982e-02, -6.67938963e-02, -4.17233445e-03,\n",
       "          4.78796251e-02, -4.12687771e-02, -2.89239865e-02, -8.76802765e-03,\n",
       "          5.36507107e-02,  2.72666151e-03, -7.44001893e-03,  1.67010035e-02,\n",
       "         -1.46368789e-02,  2.19129585e-02,  3.86728048e-02, -9.29687843e-02,\n",
       "         -1.68113094e-02,  3.84575017e-02,  3.84596363e-02,  3.18387779e-03,\n",
       "         -2.48039234e-02,  1.79405008e-02, -7.49685392e-02,  4.22921069e-02,\n",
       "          4.09369431e-02, -1.30734652e-01, -4.20836778e-03,  3.36505063e-02,\n",
       "          6.22243434e-03, -3.58774886e-02,  2.99053616e-03, -1.17995765e-03,\n",
       "          4.15788293e-02,  6.38753641e-03,  1.66055225e-02,  3.71052995e-02,\n",
       "         -3.23468400e-03,  1.13102412e-02, -2.50255354e-02, -3.87251303e-02,\n",
       "         -1.95320398e-02, -7.81948969e-04,  4.86332774e-02,  4.03500982e-02,\n",
       "         -2.12702882e-02,  2.73721362e-03,  1.28061138e-02,  3.69256996e-02,\n",
       "         -1.90531183e-02,  3.71080860e-02, -3.16936001e-02,  1.10880379e-02,\n",
       "          1.04864745e-03,  1.59167312e-02, -5.74873108e-03, -4.57671052e-03,\n",
       "         -4.98432890e-02, -3.56702842e-02, -3.37214433e-02,  9.17144865e-02,\n",
       "         -5.11683300e-02, -1.59755144e-02,  3.25789093e-03, -9.99359181e-04,\n",
       "         -4.30807509e-02,  1.19162826e-02,  4.27754386e-33, -6.09504897e-03,\n",
       "         -2.74966974e-02,  3.01873162e-02,  2.56684609e-03, -3.29368040e-02,\n",
       "          6.98688021e-03, -1.63819250e-02,  8.22190195e-02,  3.09333783e-02,\n",
       "         -1.63042191e-02,  6.00787485e-03, -9.05729737e-03,  2.30211113e-03,\n",
       "          5.05167106e-03, -1.73558239e-02,  7.66254542e-03,  2.23023724e-02,\n",
       "          3.73630249e-03,  1.30746095e-02, -5.31793991e-03, -1.60943307e-02,\n",
       "          3.14654149e-02, -3.16069946e-02,  3.75749916e-02,  1.70237236e-02,\n",
       "          1.07377833e-02, -9.00232233e-03, -2.55788974e-02, -1.43393800e-02,\n",
       "          3.21290158e-02,  1.77629609e-02, -2.57320423e-03,  5.91032580e-02,\n",
       "         -3.15307900e-02, -6.86515346e-02, -4.16559391e-02,  1.69225037e-02,\n",
       "          5.25575578e-02, -3.62241939e-02,  1.51072238e-02, -1.51069388e-02,\n",
       "          8.32962617e-03,  3.51955486e-03,  3.79392654e-02, -5.10454737e-02,\n",
       "         -2.44247764e-02,  3.56311090e-02, -3.23880129e-02,  4.61577699e-02,\n",
       "          2.05099918e-02,  2.53070910e-02,  6.01804815e-02,  2.18816958e-02,\n",
       "         -5.76071180e-02,  2.74585374e-02,  1.91116205e-03,  1.41458645e-01,\n",
       "         -5.08590601e-02,  7.36397877e-02, -2.82270019e-03, -2.95254979e-02,\n",
       "          3.91936162e-03, -1.00799860e-03,  3.78107540e-02, -2.43857484e-02,\n",
       "          2.66591068e-02,  1.62265599e-02,  2.52486486e-02,  3.95109653e-02,\n",
       "          3.46722640e-02, -4.41956706e-03, -2.80946065e-02,  2.19147690e-02,\n",
       "          3.20065543e-02, -6.23205528e-02, -3.95980589e-02,  6.97286520e-03,\n",
       "         -6.71963301e-03,  2.29924824e-02, -2.05027917e-03,  9.34007578e-03,\n",
       "         -2.13239212e-02,  4.76762019e-02, -8.15084204e-03, -2.22796872e-02,\n",
       "         -2.32097581e-02, -2.35485807e-02,  1.47458250e-02,  9.24115069e-03,\n",
       "          7.14133540e-03, -3.67450900e-02,  1.17634069e-02, -1.66296470e-03,\n",
       "          2.26409989e-03, -4.30258065e-02,  1.07012661e-02,  1.25870863e-02,\n",
       "          3.14823352e-03,  1.29579089e-03, -1.89920068e-02,  3.92867588e-02,\n",
       "         -5.16944565e-02,  7.10832607e-03, -5.75242005e-02,  1.64763611e-02,\n",
       "         -1.44158229e-02, -1.96467829e-03,  3.62334512e-02, -2.92809005e-03,\n",
       "          9.24003311e-03,  3.70932221e-02,  7.39674866e-02,  2.72112899e-03,\n",
       "         -6.83576018e-02,  3.75159197e-02,  1.86473709e-02, -1.50108663e-02,\n",
       "         -2.31030285e-02,  7.10832477e-02,  1.59719177e-02, -5.07567860e-02,\n",
       "         -2.26247907e-02,  7.91319739e-03, -7.00887367e-02,  8.15175399e-02,\n",
       "          3.70870829e-02, -1.72667634e-02, -1.01784468e-02, -2.60468982e-02,\n",
       "         -8.40529203e-02, -1.25974584e-02,  2.84655672e-03, -1.34773338e-02,\n",
       "         -3.17926481e-02, -1.51518648e-02, -4.05001156e-02,  2.48974934e-02,\n",
       "          1.93710290e-02, -4.76880372e-02, -2.65220776e-02,  3.44140381e-02,\n",
       "         -1.29816374e-02,  6.63509443e-02,  2.94425189e-02, -4.13878188e-02,\n",
       "         -1.47352600e-02, -2.23514680e-02, -8.87515303e-03, -3.01662255e-02,\n",
       "         -8.55705235e-03,  7.84116909e-02, -8.86369683e-03,  4.67609242e-02,\n",
       "         -1.47783682e-02,  8.12433753e-03,  3.93039873e-03,  2.57321931e-02,\n",
       "         -1.13301119e-02, -3.38333063e-02,  3.57430391e-02, -2.67252699e-02,\n",
       "          1.73724014e-02,  3.34787071e-02, -4.37206849e-02,  1.63440816e-02,\n",
       "         -6.46887859e-03,  9.51528084e-03,  2.76791360e-02, -1.10948272e-02,\n",
       "         -4.97914199e-03, -2.39854250e-02, -1.35968560e-02,  2.86173820e-02,\n",
       "          2.26033609e-02,  5.97855030e-03,  4.52219211e-02,  3.46886180e-03,\n",
       "          8.06061551e-03,  2.30985638e-02, -7.21102431e-02,  6.68670163e-02,\n",
       "          1.25902686e-02, -5.28985672e-02,  8.94159451e-03,  3.46932076e-02,\n",
       "         -3.73429470e-02, -5.58154583e-02,  7.54103363e-02, -6.71421811e-02,\n",
       "          4.56295162e-03,  8.54311325e-03,  7.56809348e-03,  1.11282133e-02,\n",
       "         -3.98348235e-02,  1.95068344e-02, -7.03083631e-03,  2.56303623e-02,\n",
       "         -2.74438187e-02,  1.96644105e-02, -4.81380336e-02,  3.94451246e-02,\n",
       "          3.74231413e-02, -2.13113762e-02, -2.14623082e-02,  3.52828875e-02,\n",
       "         -5.43692987e-03, -4.76187803e-02, -4.92684310e-03, -4.56422083e-02,\n",
       "         -1.71679761e-02, -9.96383373e-03,  7.94400945e-02, -1.03317298e-01,\n",
       "         -3.13165784e-03, -5.30496798e-02, -2.83769127e-02, -1.18417054e-04,\n",
       "          7.43193328e-02, -1.20016504e-02, -2.16971319e-02,  5.02202800e-03,\n",
       "         -3.59767675e-02, -1.08737564e-02, -1.34973712e-02,  3.98543105e-02,\n",
       "          7.26082772e-02, -6.53444380e-02, -3.00348047e-02, -3.11114490e-02,\n",
       "          6.90044742e-03, -4.81955968e-02,  9.57671776e-02,  1.69496872e-02,\n",
       "         -5.42123057e-02, -5.74912168e-02,  6.29362687e-02,  1.87916271e-02,\n",
       "         -7.41115911e-03,  4.74994816e-02,  3.12443171e-03,  4.10049111e-02,\n",
       "          3.16861388e-03,  5.33906706e-02, -6.11183755e-02, -6.15365207e-02,\n",
       "          1.72209777e-02, -1.76687110e-02,  4.41823713e-02, -5.20391241e-02,\n",
       "          2.01773960e-02, -4.62640300e-02,  5.00896201e-03,  1.68338101e-02,\n",
       "         -4.55233790e-02,  5.03690355e-02, -6.76084310e-02,  5.19565195e-02,\n",
       "          4.22631623e-03,  2.37879772e-02,  2.81154015e-03, -3.99170071e-02,\n",
       "         -3.55665423e-02,  1.04783885e-02,  2.34420542e-02,  1.55233061e-02,\n",
       "         -2.68227383e-02, -4.74548377e-02,  1.84453297e-02, -5.27393585e-03,\n",
       "          1.18156197e-02, -1.53025053e-02, -3.32492329e-02,  2.87052952e-02]),\n",
       "  'score': tensor(0.0761)}]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = \"what made harry frightened?\"\n",
    "print(f\"Query: {ques}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=ques, \n",
    "                            temperature=0.3,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local RAG workflow complete!\n",
    "\n",
    "We've now officially got a way to Retrieve, Augment and Generate answers based on a source.\n",
    "\n",
    "For now we can verify our answers manually by reading them and reading through the textbook.\n",
    "\n",
    "But if you want to put this into a production system, it'd be a good idea to have some kind of evaluation on how well our pipeline works.\n",
    "\n",
    "For example, you could use another LLM to rate the answers returned by our LLM and then use those ratings as a proxy evaluation.\n",
    "\n",
    "However, I'll leave this and a few more interesting ideas as extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "* May want to improve text extraction with something like Marker - https://github.com/VikParuchuri/marker\n",
    "* Guide to more advanced PDF extraction - https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517 \n",
    "* See the following prompt engineering resources for more prompting techniques - promptinguide.ai, Brex's Prompt Engineering Guide \n",
    "* What happens when a query comes through that there isn't any context in the textbook on?\n",
    "* Try another embedding model (e.g. Mixed Bread AI large, `mixedbread-ai/mxbai-embed-large-v1`, see: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n",
    "* Try another LLM... (e.g. Mistral-Instruct)\n",
    "* Try different prompts (e.g. see prompting techniques online)\n",
    "* Our example only focuses on text from a PDF, however, we could extend it to include figures and images \n",
    "* Evaluate the answers -> could use another LLM to rate our answers (e.g. use GPT-4 to make)\n",
    "* Vector database/index for larger setup (e.g. 100,000+ chunks)\n",
    "* Libraries/frameworks such as LangChain / LlamaIndex can help do many of the steps for you - so it's worth looking into those next, wanted to recreate a workflow with lower-level tools to show the principles\n",
    "* Optimizations for speed\n",
    "    * See Hugging Face docs for recommended speed ups on GPU - https://huggingface.co/docs/transformers/perf_infer_gpu_one \n",
    "    * Optimum NVIDIA - https://huggingface.co/blog/optimum-nvidia, GitHub: https://github.com/huggingface/optimum-nvidia \n",
    "    * See NVIDIA TensorRT-LLM - https://github.com/NVIDIA/TensorRT-LLM \n",
    "    * See GPT-Fast for PyTorch-based optimizations - https://github.com/pytorch-labs/gpt-fast \n",
    "    * Flash attention 2 (requires Ampere GPUs or newer) - https://github.com/Dao-AILab/flash-attention\n",
    "* Stream text output so it looks prettier (e.g. each token appears as it gets output from the model)\n",
    "* Turn the workflow into an app, see Gradio type chatbots for this - https://www.gradio.app/guides/creating-a-chatbot-fast, see local example: https://www.gradio.app/guides/creating-a-chatbot-fast#example-using-a-local-open-source-llm-with-hugging-face "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
